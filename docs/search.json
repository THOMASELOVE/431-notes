[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes for 431",
    "section": "",
    "text": "This document is broken down into multiple chapters. Use the table of contents on the left side of the screen to navigate between chapters, or use the right side to navigate within the current chapter.\nYou can also search the document, using an automated index.\nAny of the code provided in the document can be copied to the clipboard using the Copy icon at the top right of the code block.\nThe document will be updated (unpredictably) throughout the semester.\n\n\n\nThese Notes provide a series of examples using R to work through issues that are likely to come up in PQHS/CRSP/MPHP 431. What you will mostly find are brief explanations of a key idea or summary, accompanied (most of the time) by R code and a demonstration of the results of applying that code.\nWhile these Notes share some of the features of a textbook, they are neither comprehensive nor completely original. The main purpose is to give 431 students a set of common materials on which to draw during the course. In class, we will sometimes:\n\nreiterate points made in this document,\namplify what is here,\nsimplify the presentation of things done here,\nuse new examples to show some of the same techniques,\nrefer to issues not mentioned in this document,\n\nbut what we don’t do is follow these notes very precisely. We assume instead that you will read the materials and try to learn from them, just as you will attend classes and try to learn from them. We welcome feedback of all kinds on this document or anything else.\n\n\n\nThe online home for Dr. Love’s 431 course in Fall 2022 is\nhttps://thomaselove.github.io/431-2022/.\nGo there for all information related to the course.\nAll of the code and text in these Notes is posted online as HTML, and it is also possible to download PDF and ePub versions of the document from the down arrow next to the title (Notes for 431) at the top left of this screen. All data and R code related to these notes are also available to you through our course web site.\nBy the end of the semester, you will also have access to the Quarto files which generate everything in the document, including all of the R results. Quarto is a souped-up version of R Markdown, which you will use during the semester to complete your assignments, and which I used to develop previous versions of these notes. We will demonstrate the use of R Markdown and RStudio (the “program” we use to interface with the R language) in class."
  },
  {
    "objectID": "int_r_setup.html",
    "href": "int_r_setup.html",
    "title": "Setting Up R",
    "section": "",
    "text": "These Notes make extensive use of\nboth of which are free, and you’ll need to install them on your machine. Instructions for doing so will be found on the course website.\nIf you need a gentle introduction, or if you’re just new to R and RStudio and need to learn about them, we encourage you to take a look at https://moderndive.com/, which provides an introduction to statistical and data sciences via R at Ismay and Kim (2022)."
  },
  {
    "objectID": "int_r_setup.html#r-markdown",
    "href": "int_r_setup.html#r-markdown",
    "title": "Setting Up R",
    "section": "R Markdown",
    "text": "R Markdown\nThese notes were written using Quarto, which is an amplification of R Markdown (which we’ll learn in 431.) R Markdown, like R and RStudio and Quarto, is free and open source.\nR Markdown is described as an authoring framework for data science, which lets you\n\nsave and execute R code\ngenerate high-quality reports that can be shared with an audience\n\nThis description comes from RStudio’s introduction to R Markdown which provides an overview and quick tour of what’s possible with R Markdown.\nAnother excellent resource to learn more about R Markdown tools is the Communicate section (especially the R Markdown chapter) of Wickham and Grolemund (2022)."
  },
  {
    "objectID": "int_r_setup.html#r-packages",
    "href": "int_r_setup.html#r-packages",
    "title": "Setting Up R",
    "section": "R Packages",
    "text": "R Packages\nAt the start of each chapter that involves R code, I’ll present a series of commands I run to set up R to use several packages (libraries) of functions that expand its capabilities, make a specific change to how I want R output to be displayed (that’s the comment = NA piece) and sets the theme for most graphs to theme_bw(). A chunk of code like this will occur near the top of any R Markdown work.\nFor example, this is the setup for one of our early chapters that loads four packages.\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(palmerpenguins)\nlibrary(janitor)\nlibrary(knitr)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\n\nYou only need to install a package once, but you need to reload it (using the library() function) every time you start a new session. I always load the package called tidyverse last, since doing so avoids some annoying problems."
  },
  {
    "objectID": "int_r_setup.html#the-love-boost.r-script",
    "href": "int_r_setup.html#the-love-boost.r-script",
    "title": "Setting Up R",
    "section": "The Love-boost.R script",
    "text": "The Love-boost.R script\nIn October, when we start Part B of the course, we’ll use some special R functions I’ve gathered for you in a script called Love-boost. I’ll tell R about that code using the following command…\n\nsource(\"data/Love-boost.R\")\n\nThe Love-boost.R script includes four functions:\n\nbootdif\nsaifs.ci\ntwobytwo\nretrodesign"
  },
  {
    "objectID": "int_r_setup.html#packages-used-in-these-notes",
    "href": "int_r_setup.html#packages-used-in-these-notes",
    "title": "Setting Up R",
    "section": "Packages Used in these Notes",
    "text": "Packages Used in these Notes\nA complete list of all R packages we want you to install this semester (which includes some packages not included in these Notes) is maintained at our course web site.\n\n\n\nPackage\nParts\nKey functions in the Package\n\n\n\n\narm\nC\n–\n\n\nboot\nB\n–\n\n\nbroom\nA, B, C\ntidy, augment, glance\n\n\ncar\nA, C\nboxCox, powerTransform, testTransform\n\n\nEpi\nB\ntwoby2\n\n\nfivethirtyeight\nAppendix\nsource of data\n\n\nGGally\nA, C\nggpairs\n\n\nggrepel\nC\n–\n\n\nggridges\nA, B\n–\n\n\ngt\nA\nfor presenting tables\n\n\nHmisc\nA, B, C\ndescribe and others\n\n\njanitor\nA, B, C\ntabyl and others\n\n\nknitr\nA, B, C\nkable\n\n\nmice\nC\n–\n\n\nmosaic\nA, B, C\nfavstats\n\n\nnaniar\nA\nn_miss, miss_case_table, gg_miss_var\n\n\nNHANES\nA\nsource of data\n\n\npalmerpenguins\nA\nsource of data\n\n\npatchwork\nA, B, C\nfor combining/annotating plots\n\n\npsych\nA, B\ndescribe\n\n\npwr\nB\n–\n\n\nrms\nC\n–\n\n\nsimputation\nA\nvarious impuation functions\n\n\ntidyverse\nA, B, C, Appendix\ndozens of functions\n\n\nvcd\nB\n–\n\n\n\n\nThe tidyverse\nThe tidyverse package is actually a meta-package which includes the following core packages:\n\nggplot2 for creating graphics\ndplyr for data manipulation\ntidyr for creating tidy data\nreadr for reading in rectangular data\npurrr for working with functions and vectors\ntibble for creating tibbles - lazy, surly data frames\nstringr for working with data strings\nforcats for solving problems with factors\n\nLoading the tidyverse with library(tidyverse) loads those eight packages.\nInstalling the tidyverse also installs several other useful packages on your machine. Read more about the tidyverse at https://www.tidyverse.org/\n\n\nPackages Not Included in the Notes at Present\n\ndevtools\nequatiomatic\ngapminder\nhere\nkableExtra\nmagrittr\nmarkdown\nmodelsummary\nnhanesA\nrmarkdown\nrmdformats\nrstanarm\nsessioninfo\ntableone\ntidymodels\nvisdat\n\n\n\n\n\nIsmay, Chester, and Albert Y. Kim. 2022. ModernDive: Statistical Inference via Data Science. http://moderndive.com/.\n\n\nWickham, Hadley, and Garrett Grolemund. 2022. R for Data Science. Second. O’Reilly. https://r4ds.hadley.nz/."
  },
  {
    "objectID": "01-datascience.html",
    "href": "01-datascience.html",
    "title": "1  Data Science and 431",
    "section": "",
    "text": "The definition of data science can be a little slippery. One current view of data science, is exemplified by Steven Geringer’s 2014 Venn diagram.\nData Science is a team activity. Everyone working in data science brings some part of the necessary skill set, but no one person can cover all three areas alone for excellent projects."
  },
  {
    "objectID": "01-datascience.html#data-science-project-cycle",
    "href": "01-datascience.html#data-science-project-cycle",
    "title": "1  Data Science and 431",
    "section": "1.1 Data Science Project Cycle",
    "text": "1.1 Data Science Project Cycle\nA typical data science project can be modeled as follows, which comes from the introduction to the amazing book R for Data Science, by Garrett Grolemund and Hadley Wickham, which is a key text for this course (Wickham and Grolemund 2022).\n\n\n\n\n\nSource: R for Data Science: Introduction\n\n\n\n\nThis diagram is sometimes referred to as the Krebs Cycle of Data Science. For more on the steps of a data science project, we encourage you to read the Introduction of Wickham and Grolemund (2022)."
  },
  {
    "objectID": "01-datascience.html#data-science-and-the-431-course",
    "href": "01-datascience.html#data-science-and-the-431-course",
    "title": "1  Data Science and 431",
    "section": "1.2 Data Science and the 431 Course",
    "text": "1.2 Data Science and the 431 Course\nWe’ll discuss each of these elements in the 431 course, focusing at the start on understanding our data through transformation, modeling and (especially in the early stages) visualization. In 431, we learn how to get things done.\n\nWe get people working with R and R Studio and R Markdown, even if they are completely new to coding. A gentle introduction is provided at Ismay and Kim (2022)\nWe learn how to use the tidyverse (http://www.tidyverse.org/), an array of tools in R (mostly developed by Hadley Wickham and his colleagues at R Studio) which share an underlying philosophy to make data science faster, easier, more reproducible and more fun. A critical text for understanding the tidyverse is Wickham and Grolemund (2022). Tidyverse tools facilitate:\n\nimporting data into R, which can be the source of intense pain for some things, but is really quite easy 95% of the time with the right tool.\ntidying data, that is, storing it in a format that includes one row per observation and one column per variable. This is harder, and more important, than you might think.\ntransforming data, perhaps by identifying specific subgroups of interest, creating new variables based on existing ones, or calculating summaries.\nvisualizing data to generate actual knowledge and identify questions about the data - this is an area where R really shines, and we’ll start with it in class.\nmodeling data, taking the approach that modeling is complementary to visualization, and allows us to answer questions that visualization helps us identify.\nand last, but definitely not least, communicating results, models and visualizations to others, in a way that is reproducible and effective.\n\nSome programming/coding is an inevitable requirement to accomplish all of these aims. If you are leery of coding, you’ll need to get past that, with the help of this course and our stellar teaching assistants. Getting started is always the most challenging part, but our experience is that most of the pain of developing these new skills evaporates by early October."
  },
  {
    "objectID": "01-datascience.html#what-the-course-is-and-isnt",
    "href": "01-datascience.html#what-the-course-is-and-isnt",
    "title": "1  Data Science and 431",
    "section": "1.3 What The Course Is and Isn’t",
    "text": "1.3 What The Course Is and Isn’t\nThe 431 course is about getting things done. In developing this course, we adopt a modern approach that places data at the center of our work. Our goal is to teach you how to do truly reproducible research with modern tools. We want you to be able to collect and use data effectively to address questions of interest.\nThe curriculum includes more on several topics than you might expect from a standard graduate introduction to biostatistics.\n\ndata gathering\ndata wrangling\nexploratory data analysis and visualization\nmultivariate modeling\ncommunication\n\nIt also nearly completely avoids formalism and is extremely applied - this is absolutely not a course in theoretical or mathematical statistics, and these Notes reflect that approach.\nThere’s very little of the mathematical underpinnings here:\n\\[\nf(x) = \\frac{e^{-(x - \\mu)^{2}/(2\\sigma^{2})}}{\\sigma{\\sqrt{2 \\pi }}}\n\\]\nInstead, these notes (and the course) focus on how we get R to do the things we want to do, and how we interpret the results of our work. Our next Chapter provides a first example.\n\n\n\n\nGelman, Andrew, and Deborah Nolan. 2017. Teaching Statistics: A Bag of Tricks. Second Edition. Oxford, UK: Oxford University Press.\n\n\nIsmay, Chester, and Albert Y. Kim. 2022. ModernDive: Statistical Inference via Data Science. http://moderndive.com/.\n\n\nWickham, Hadley, and Garrett Grolemund. 2022. R for Data Science. Second. O’Reilly. https://r4ds.hadley.nz/."
  },
  {
    "objectID": "02-penguins.html",
    "href": "02-penguins.html",
    "title": "2  The Palmer Penguins",
    "section": "",
    "text": "The data in the palmerpenguins package in R includes information on several measurements of interest for adult foraging penguins observed on islands in the Palmer Archipelago near Palmer Station, Antarctica. Dr. Kristen Gorman and the Palmer Station Long Term Ecological Research (LTER) Program collected the data and made it available1. The data describe three species of penguins, called Adelie, Chinstrap and Gentoo.\nFor more on the palmerpenguins package, visit https://allisonhorst.github.io/palmerpenguins/."
  },
  {
    "objectID": "02-penguins.html#setup-packages-used-here",
    "href": "02-penguins.html#setup-packages-used-here",
    "title": "2  The Palmer Penguins",
    "section": "2.1 Setup: Packages Used Here",
    "text": "2.1 Setup: Packages Used Here\nWe will use the palmerpenguins package to supply us with data for this chapter. The janitor packages includes several useful functions, including tabyl. The knitr package includes the kable() function we’ll use. Finally, the tidyverse package will provide the bulk of the functions we’ll use in our work throughout the semester.\nI always load the tidyverse last, because it solves some problems to do so.\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(palmerpenguins) \nlibrary(janitor) \nlibrary(knitr) \nlibrary(gt)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "02-penguins.html#viewing-a-data-set",
    "href": "02-penguins.html#viewing-a-data-set",
    "title": "2  The Palmer Penguins",
    "section": "2.2 Viewing a Data Set",
    "text": "2.2 Viewing a Data Set\nThe penguins data from the palmerpenguins package contains 344 rows and 8 columns. Each row contains data for a different penguin, and each column describes a variable contained in the data set.\n\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema…  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema…  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA <NA>   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema…  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema…  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 <NA>   2007\n10 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n# … with 334 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n# ℹ Use `print(n = ...)` to see more rows\n\n\nFor instance, the first penguin in the data is of the species Adelie (the three species included in the data are Adelie, Chinstrap and Gentoo), and was observed on the island called Torgeson. The remaining data for that penguin include measures of its bill length and depth, its flipper length and body mass, its sex and the year in which it was observed.\nNote that though there are 344 rows in the tibble of data called penguins, only the first ten rows (penguins) are shown in the table above. Note also that the symbol <NA> is used to indicate a missing (not available) value."
  },
  {
    "objectID": "02-penguins.html#create-newpenguins-eliminating-missing-data",
    "href": "02-penguins.html#create-newpenguins-eliminating-missing-data",
    "title": "2  The Palmer Penguins",
    "section": "2.3 Create newpenguins: Eliminating Missing Data",
    "text": "2.3 Create newpenguins: Eliminating Missing Data\nNext, let’s take the penguins data from the palmerpenguins package, and identify those observations which have complete data (so, no missing values) in four variables of interest. We’ll store that result in a new tibble (data set) called new_penguins and then take a look at that result using the following code.\nNote that the code below:\n\nuses the “pipe” |> to send the penguins tibble to the filter() function\nuses <- to assign the result of our work to the new_penguins tibble\nuses the complete.cases() function to remove cases within penguins that have missing data on any of the four variables (flipper_length_mm, body_mass_g, species or sex) that we identify\n\n\nnew_penguins <- penguins |>\n    filter(complete.cases(flipper_length_mm, body_mass_g, species, sex))\n\nnew_penguins\n\n# A tibble: 333 × 8\n   species island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema…  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema…  2007\n 4 Adelie  Torgersen           36.7          19.3        193    3450 fema…  2007\n 5 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 6 Adelie  Torgersen           38.9          17.8        181    3625 fema…  2007\n 7 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 8 Adelie  Torgersen           41.1          17.6        182    3200 fema…  2007\n 9 Adelie  Torgersen           38.6          21.2        191    3800 male   2007\n10 Adelie  Torgersen           34.6          21.1        198    4400 male   2007\n# … with 323 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "02-penguins.html#counting-things-and-making-tables",
    "href": "02-penguins.html#counting-things-and-making-tables",
    "title": "2  The Palmer Penguins",
    "section": "2.4 Counting Things and Making Tables",
    "text": "2.4 Counting Things and Making Tables\nSo, how many penguins are in our new_penguins data? When we printed out the result, we got an answer, but (as with many things in R) there are many ways to get the same result.\n\nnrow(new_penguins)\n\n[1] 333\n\n\nHow do our new_penguins data break down by sex and species? We’ll use the tabyl() function from the janitor package to look at this.\n\nnew_penguins |> \n    tabyl(sex, species) \n\n    sex Adelie Chinstrap Gentoo\n female     73        34     58\n   male     73        34     61\n\n\nThe output is reasonably clear (there are 73 female and 73 male Adelie penguins in the newpenguins tibble, for example) but could we make that table a little prettier, and while we’re at it, can we add the row and column totals?\n\nnew_penguins |> \n    tabyl(sex, species) |>\n    adorn_totals(where = c(\"row\", \"col\")) |> # add row, column totals\n    kable()  # one convenient way to make the table prettier\n\n\n\n\nsex\nAdelie\nChinstrap\nGentoo\nTotal\n\n\n\n\nfemale\n73\n34\n58\n165\n\n\nmale\n73\n34\n61\n168\n\n\nTotal\n146\n68\n119\n333\n\n\n\n\n\nThe kable() function comes from the knitr package we loaded earlier. Notice that we added some comments to the code here with the prefix #. These comments are ignored by R in processing the data.\nWe can switch the rows and columns, and add some additional features, using the code below, which makes use of the gt() and tab_header() functions from the gt package, which is designed to help build complex tables. More on the incredibly versatile gt() package is available at https://gt.rstudio.com/.\n\nnew_penguins |> \n    tabyl(species, sex) |>\n    adorn_totals(where = c(\"row\", \"col\")) |> \n    gt() |>\n    tab_header(\n      title = md(\"Palmer Penguins in **newpenguins**\"),\n      subtitle = \"Comparing sexes by species\"\n    )\n\n\n\n\n\n  \n    \n      Palmer Penguins in newpenguins\n    \n    \n      Comparing sexes by species\n    \n  \n  \n    \n      species\n      female\n      male\n      Total\n    \n  \n  \n    Adelie\n73\n73\n146\n    Chinstrap\n34\n34\n68\n    Gentoo\n58\n61\n119\n    Total\n165\n168\n333"
  },
  {
    "objectID": "02-penguins.html#creating-a-scatterplot",
    "href": "02-penguins.html#creating-a-scatterplot",
    "title": "2  The Palmer Penguins",
    "section": "2.5 Creating a Scatterplot",
    "text": "2.5 Creating a Scatterplot\nNow, let’s look at the other two variables of interest. Let’s create a graph showing the association of body mass with flipper length across the complete set of 333 penguins.\n\nggplot(new_penguins, aes(x = body_mass_g, y = flipper_length_mm)) +\n    geom_point() \n\n\n\n\nSome of you may want to include a straight-line model (fit by a classical linear regression) to this plot. One way to do that in R involves the addition of a single line of code, like this:\n\nggplot(new_penguins, aes(x = body_mass_g, y = flipper_length_mm)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x,\n                col = \"red\", se = FALSE)\n\n\n\n\nWhenever we build a graph for ourselves, these default choices may be sufficient. But I’d like to see a prettier version if I was going to show it to someone else. So, I might use a different color for each species, and I might add a title, like this.\n\nggplot(new_penguins, aes(x = body_mass_g, y = flipper_length_mm, col = species)) +\n    geom_point() + \n    labs(title = \"Flipper Length and Body Mass for 333 of the Palmer Penguins\")"
  },
  {
    "objectID": "02-penguins.html#six-ways-to-improve-this-graph",
    "href": "02-penguins.html#six-ways-to-improve-this-graph",
    "title": "2  The Palmer Penguins",
    "section": "2.6 Six Ways To “Improve” This Graph",
    "text": "2.6 Six Ways To “Improve” This Graph\nNow, let’s build a new graph to incorporate some additional information and improve the appearance. Here, I want to:\n\nplot the relationship between body mass and flipper length in light of both Sex and Species\nincrease the size of the points and add a little transparency so we can see if points overlap,\nadd some smooth curves to summarize the relationships between the two quantities (body mass and flipper length) within each combination of species and sex,\nsplit the graph into two “facets” (one for each sex),\nimprove the axis labels,\nimprove the titles by adding a subtitle, and also adding in some code to count the penguins (rather than hard-coding in the total number.)\n\n\nggplot(new_penguins, aes(x = body_mass_g, y = flipper_length_mm, \n                         col = species)) +\n    geom_point(size = 2, alpha = 0.5) + \n    geom_smooth(method = \"loess\", formula = y ~ x, \n                se = FALSE, size = 1.5) +\n    facet_grid(~ sex) +\n    labs(title = \"Flipper Length and Body Mass, by Sex & Species\",\n         subtitle = str_glue(nrow(new_penguins), \" of the Palmer Penguins\"),\n         x = \"Body Mass (g)\", \n         y = \"Flipper Length (mm)\")"
  },
  {
    "objectID": "02-penguins.html#a-little-reflection",
    "href": "02-penguins.html#a-little-reflection",
    "title": "2  The Palmer Penguins",
    "section": "2.7 A Little Reflection",
    "text": "2.7 A Little Reflection\nWhat can we learn from these plots and their construction? In particular,\n\nWhat do these plots suggest about the center of the distribution of each quantity (body mass and flipper length) overall, and within each combination of Sex and Species?\nWhat does the final plot suggest about the spread of the distribution of each of those quantities in each combination of Sex and Species?\nWhat do the plots suggest about the association of body mass and flipper length across the complete set of penguins?\nHow does the shape and nature of this body mass - flipper length relationship change based on Sex and Species?\nDo you think it would be helpful to plot a straight-line relationship (rather than a smooth curve) within each combination of Sex and Species in the final plot? Why or why not? (Also, what would we have to do to the code to accomplish this?)\nHow was the R code for the plot revised to accomplish each of the six “wants” specified above?"
  },
  {
    "objectID": "03-nhanes.html",
    "href": "03-nhanes.html",
    "title": "3  NHANES: A First Look",
    "section": "",
    "text": "Next, we’ll explore some data from the US National Health and Nutrition Examination Survey, or NHANES.\nWe’ll display R code as we go, but we’ll return to all of the key coding ideas involved later in the Notes."
  },
  {
    "objectID": "03-nhanes.html#setup-packages-used-here",
    "href": "03-nhanes.html#setup-packages-used-here",
    "title": "3  NHANES: A First Look",
    "section": "3.1 Setup: Packages Used Here",
    "text": "3.1 Setup: Packages Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(NHANES)\nlibrary(patchwork)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "03-nhanes.html#the-nhanes-data-a-first-sample",
    "href": "03-nhanes.html#the-nhanes-data-a-first-sample",
    "title": "3  NHANES: A First Look",
    "section": "3.2 The NHANES data: A First Sample",
    "text": "3.2 The NHANES data: A First Sample\nThe NHANES package provides a sample of 10,000 NHANES responses from the 2009-10 and 2011-12 administrations, in a data frame also called NHANES. We can obtain the dimensions of this data frame (think of it as a rectangle of data) with the dim() function.\n\ndim(NHANES)\n\n[1] 10000    76\n\n\nWe see that we have 10000 rows and 76 columns in the NHANES data frame.\nFor the moment, let’s gather a random sample of 1,000 responses from the 10000 rows listed in the NHANES data frame, and then look at three variables (labeled Gender, Age and Height) that describe those subjects1. Some of the motivation for this example came from a Figure in Baumer, Kaplan, and Horton (2017).\n\n# library(NHANES) # already loaded NHANES package/library of functions, data\n\nset.seed(431001) \n# use set.seed to ensure that we all get the same random sample \n# of 1,000 NHANES subjects in our nh_1 collection\n\nnh_1 <- \n    slice_sample(NHANES, n = 1000, replace = FALSE) |>\n    select(ID, SurveyYr, Gender, Age, Height)\n\nnh_1\n\n# A tibble: 1,000 × 5\n      ID SurveyYr Gender   Age Height\n   <int> <fct>    <fct>  <int>  <dbl>\n 1 69638 2011_12  female     5   106.\n 2 70782 2011_12  male      64   176.\n 3 52408 2009_10  female    54   162.\n 4 59031 2009_10  female    15   155.\n 5 64530 2011_12  male      53   185.\n 6 71040 2011_12  male      63   169.\n 7 55186 2009_10  female    30   168.\n 8 60211 2009_10  male       5   103.\n 9 55730 2009_10  male      66   161.\n10 68229 2011_12  female    36   170.\n# … with 990 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nWe have 1000 rows (observations) and 5 columns (variables) that describe the responses listed in the rows."
  },
  {
    "objectID": "03-nhanes.html#a-quick-numerical-summary",
    "href": "03-nhanes.html#a-quick-numerical-summary",
    "title": "3  NHANES: A First Look",
    "section": "3.3 A Quick Numerical Summary",
    "text": "3.3 A Quick Numerical Summary\n\nsummary(nh_1)\n\n       ID           SurveyYr      Gender         Age            Height     \n Min.   :51624   2009_10:512   female:504   Min.   : 0.00   Min.   : 85.0  \n 1st Qu.:57011   2011_12:488   male  :496   1st Qu.:18.00   1st Qu.:156.2  \n Median :61979                              Median :36.00   Median :165.0  \n Mean   :61903                              Mean   :37.42   Mean   :162.3  \n 3rd Qu.:67178                              3rd Qu.:56.00   3rd Qu.:174.5  \n Max.   :71875                              Max.   :80.00   Max.   :195.9  \n                                                            NA's   :37     \n\n\nFor the two variables that R recognizes as describing categories, SurveyYr and Gender, this numeric summary provides a small table of counts. For the Age and Height variables, we see the minimum, mean, maximum and other summary statistics."
  },
  {
    "objectID": "03-nhanes.html#plotting-age-vs.-height",
    "href": "03-nhanes.html#plotting-age-vs.-height",
    "title": "3  NHANES: A First Look",
    "section": "3.4 Plotting Age vs. Height",
    "text": "3.4 Plotting Age vs. Height\nSuppose we want to visualize the relationship of Height and Age in our 1,000 NHANES observations. The best choice is likely to be a scatterplot.\n\nggplot(data = nh_1, aes(x = Age, y = Height)) +\n    geom_point()\n\nWarning: Removed 37 rows containing missing values (geom_point).\n\n\n\n\n\nWe note several interesting results here.\n\nAs a warning, R tells us that it has “Removed 37 rows containing missing values (geom_point).” Only 963 subjects plotted here, because the remaining 37 people have missing (NA) values for either Height, Age or both.\nUnsurprisingly, the measured Heights of subjects grow from Age 0 to Age 20 or so, and we see that a typical Height increases rapidly across these Ages. The middle of the distribution at later Ages is pretty consistent at at a Height somewhere between 150 and 175. The units aren’t specified, but we expect they must be centimeters. The Ages are clearly reported in Years.\nNo Age is reported over 80, and it appears that there is a large cluster of Ages at 80. This may be due to a requirement that Ages 80 and above be reported at 80 so as to help mask the identity of those individuals.2\n\nAs in this case, we’re going to build most of our visualizations using tools from the ggplot2 package, which is part of the tidyverse series of packages. You’ll see similar coding structures throughout this Chapter, most of which are covered as well in Chapter 3 of Wickham and Grolemund (2022)."
  },
  {
    "objectID": "03-nhanes.html#restriction-to-complete-cases",
    "href": "03-nhanes.html#restriction-to-complete-cases",
    "title": "3  NHANES: A First Look",
    "section": "3.5 Restriction to Complete Cases",
    "text": "3.5 Restriction to Complete Cases\nBefore we move on, let’s manipulate the data frame a bit, to focus on only those subjects who have complete data on both Age and Height. This will help us avoid that warning message.\n\nnh_1cc <- nh_1 |>\n    filter(complete.cases(Age, Height)) \n\nsummary(nh_1cc)\n\n       ID           SurveyYr      Gender         Age            Height     \n Min.   :51624   2009_10:487   female:484   Min.   : 2.00   Min.   : 85.0  \n 1st Qu.:57034   2011_12:476   male  :479   1st Qu.:19.00   1st Qu.:156.2  \n Median :62056                              Median :37.00   Median :165.0  \n Mean   :61967                              Mean   :38.29   Mean   :162.3  \n 3rd Qu.:67269                              3rd Qu.:56.00   3rd Qu.:174.5  \n Max.   :71875                              Max.   :80.00   Max.   :195.9  \n\n\nNote that the units and explanations for these variables are contained in the NHANES help file, available via typing ?NHANES in the Console of R Studio, or by typing NHANES into the Search bar in R Studio’s Help window."
  },
  {
    "objectID": "03-nhanes.html#the-distinction-between-gender-and-sex",
    "href": "03-nhanes.html#the-distinction-between-gender-and-sex",
    "title": "3  NHANES: A First Look",
    "section": "3.6 The Distinction between Gender and Sex",
    "text": "3.6 The Distinction between Gender and Sex\nThe Gender variable here is mis-named. These data refer to the biological status of these subjects, which is their Sex, and not the social construct of Gender which can be quite different. In our effort to avoid further confusion, we’ll rename the variable Gender to Sex so as to more accurately describe what is actually measured here.\nTo do this, we can use this approach…\n\nnh_1cc <- nh_1 |>\n    rename(Sex = Gender) |>\n    filter(complete.cases(Age, Height)) \n\nsummary(nh_1cc)\n\n       ID           SurveyYr       Sex           Age            Height     \n Min.   :51624   2009_10:487   female:484   Min.   : 2.00   Min.   : 85.0  \n 1st Qu.:57034   2011_12:476   male  :479   1st Qu.:19.00   1st Qu.:156.2  \n Median :62056                              Median :37.00   Median :165.0  \n Mean   :61967                              Mean   :38.29   Mean   :162.3  \n 3rd Qu.:67269                              3rd Qu.:56.00   3rd Qu.:174.5  \n Max.   :71875                              Max.   :80.00   Max.   :195.9  \n\n\nThat’s better. How many observations do we have now? We could use dim to find out the number of rows and columns in this new data frame.\n\ndim(nh_1cc)\n\n[1] 963   5\n\n\nOr, we could simply list the data frame and read off the result.\n\nnh_1cc\n\n# A tibble: 963 × 5\n      ID SurveyYr Sex      Age Height\n   <int> <fct>    <fct>  <int>  <dbl>\n 1 69638 2011_12  female     5   106.\n 2 70782 2011_12  male      64   176.\n 3 52408 2009_10  female    54   162.\n 4 59031 2009_10  female    15   155.\n 5 64530 2011_12  male      53   185.\n 6 71040 2011_12  male      63   169.\n 7 55186 2009_10  female    30   168.\n 8 60211 2009_10  male       5   103.\n 9 55730 2009_10  male      66   161.\n10 68229 2011_12  female    36   170.\n# … with 953 more rows\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "03-nhanes.html#age-height-by-sex",
    "href": "03-nhanes.html#age-height-by-sex",
    "title": "3  NHANES: A First Look",
    "section": "3.7 Age-Height by Sex?",
    "text": "3.7 Age-Height by Sex?\nLet’s add Sex to the plot using color, and also adjust the y axis label to incorporate the units of measurement.\n\nggplot(data = nh_1cc, aes(x = Age, y = Height, color = Sex)) +\n    geom_point() +\n    labs(title = \"Height-Age Relationship in NHANES sample\", \n         y = \"Height in cm.\")\n\n\n\n\n\n3.7.1 Can we show the Female and Male relationships in separate panels?\nSure.\n\nggplot(data = nh_1cc, aes(x = Age, y = Height, color = Sex)) +\n    geom_point() + \n    labs(title = \"Height-Age Relationship in NHANES sample\", \n         y = \"Height in cm.\") +\n    facet_wrap(~ Sex)\n\n\n\n\n\n\n3.7.2 Can we add a smooth curve to show the relationship in each plot?\nYes, by adding a call to the geom_smooth() function.\n\nggplot(data = nh_1cc, aes(x = Age, y = Height, color = Sex)) +\n    geom_point() + \n    geom_smooth(method = \"loess\", formula = y ~ x) +\n    labs(title = \"Height-Age Relationship in NHANES sample\", \n         y = \"Height in cm.\") +\n    facet_wrap(~ Sex)\n\n\n\n\n\n\n3.7.3 What if we want to assume straight line relationships?\nWe could look at a linear model in each part of the plot instead. Does this make sense here?\n\nggplot(data = nh_1cc, aes(x = Age, y = Height, color = Sex)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", formula = y ~ x) +\n    labs(title = \"Height-Age Relationship in NHANES sample\", \n         y = \"Height in cm.\") +\n    facet_wrap(~ Sex)\n\n\n\n\nIt seems like the more complex relationship between Height and Age isn’t well described by the straight line model."
  },
  {
    "objectID": "03-nhanes.html#combining-plots-with-patchwork",
    "href": "03-nhanes.html#combining-plots-with-patchwork",
    "title": "3  NHANES: A First Look",
    "section": "3.8 Combining Plots with patchwork",
    "text": "3.8 Combining Plots with patchwork\nThe patchwork package in R allows us to use some simple commands to put two plots together.\nSuppose we create plots called p1 and p2, as follows.\n\np1 <- ggplot(data = nh_1cc, aes(x = Age, y = Height)) +\n    geom_point() + \n    labs(title = \"Height and Age\")\n\np2 <- ggplot(data = nh_1cc, aes(x = Sex, y = Height)) +\n    geom_point() +\n    labs(title = \"Height, by Sex\")\n\nNow, suppose we want to put them together in a single figure. Thanks to patchwork, we can simply type in the following.\n\np1 / p2\n\n\n\n\nor we can place the images next to each other, and add an annotation, like this:\n\np1 + p2 +\n    plot_annotation(title = \"Our Combined Plots\")\n\n\n\n\nThe patchwork package website provides lots of great examples and guides to make it very easy to combine separate ggplots into the same graphic. While there are other packages (gridExtra and cowplot are very nice, for instance) to do this task, I think patchwork is the most user-friendly, so that’s the focus of these notes."
  },
  {
    "objectID": "03-nhanes.html#coming-up",
    "href": "03-nhanes.html#coming-up",
    "title": "3  NHANES: A First Look",
    "section": "3.9 Coming Up",
    "text": "3.9 Coming Up\nNext, we’ll select a new sample of NHANES respondents a bit more carefully, introduce some new ways of thinking about data and variables, then we’ll study those subjects in greater detail.\n\n\n\n\nBaumer, Benjamin S., Daniel T. Kaplan, and Nicholas J. Horton. 2017. Modern Data Science with r. Boca Raton, FL: CRC Press. https://mdsr-book.github.io/.\n\n\nWickham, Hadley, and Garrett Grolemund. 2022. R for Data Science. Second. O’Reilly. https://r4ds.hadley.nz/."
  },
  {
    "objectID": "04-data_types.html",
    "href": "04-data_types.html",
    "title": "4  Data Structures, Variable Types & Sampling NHANES",
    "section": "",
    "text": "knitr::opts_chunk$set(comment = NA)\n\nlibrary(NHANES)\nlibrary(janitor)\nlibrary(naniar)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "04-data_types.html#data-require-structure-and-context",
    "href": "04-data_types.html#data-require-structure-and-context",
    "title": "4  Data Structures, Variable Types & Sampling NHANES",
    "section": "4.2 Data require structure and context",
    "text": "4.2 Data require structure and context\nDescriptive statistics are concerned with the presentation, organization and summary of data, as suggested in Norman and Streiner (2014). This includes various methods of organizing and graphing data to get an idea of what those data can tell us.\nAs Vittinghoff et al. (2012) suggest, the nature of the measurement determines how best to describe it statistically, and the main distinction is between numerical and categorical variables. Even this is a little tricky - plenty of data can have values that look like numerical values, but are just numerals serving as labels.\nAs Bock, Velleman, and De Veaux (2004) point out, the truly critical notion, of course, is that data values, no matter what kind, are useless without their contexts. The Five W’s (Who, What [and in what units], When, Where, Why, and often How) are just as useful for establishing the context of data as they are in journalism. If you can’t answer Who and What, in particular, you don’t have any useful information.\nIn general, each row of a data frame corresponds to an individual (respondent, experimental unit, record, or observation) about whom some characteristics are gathered in columns (and these characteristics may be called variables, factors or data elements.) Every column / variable should have a name that indicates what it is measuring, and every row / observation should have a name that indicates who is being measured."
  },
  {
    "objectID": "04-data_types.html#newNHANES",
    "href": "04-data_types.html#newNHANES",
    "title": "4  Data Structures, Variable Types & Sampling NHANES",
    "section": "4.3 Sampling Adults in NHANES",
    "text": "4.3 Sampling Adults in NHANES\nIn Chapter 3, we spent some time with a sample from the National Health and Nutrition Examination. Now, by changing the value of the set.seed function which determines the starting place for the random sampling, and changing some other specifications, we’ll generate a new sample describing 750 unique (distinct) adult subjects who completed the 2011-12 version of the survey when they were between the ages of 21 and 64.\n\n4.3.1 Creating a Temporary, Cleaner Data Frame\nI’ll start by describing the plan we will use to create a new data frame called nh_temp from which we will eventually build our final sample. In particular, let me lay out the steps I will use to create the nh_temp frame from the original NHANES data frame available in the R package called NHANES.\n\nWe’ll filter the original NHANES data frame to include only the responses from the 2011-12 administration of the survey. This will cut the sample in half, from 10,000 rows to 5,000.\nWe’ll then filter again to restrict the sample to adults whose age is at least 21 and also less than 65. I’ll do this because I want to avoid problems with including both children and adults in my sample, and because I also want to focus on the population of people in the US who are usually covered by private insurance from their job, or by Medicaid insurance from the government, rather than those covered by Medicare.\nAs we discussed previously, what is listed in the NHANES data frame as Gender should be more correctly referred to as Sex. Sex is a biological feature of an individual, while Gender is a social construct. This is an important distinction, so I’ll change the name of the variable.\nWe’ll also rename three other variables, specifically we’ll use Race to describe the Race3 variable in the original NHANES data frame, as well as SBP to refer to the average systolic blood pressure, which is specified as BPSysAve, and DBP to refer to the average diastolic blood pressure, which is specified as BPDiaAve.\nHaving accomplished the previous four steps, we’ll then select the variables we want to keep in the sample. (We use select for choosing variables or columns in the data frame, and filter for selecting subjects or rows.) The sixteen variables we will select are: ID, Sex, Age, Height, Weight, Race, Education, BMI, SBP, DBP, Pulse, PhysActive, Smoke100, SleepTrouble, MaritalStatus and HealthGen.\nThe original NHANES data frame includes some subjects (rows) multiple times in an effort to incorporate some of the sampling weights used in most NHANES analyses. For our purposes, though, we’d like to only include each subject one time. We use the distinct() function to limit the data frame to completely unique subjects (so that, for example, we don’t wind up with two or more rows that have the same ID number.)\n\nHere is the code I used to complete the six steps listed above and create the nh_temp data frame.\n\nnh_temp <- NHANES |>\n    filter(SurveyYr == \"2011_12\") |>\n    filter(Age >= 21 & Age < 65) |>\n    rename(Sex = Gender, Race = Race3, SBP = BPSysAve, DBP = BPDiaAve) |>\n    select(ID, Sex, Age, Height, Weight, Race, Education, BMI, SBP, DBP, \n           Pulse, PhysActive, Smoke100, SleepTrouble, \n           MaritalStatus, HealthGen) |>\n   distinct()\n\nThe resulting nh_temp data frame has 1700 rows and 16 columns.\n\nnh_temp\n\n# A tibble: 1,700 × 16\n      ID Sex     Age Height Weight Race  Educa…¹   BMI   SBP   DBP Pulse PhysA…²\n   <int> <fct> <int>  <dbl>  <dbl> <fct> <fct>   <dbl> <int> <int> <int> <fct>  \n 1 62172 fema…    43   172    98.6 Black High S…  33.3   103    72    80 No     \n 2 62176 fema…    34   172.   68.7 White Colleg…  23.3   107    69    92 Yes    \n 3 62180 male     35   179.   89   White Colleg…  27.9   107    66    66 No     \n 4 62199 male     57   186    96.9 White Colleg…  28     110    65    84 Yes    \n 5 62205 male     28   171.   84.8 White Colleg…  28.9   122    87    70 Yes    \n 6 62206 fema…    35   167.   81.5 White Some C…  29.1   106    50    58 No     \n 7 62208 male     38   169.   63.2 Hisp… Some C…  22.2   105    59    52 Yes    \n 8 62209 fema…    62   143.   53.5 Mexi… 8th Gr…  26     108    57    72 No     \n 9 62220 fema…    31   167.  113.  Black Colleg…  40.4   120    71    62 Yes    \n10 62222 male     32   179    80.1 White Colleg…  25     104    73    78 No     \n# … with 1,690 more rows, 4 more variables: Smoke100 <fct>, SleepTrouble <fct>,\n#   MaritalStatus <fct>, HealthGen <fct>, and abbreviated variable names\n#   ¹​Education, ²​PhysActive\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\n\n\n4.3.2 Sampling nh_temp to obtain our nh_adult750 sample\nHaving established the nh_temp sampling frame, we now select a random sample of 750 adults from the 1700 available responses.\n\nWe will use the set.seed() function in R to set a random numerical seed to ensure that if you redo this work, you will obtain the same sample.\n\nSetting a seed is an important part of being able to replicate the work later when sampling is involved.\n\nThen we will use the slice_sample() function to actually draw the random sample, without replacement.\n\n“Without replacement” means that once we’ve selected a particular subject, we won’t select them again.\n\n\n\nset.seed(431002) \n# use set.seed to ensure that we all get the same random sample \n\nnh_adult750 <- slice_sample(nh_temp, n = 750, replace = F) \n\nnh_adult750\n\n# A tibble: 750 × 16\n      ID Sex     Age Height Weight Race  Educa…¹   BMI   SBP   DBP Pulse PhysA…²\n   <int> <fct> <int>  <dbl>  <dbl> <fct> <fct>   <dbl> <int> <int> <int> <fct>  \n 1 68648 fema…    30   181.   67.1 White Colleg…  20.4   103    59    78 No     \n 2 67200 male     30   180.   86.6 White Colleg…  26.7   113    68    70 Yes    \n 3 66404 fema…    35   160.   71.1 White Colleg…  27.8   116    80    68 Yes    \n 4 70535 male     40   177.   82   White Colleg…  26.3   130    79    68 No     \n 5 65308 fema…    54   151.   60.6 Mexi… 8th Gr…  26.6   130    64    48 No     \n 6 67392 male     41   171.   90.7 Hisp… Colleg…  31.2   124    82    68 Yes    \n 7 63218 male     35   163.   81   Mexi… 8th Gr…  30.3   128    96    82 No     \n 8 65879 fema…    32   160.   66.4 Mexi… Colleg…  25.9   104    70    78 Yes    \n 9 63617 male     29   189.   83.3 White Colleg…  23.2   105    72    76 Yes    \n10 64720 male     29   174.   62.3 Black Colleg…  20.6   127    60    84 Yes    \n# … with 740 more rows, 4 more variables: Smoke100 <fct>, SleepTrouble <fct>,\n#   MaritalStatus <fct>, HealthGen <fct>, and abbreviated variable names\n#   ¹​Education, ²​PhysActive\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nThe nh_adult750 data frame now includes 750 rows (observations) on 16 variables (columns). Essentially, we have 16 pieces of information on each of 750 adult NHANES subjects who were included in the 2011-12 panel.\n\n\n4.3.3 Summarizing the Data’s Structure\nWe can identify the number of rows and columns in a data frame or tibble with the dim function.\n\ndim(nh_adult750)\n\n[1] 750  16\n\n\nThe str function provides a lot of information about the structure of a data frame or tibble.\n\nstr(nh_adult750)\n\ntibble [750 × 16] (S3: tbl_df/tbl/data.frame)\n $ ID           : int [1:750] 68648 67200 66404 70535 65308 67392 63218 65879 63617 64720 ...\n $ Sex          : Factor w/ 2 levels \"female\",\"male\": 1 2 1 2 1 2 2 1 2 2 ...\n $ Age          : int [1:750] 30 30 35 40 54 41 35 32 29 29 ...\n $ Height       : num [1:750] 181 180 160 177 151 ...\n $ Weight       : num [1:750] 67.1 86.6 71.1 82 60.6 90.7 81 66.4 83.3 62.3 ...\n $ Race         : Factor w/ 6 levels \"Asian\",\"Black\",..: 5 5 5 5 4 3 4 4 5 2 ...\n $ Education    : Factor w/ 5 levels \"8th Grade\",\"9 - 11th Grade\",..: 5 5 5 5 1 5 1 5 5 5 ...\n $ BMI          : num [1:750] 20.4 26.7 27.8 26.3 26.6 31.2 30.3 25.9 23.2 20.6 ...\n $ SBP          : int [1:750] 103 113 116 130 130 124 128 104 105 127 ...\n $ DBP          : int [1:750] 59 68 80 79 64 82 96 70 72 60 ...\n $ Pulse        : int [1:750] 78 70 68 68 48 68 82 78 76 84 ...\n $ PhysActive   : Factor w/ 2 levels \"No\",\"Yes\": 1 2 2 1 1 2 1 2 2 2 ...\n $ Smoke100     : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 2 2 1 2 1 2 2 ...\n $ SleepTrouble : Factor w/ 2 levels \"No\",\"Yes\": 2 1 1 1 1 1 1 1 2 1 ...\n $ MaritalStatus: Factor w/ 6 levels \"Divorced\",\"LivePartner\",..: 3 4 3 3 2 3 3 3 3 2 ...\n $ HealthGen    : Factor w/ 5 levels \"Excellent\",\"Vgood\",..: 1 1 1 2 4 3 NA 1 2 4 ...\n\n\nTo see the first few observations, use head, and to see the last few, try tail…\n\ntail(nh_adult750, 5) # shows the last five observations in the data set\n\n# A tibble: 5 × 16\n     ID Sex      Age Height Weight Race  Educa…¹   BMI   SBP   DBP Pulse PhysA…²\n  <int> <fct>  <int>  <dbl>  <dbl> <fct> <fct>   <dbl> <int> <int> <int> <fct>  \n1 63924 female    29   165.  113.  Black High S…  41.9    98    56    74 No     \n2 69825 female    43   164.   63.3 White Colleg…  23.7   122    83    88 Yes    \n3 68109 male      45   170.   78.7 Black High S…  27.1   140    79   102 Yes    \n4 64598 female    60   158    74.5 White Some C…  29.8   137    80    78 Yes    \n5 64048 female    54   161.   67.5 White Some C…  26.2   121    87    72 No     \n# … with 4 more variables: Smoke100 <fct>, SleepTrouble <fct>,\n#   MaritalStatus <fct>, HealthGen <fct>, and abbreviated variable names\n#   ¹​Education, ²​PhysActive\n# ℹ Use `colnames()` to see all variable names\n\n\n\n\n4.3.4 What are the variables?\nWe can use the glimpse function to get a short preview of the data.\n\nglimpse(nh_adult750)\n\nRows: 750\nColumns: 16\n$ ID            <int> 68648, 67200, 66404, 70535, 65308, 67392, 63218, 65879, …\n$ Sex           <fct> female, male, female, male, female, male, male, female, …\n$ Age           <int> 30, 30, 35, 40, 54, 41, 35, 32, 29, 29, 64, 28, 31, 59, …\n$ Height        <dbl> 181.3, 180.2, 159.8, 176.6, 150.9, 170.6, 163.4, 160.2, …\n$ Weight        <dbl> 67.1, 86.6, 71.1, 82.0, 60.6, 90.7, 81.0, 66.4, 83.3, 62…\n$ Race          <fct> White, White, White, White, Mexican, Hispanic, Mexican, …\n$ Education     <fct> College Grad, College Grad, College Grad, College Grad, …\n$ BMI           <dbl> 20.4, 26.7, 27.8, 26.3, 26.6, 31.2, 30.3, 25.9, 23.2, 20…\n$ SBP           <int> 103, 113, 116, 130, 130, 124, 128, 104, 105, 127, 128, 1…\n$ DBP           <int> 59, 68, 80, 79, 64, 82, 96, 70, 72, 60, 74, 76, 82, 66, …\n$ Pulse         <int> 78, 70, 68, 68, 48, 68, 82, 78, 76, 84, 62, 56, 78, 66, …\n$ PhysActive    <fct> No, Yes, Yes, No, No, Yes, No, Yes, Yes, Yes, Yes, No, N…\n$ Smoke100      <fct> No, Yes, No, Yes, Yes, No, Yes, No, Yes, Yes, No, No, Ye…\n$ SleepTrouble  <fct> Yes, No, No, No, No, No, No, No, Yes, No, No, Yes, No, Y…\n$ MaritalStatus <fct> Married, NeverMarried, Married, Married, LivePartner, Ma…\n$ HealthGen     <fct> Excellent, Excellent, Excellent, Vgood, Fair, Good, NA, …\n\n\nThe variables we have collected are described in the brief table below1.\n\n\n\n\n\n\n\n\nVariable\nDescription\nSample Values\n\n\n\n\nID\na numerical code identifying the subject\n68648, 67200\n\n\nSex\nsex of subject (2 levels)\nfemale, male\n\n\nAge\nage (years) at screening of subject\n30, 35\n\n\nHeight\nheight (in cm) at screening of subject\n181.3, 180.2\n\n\nWeight\nweight (in kg) at screening of subject\n67.1, 86.6\n\n\nRace\nreported race of subject (6 levels)\nWhite, Black\n\n\nEducation\neducational level of subject (5 levels)\nCollege Grad, High School\n\n\nBMI\nbody-mass index, in kg/m2\n20.4, 26.7\n\n\nSBP\nsystolic blood pressure in mm Hg\n103, 113\n\n\nDBP\ndiastolic blood pressure in mm Hg\n59, 68\n\n\nPulse\n60 second pulse rate in beats per minute\n78, 70\n\n\nPhysActive\nModerate or vigorous-intensity sports?\nYes, No\n\n\nSmoke100\nSmoked at least 100 cigarettes lifetime?\nYes, No\n\n\nSleepTrouble\nTold a doctor they have trouble sleeping?\nYes, No\n\n\nMaritalStatus\nMarital Status\nMarried, Divorced\n\n\nHealthGen\nSelf-report general health rating (5 levels)\nVgood, Fair\n\n\n\nThe levels for the multi-categorical variables are:\n\nRace: Mexican, Hispanic, White, Black, Asian, or Other.\nEducation: 8th Grade, 9 - 11th Grade, High School, Some College, or College Grad.\nMaritalStatus: Married, Widowed, Divorced, Separated, NeverMarried or LivePartner (living with partner).\nHealthGen: Excellent, Vgood, Good, Fair or Poor.\n\nSome details can be obtained using the summary function.\n\nsummary(nh_adult750)\n\n       ID            Sex           Age            Height          Weight      \n Min.   :62206   female:388   Min.   :21.00   Min.   :142.4   Min.   : 39.30  \n 1st Qu.:64277   male  :362   1st Qu.:30.00   1st Qu.:161.8   1st Qu.: 67.40  \n Median :66925                Median :40.00   Median :168.9   Median : 80.00  \n Mean   :66936                Mean   :40.82   Mean   :168.9   Mean   : 83.16  \n 3rd Qu.:69414                3rd Qu.:51.00   3rd Qu.:175.7   3rd Qu.: 95.30  \n Max.   :71911                Max.   :64.00   Max.   :200.4   Max.   :198.70  \n                                              NA's   :5       NA's   :5       \n       Race              Education        BMI             SBP       \n Asian   : 70   8th Grade     : 50   Min.   :16.70   Min.   : 83.0  \n Black   :128   9 - 11th Grade: 76   1st Qu.:24.20   1st Qu.:108.0  \n Hispanic: 63   High School   :143   Median :27.90   Median :118.0  \n Mexican : 80   Some College  :241   Mean   :29.08   Mean   :118.8  \n White   :393   College Grad  :240   3rd Qu.:32.10   3rd Qu.:127.0  \n Other   : 16                        Max.   :80.60   Max.   :209.0  \n                                     NA's   :5       NA's   :33     \n      DBP             Pulse        PhysActive Smoke100  SleepTrouble\n Min.   :  0.00   Min.   : 40.00   No :326    No :453   No :555     \n 1st Qu.: 66.00   1st Qu.: 66.00   Yes:424    Yes:297   Yes:195     \n Median : 73.00   Median : 72.00                                    \n Mean   : 72.69   Mean   : 73.53                                    \n 3rd Qu.: 80.00   3rd Qu.: 80.00                                    \n Max.   :108.00   Max.   :124.00                                    \n NA's   :33       NA's   :32                                        \n      MaritalStatus     HealthGen  \n Divorced    : 78   Excellent: 84  \n LivePartner : 70   Vgood    :197  \n Married     :388   Good     :252  \n NeverMarried:179   Fair     :104  \n Separated   : 19   Poor     : 14  \n Widowed     : 16   NA's     : 99  \n                                   \n\n\nNote the appearance of NA's (indicating missing values) in some columns, and that some variables are summarized by a list of their (categorical) values (with counts) and some (quantitative/numeric) variables are summarized with a minimum, quartiles and means."
  },
  {
    "objectID": "04-data_types.html#quantitative-variables",
    "href": "04-data_types.html#quantitative-variables",
    "title": "4  Data Structures, Variable Types & Sampling NHANES",
    "section": "4.4 Quantitative Variables",
    "text": "4.4 Quantitative Variables\nVariables recorded in numbers that we use as numbers are called quantitative. Familiar examples include incomes, heights, weights, ages, distances, times, and counts. All quantitative variables have measurement units, which tell you how the quantitative variable was measured. Without units (like miles per hour, angstroms, yen or degrees Celsius) the values of a quantitative variable have no meaning.\n\nIt does little good to be told the price of something if you don’t know the currency being used.\nYou might be surprised to see someone whose age is 72 listed in a database on childhood diseases until you find out that age is measured in months.\nOften just seeking the units can reveal a variable whose definition is challenging - just how do we measure “friendliness”, or “success,” for example.\nQuantitative variables may also be classified by whether they are continuous or can only take on a discrete set of values. Continuous data may take on any value, within a defined range. Suppose we are measuring height. While height is really continuous, our measuring stick usually only lets us measure with a certain degree of precision. If our measurements are only trustworthy to the nearest centimeter with the ruler we have, we might describe them as discrete measures. But we could always get a more precise ruler. The measurement divisions we make in moving from a continuous concept to a discrete measurement are usually fairly arbitrary. Another way to think of this, if you enjoy music, is that, as suggested in Norman and Streiner (2014), a piano is a discrete instrument, but a violin is a continuous one, enabling finer distinctions between notes than the piano is capable of making. Sometimes the distinction between continuous and discrete is important, but usually, it’s not.\n\nThe nh_adult750 data includes several quantitative variables, specifically Age, Height, BMI,SBP,DBPandPulse`.\nWe know these are quantitative because they have units: Age in years, Height in centimeters, BMI in kg/m2, the BP measurements in mm Hg, and Pulse in beats per minute.\nDepending on the context, we would likely treat most of these as discrete given that are measurements are fairly crude (this is certainly true for Age, measured in years) although BMI is probably continuous in most settings, even though it is a function of two other measures (Height and Weight) which are rounded off to integer numbers of centimeters and kilograms, respectively.\n\nIt is also possible to separate out quantitative variables into ratio variables or interval variables. An interval variable has equal distances between values, but the zero point is arbitrary. A ratio variable has equal intervals between values, and a meaningful zero point. For example, weight is an example of a ratio variable, while IQ is an example of an interval variable. We all know what zero weight is. An intelligence score like IQ is a different matter. We say that the average IQ is 100, but that’s only by convention. We could just as easily have decided to add 400 to every IQ value and make the average 500 instead. Because IQ’s intervals are equal, the difference between and IQ of 70 and an IQ of 80 is the same as the difference between 120 and 130. However, an IQ of 100 is not twice as high as an IQ of 50. The point is that if the zero point is artificial and movable, then the differences between numbers are meaningful but the ratios between them are not. On the other hand, most lab test values are ratio variables, as are physical characteristics like height and weight. A person who weighs 100 kg is twice as heavy as one who weighs 50 kg; even when we convert kg to pounds, this is still true. For the most part, we can treat and analyze interval or ratio variables the same way.\n\nEach of the quantitative variables in our nh_adult750 data can be thought of as ratio variables.\n\nQuantitative variables lend themselves to many of the summaries we will discuss, like means, quantiles, and our various measures of spread, like the standard deviation or inter-quartile range. They also have at least a chance to follow the Normal distribution.\n\n\n4.4.1 A look at BMI (Body-Mass Index)\nThe definition of BMI (body-mass index) for adult subjects (which is expressed in units of kg/m2) is:\n\\[\n\\mbox{Body Mass Index} = \\frac{\\mbox{weight in kg}}{(\\mbox{height in meters})^2} = 703 \\times \\frac{\\mbox{weight in pounds}}{(\\mbox{height in inches})^2}\n\\]\n\n[BMI is essentially] … a measure of a person’s thinness or thickness… BMI was designed for use as a simple means of classifying average sedentary (physically inactive) populations, with an average body composition. For these individuals, the current value recommendations are as follow: a BMI from 18.5 up to 25 may indicate optimal weight, a BMI lower than 18.5 suggests the person is underweight, a number from 25 up to 30 may indicate the person is overweight, and a number from 30 upwards suggests the person is obese.\nWikipedia, https://en.wikipedia.org/wiki/Body_mass_index"
  },
  {
    "objectID": "04-data_types.html#qualitative-categorical-variables",
    "href": "04-data_types.html#qualitative-categorical-variables",
    "title": "4  Data Structures, Variable Types & Sampling NHANES",
    "section": "4.5 Qualitative (Categorical) Variables",
    "text": "4.5 Qualitative (Categorical) Variables\nQualitative or categorical variables consist of names of categories. These names may be numerical, but the numbers (or names) are simply codes to identify the groups or categories into which the individuals are divided. Categorical variables with two categories, like yes or no, up or down, or, more generally, 1 and 0, are called binary variables. Those with more than two-categories are sometimes called multi-categorical variables.\n\nWhen the categories included in a variable are merely names, and come in no particular order, we sometimes call them nominal variables. The most important summary of such a variable is usually a table of frequencies, and the mode becomes an important single summary, while the mean and median are essentially useless.\n\nIn the nh_adult750 data, Race is a nominal variable with multiple unordered categories. So is MaritalStatus.\n\nThe alternative categorical variable (where order matters) is called ordinal, and includes variables that are sometimes thought of as falling right in between quantitative and qualitative variables.\n\nExamples of ordinal multi-categorical variables in the nh_adult750 data include the Education and HealthGen variables.\nAnswers to questions like “How is your overall physical health?” with available responses Excellent, Very Good, Good, Fair or Poor, which are often coded as 1-5, certainly provide a perceived order, but a group of people with average health status 4 (Very Good) is not necessarily twice as healthy as a group with average health status of 2 (Fair).\n\nSometimes we treat the values from ordinal variables as sufficiently scaled to permit us to use quantitative approaches like means, quantiles, and standard deviations to summarize and model the results, and at other times, we’ll treat ordinal variables as if they were nominal, with tables and percentages our primary tools.\nNote that all binary variables may be treated as ordinal, or nominal.\n\nBinary variables in the nh_adult750 data include Sex, PhysActive, Smoke100, SleepTrouble. Each can be thought of as either ordinal or nominal.\n\n\nLots of variables may be treated as either quantitative or qualitative, depending on how we use them. For instance, we usually think of age as a quantitative variable, but if we simply use age to make the distinction between “child” and “adult” then we are using it to describe categorical information. Just because your variable’s values are numbers, don’t assume that the information provided is quantitative."
  },
  {
    "objectID": "04-data_types.html#counting-missing-values",
    "href": "04-data_types.html#counting-missing-values",
    "title": "4  Data Structures, Variable Types & Sampling NHANES",
    "section": "4.6 Counting Missing Values",
    "text": "4.6 Counting Missing Values\nThe summary() command counts the number of missing observations in each variable, but sometimes you want considerably more information.\nWe can use some functions from the naniar package to learn useful things about the missing data in our nh_adult750 sample.\nThe miss_var_table command provides a table of the number of variables with 0, 1, 2, up to n, missing values and the percentage of the total number of variables those variables make up.\n\nmiss_var_table(nh_adult750)\n\n# A tibble: 5 × 3\n  n_miss_in_var n_vars pct_vars\n          <int>  <int>    <dbl>\n1             0      9    56.2 \n2             5      3    18.8 \n3            32      1     6.25\n4            33      2    12.5 \n5            99      1     6.25\n\n\nSo, for instance, we have 9 variables with no missing data, and that constitutes 56.25% of the 16 variables in our nh_adult750 data.\nThe miss_var_summary() function tabulates the number, percent missing, and cumulative sum of missing of each variable in our data frame, in order of most to least missing values.\n\nmiss_var_summary(nh_adult750)\n\n# A tibble: 16 × 3\n   variable      n_miss pct_miss\n   <chr>          <int>    <dbl>\n 1 HealthGen         99   13.2  \n 2 SBP               33    4.4  \n 3 DBP               33    4.4  \n 4 Pulse             32    4.27 \n 5 Height             5    0.667\n 6 Weight             5    0.667\n 7 BMI                5    0.667\n 8 ID                 0    0    \n 9 Sex                0    0    \n10 Age                0    0    \n11 Race               0    0    \n12 Education          0    0    \n13 PhysActive         0    0    \n14 Smoke100           0    0    \n15 SleepTrouble       0    0    \n16 MaritalStatus      0    0    \n\n\nSo, for example, the rmiss_var_summary(nh_temp) |> slice_head(n = 1) |> select(variable)variable is the one missing more of our data than anything else within thenh_adult750` data frame.\nA graph of this information is available, as well.\n\ngg_miss_var(nh_adult750)\n\n\n\n\nI’ll note that there are also functions to count the number of missing observations by case (observation) rather than variable. For example, we can use miss_case_table.\n\nmiss_case_table(nh_adult750)\n\n# A tibble: 6 × 3\n  n_miss_in_case n_cases pct_cases\n           <int>   <int>     <dbl>\n1              0     636    84.8  \n2              1      78    10.4  \n3              3      15     2    \n4              4      19     2.53 \n5              6       1     0.133\n6              7       1     0.133\n\n\nNow we see that 636 observations, or 84.8% of all cases have no missing data.\nWe can use miss_case_summary() to identify cases with missing data, as well.\n\nmiss_case_summary(nh_adult750)\n\n# A tibble: 750 × 3\n    case n_miss pct_miss\n   <int>  <int>    <dbl>\n 1   342      7     43.8\n 2   606      6     37.5\n 3   157      4     25  \n 4   169      4     25  \n 5   204      4     25  \n 6   234      4     25  \n 7   323      4     25  \n 8   415      4     25  \n 9   478      4     25  \n10   483      4     25  \n# … with 740 more rows\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "04-data_types.html#nh_cc",
    "href": "04-data_types.html#nh_cc",
    "title": "4  Data Structures, Variable Types & Sampling NHANES",
    "section": "4.7 nh_adults500cc: A Sample of Complete Cases",
    "text": "4.7 nh_adults500cc: A Sample of Complete Cases\nIf we wanted a sample of exactly 750 subjects with complete data, we would have needed to add a step in the development of our nh_temp sampling frame to filter for complete cases.\n\nnh_temp2 <- NHANES |>\n    filter(SurveyYr == \"2011_12\") |>\n    filter(Age >= 21 & Age < 65) |>\n    rename(Sex = Gender, Race = Race3, SBP = BPSysAve, DBP = BPDiaAve) |>\n    select(ID, Sex, Age, Height, Weight, Race, Education, BMI, SBP, DBP, \n           Pulse, PhysActive, Smoke100, SleepTrouble, \n           MaritalStatus, HealthGen) |>\n    distinct() |>\n    na.omit()\n\nLet’s check that this new sampling frame has no missing data.\n\nmiss_var_table(nh_temp2)\n\n# A tibble: 1 × 3\n  n_miss_in_var n_vars pct_vars\n          <int>  <int>    <dbl>\n1             0     16      100\n\n\nOK. Now, let’s create a second sample, called nh_adult500cc, where now, we will select 500 adults with complete data on all of the variables of interest, and using a different random seed. The cc here stands for complete cases.\n\nset.seed(431003) \n# use set.seed to ensure that we all get the same random sample \n\nnh_adult500cc <- slice_sample(nh_temp2, n = 500, replace = F) \n\nnh_adult500cc\n\n# A tibble: 500 × 16\n      ID Sex     Age Height Weight Race  Educa…¹   BMI   SBP   DBP Pulse PhysA…²\n   <int> <fct> <int>  <dbl>  <dbl> <fct> <fct>   <dbl> <int> <int> <int> <fct>  \n 1 64079 fema…    25   159.   86.2 Hisp… Some C…  34.2   120    67    84 Yes    \n 2 64374 fema…    52   169    65.5 Asian Colleg…  22.9    92    58    60 Yes    \n 3 71875 male     42   182.   94.1 Black Colleg…  28.5   102    63    76 Yes    \n 4 66396 fema…    46   161.  107.  Asian 8th Gr…  41.2   111    61    70 No     \n 5 64315 fema…    52   161.   64.5 White 9 - 11…  24.9   130    69    68 Yes    \n 6 64015 male     32   168.   82.3 Mexi… Some C…  29     119    79    70 No     \n 7 63590 male     21   181.   98.3 Black Some C…  29.9   121    67    58 Yes    \n 8 70893 fema…    30   171.   65.7 White 9 - 11…  22.5   104    75    74 Yes    \n 9 70828 male     26   178.  100.  White Some C…  31.5   119    77    66 No     \n10 67930 male     59   172.   91.7 Mexi… Colleg…  31     127    85    66 No     \n# … with 490 more rows, 4 more variables: Smoke100 <fct>, SleepTrouble <fct>,\n#   MaritalStatus <fct>, HealthGen <fct>, and abbreviated variable names\n#   ¹​Education, ²​PhysActive\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names"
  },
  {
    "objectID": "04-data_types.html#saving-our-samples-in-.rds-files",
    "href": "04-data_types.html#saving-our-samples-in-.rds-files",
    "title": "4  Data Structures, Variable Types & Sampling NHANES",
    "section": "4.8 Saving our Samples in .Rds files",
    "text": "4.8 Saving our Samples in .Rds files\nWe’ll save the nh_adult750 and nh_adult500cc samples to use in later parts of the notes. To do this, we’ll save them as .Rds files, which will have some advantages for us later on.\n\nwrite_rds(nh_adult750, file = \"data/nh_adult750.Rds\")\nwrite_rds(nh_adult500cc, file = \"data/nh_adult500cc.Rds\")\n\nYou will also find these .Rds files as part of the 431-data repository for the course.\nNext, we’ll load, explore and learn about some of the variables in these two samples.\n\n\n\n\nBock, David E., Paul F. Velleman, and Richard D. De Veaux. 2004. Stats: Modelling the World. Boston MA: Pearson Addison-Wesley.\n\n\nNorman, Geoffrey R., and David L. Streiner. 2014. Biostatistics: The Bare Essentials. Fourth. People’s Medical Publishing House.\n\n\nVittinghoff, Eric, David V. Glidden, Stephen C. Shiboski, and Charles E. McCulloch. 2012. Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models. Second. Springer-Verlag, Inc. http://www.biostat.ucsf.edu/vgsm/."
  },
  {
    "objectID": "05-nhanes_cc.html",
    "href": "05-nhanes_cc.html",
    "title": "5  Visualizing NHANES Data",
    "section": "",
    "text": "knitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor)\nlibrary(knitr) ## for kable\nlibrary(tidyverse)\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "05-nhanes_cc.html#loading-in-the-complete-cases-sample",
    "href": "05-nhanes_cc.html#loading-in-the-complete-cases-sample",
    "title": "5  Visualizing NHANES Data",
    "section": "5.2 Loading in the “Complete Cases” Sample",
    "text": "5.2 Loading in the “Complete Cases” Sample\nLet’s begin by loading into the nh_500cc data frame the information from the nh_adult500cc.Rds file we created in Section @ref(nh_cc).\n\nnh_500cc <- read_rds(\"data/nh_adult500cc.Rds\")\n\nOne obvious hurdle we’ll avoid for the moment is what to do about missing data, since the nh_500cc data are specifically drawn from complete responses. Working with complete cases only can introduce bias to our estimates and visualizations, so it will be necessary in time to address what we should do when a complete-case analysis isn’t a good choice. We’ll return to this issue in a few chapters."
  },
  {
    "objectID": "05-nhanes_cc.html#distribution-of-heights",
    "href": "05-nhanes_cc.html#distribution-of-heights",
    "title": "5  Visualizing NHANES Data",
    "section": "5.3 Distribution of Heights",
    "text": "5.3 Distribution of Heights\nWhat is the distribution of height in this new sample?\n\nggplot(data = nh_500cc, aes(x = Height)) + \n    geom_histogram() \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nWe can do several things to clean this up.\n\nWe’ll change the color of the lines for each bar of the histogram.\nWe’ll change the fill inside each bar to make them stand out a bit more.\nWe’ll add a title and relabel the horizontal (x) axis to include the units of measurement.\nWe’ll avoid the warning by selecting a number of bins (we’ll use 25 here) into which we’ll group the heights before drawing the histogram.\n\n\nggplot(data = nh_500cc, aes(x = Height)) + \n    geom_histogram(bins = 25, col = \"yellow\", fill = \"blue\") + \n    labs(title = \"Height of NHANES subjects ages 21-64\",\n         x = \"Height in cm.\")\n\n\n\n\n\n5.3.1 Changing a Histogram’s Fill and Color\nThe CWRU color guide (https://case.edu/umc/our-brand/visual-guidelines/) lists the HTML color schemes for CWRU blue and CWRU gray. Let’s match that color scheme. We will also change the bins for the histogram, to gather observations into groups of 2 cm. each, by specifying the width of the bins, rather than the number of bins.\n\ncwru.blue <- '#0a304e'\ncwru.gray <- '#626262'\n\nggplot(data = nh_500cc, aes(x = Height)) + \n    geom_histogram(binwidth = 2, \n                   col = cwru.gray, fill = cwru.blue) + \n    labs(title = \"Height of NHANES subjects ages 21-64\",\n         x = \"Height in cm.\") \n\n\n\n\n\n\n5.3.2 Using a frequency polygon\nA frequency polygon essentially smooths out the top of the histogram, and can also be used to show the distribution of Height.\n\nggplot(data = nh_500cc, aes(x = Height)) +\n    geom_freqpoly(bins = 20) +\n    labs(title = \"Height of NHANES subjects ages 21-64\",\n         x = \"Height in cm.\")\n\n\n\n\n\n\n5.3.3 Using a dotplot\nA dotplot can also be used to show the distribution of a variable like Height, and produces a somewhat more granular histogram, depending on the settings for binwidth and dotsize.\n\nggplot(data = nh_500cc, aes(x = Height)) +\n    geom_dotplot(dotsize = 0.75, binwidth = 1) +\n    scale_y_continuous(NULL, breaks = NULL) + # hide y axis\n    labs(title = \"Height of NHANES subjects ages 21-64\",\n         x = \"Height in cm.\")"
  },
  {
    "objectID": "05-nhanes_cc.html#height-and-sex",
    "href": "05-nhanes_cc.html#height-and-sex",
    "title": "5  Visualizing NHANES Data",
    "section": "5.4 Height and Sex",
    "text": "5.4 Height and Sex\nLet’s look again at the impact of a respondent’s sex on their height, but now within our sample of adults.\n\nggplot(data = nh_500cc, \n       aes(x = Sex, y = Height, color = Sex)) + \n    geom_point() + \n    labs(title = \"Height by Sex for NHANES subjects ages 21-64\",\n         y = \"Height in cm.\")\n\n\n\n\nThis plot isn’t so useful. We can improve things a little by jittering the points horizontally, so that the overlap is reduced.\n\nggplot(data = nh_500cc, aes(x = Sex, y = Height, color = Sex)) + \n    geom_jitter(width = 0.2) + \n    labs(title = \"Height by Sex (jittered) for NHANES subjects ages 21-64\",\n         y = \"Height in cm.\")\n\n\n\n\nPerhaps it might be better to summarise the distribution in a different way. We might consider a boxplot of the data.\n\n5.4.1 A Boxplot of Height by Sex\n\nggplot(data = nh_500cc, aes(x = Sex, y = Height, fill = Sex)) + \n    geom_boxplot() + \n    labs(title = \"Boxplot of Height by Sex for NHANES subjects ages 21-64\",\n         y = \"Height in cm.\")\n\n\n\n\nThe boxplot shows some summary statistics based on percentiles. The boxes in the middle of the line show the data values that include the middle half of the data once its been sorted. The 25th percentile (value that exceeds 1/4 of the data) is indicated by the bottom of the box, while the top of the box is located at the 75th percentile. The solid line inside the box indicates the median (also called the 50th percentile) of the Heights for that Sex.\n\n\n5.4.2 Adding a violin plot\nA boxplot is often supplemented with a violin plot to better show the shape of the distribution.\n\nggplot(data = nh_500cc, aes(x = Sex, y = Height, fill = Sex)) +\n    geom_violin() +\n    geom_boxplot(width = 0.3) +\n    labs(title = \"Boxplot of Height by Sex for NHANES subjects ages 21-64\",\n         y = \"Height in cm.\")\n\n\n\n\nThis usually works better if the boxes are given a different fill than the violins, as shown in the following figure.\n\nggplot(data = nh_500cc, aes(x = Sex, y = Height)) +\n    geom_violin(aes(fill = Sex)) +\n    geom_boxplot(width = 0.3) +\n    labs(title = \"Boxplot of Height by Sex for NHANES subjects ages 21-64\",\n         y = \"Height in cm.\")\n\n\n\n\nWe can also flip the boxplots on their side, using coord_flip().\n\nggplot(data = nh_500cc, aes(x = Sex, y = Height)) +\n    geom_violin() +\n    geom_boxplot(aes(fill = Sex), width = 0.3) +\n    labs(title = \"Boxplot of Height by Sex for NHANES subjects ages 21-64\",\n         y = \"Height in cm.\") +\n    coord_flip()\n\n\n\n\n\n\n5.4.3 Histograms of Height by Sex\nOr perhaps we’d like to see a pair of histograms?\n\nggplot(data = nh_500cc, aes(x = Height, fill = Sex)) + \n    geom_histogram(color = \"white\", bins = 20) + \n    labs(title = \"Histogram of Height by Sex for NHANES subjects ages 21-64\",\n         x = \"Height in cm.\") + \n    facet_wrap(~ Sex)\n\n\n\n\nCan we redraw these histograms so that they are a little more comparable, and to get rid of the unnecessary legend?\n\nggplot(data = nh_500cc, aes(x = Height, fill = Sex)) + \n    geom_histogram(color = \"white\", bins = 20) + \n    labs(title = \"Histogram of Height by Sex for NHANES subjects ages 21-64 (Revised)\",\n         x = \"Height in cm.\") + \n    guides(fill = \"none\") +\n    facet_grid(Sex ~ .)"
  },
  {
    "objectID": "05-nhanes_cc.html#looking-at-pulse-rate",
    "href": "05-nhanes_cc.html#looking-at-pulse-rate",
    "title": "5  Visualizing NHANES Data",
    "section": "5.5 Looking at Pulse Rate",
    "text": "5.5 Looking at Pulse Rate\nLet’s look at a different outcome, the pulse rate for our subjects.\nHere’s a histogram, again with CWRU colors, for the pulse rates in our sample.\n\nggplot(data = nh_500cc, aes(x = Pulse)) + \n    geom_histogram(binwidth = 1, \n                   fill = cwru.blue, col = cwru.gray) + \n    labs(title = \"Histogram of Pulse Rate: NHANES subjects ages 21-64\",\n         x = \"Pulse Rate (beats per minute)\")\n\n\n\n\nSuppose we instead bin up groups of 5 beats per minute together as we plot the Pulse rates.\n\nggplot(data = nh_500cc, aes(x = Pulse)) + \n    geom_histogram(binwidth = 5, \n                   fill = cwru.blue, col = cwru.gray) + \n    labs(title = \"Histogram of Pulse Rate: NHANES subjects ages 21-64\",\n         x = \"Pulse Rate (beats per minute)\")\n\n\n\n\nWhich is the more useful representation will depend a lot on what questions you’re trying to answer.\n\n5.5.1 Pulse Rate and Physical Activity\nWe can also split up our data into groups based on whether the subjects are physically active. Let’s try a boxplot.\n\nggplot(data = nh_500cc, \n       aes(y = Pulse, x = PhysActive, fill = PhysActive)) + \n    geom_boxplot() + \n    labs(title = \"Pulse Rate by Physical Activity Status for NHANES ages 21-64\")\n\n\n\n\nAs an accompanying numerical summary, we might ask how many people fall into each of these PhysActive categories, and what is their “average” Pulse rate.\n\nnh_500cc |>\n    group_by(PhysActive) |>\n    summarise(count = n(), mean(Pulse), median(Pulse)) |>\n    kable(digits = 2) \n\n\n\n\nPhysActive\ncount\nmean(Pulse)\nmedian(Pulse)\n\n\n\n\nNo\n216\n74.44\n74\n\n\nYes\n284\n73.96\n74\n\n\n\n\n\nThe kable(digits = 2) piece of this command tells R Markdown to generate a table with some attractive formatting, and rounding any decimals to two figures.\n\n\n5.5.2 Pulse by Sleeping Trouble\n\nggplot(data = nh_500cc, aes(x = Pulse, fill = SleepTrouble)) + \n    geom_histogram(color = \"white\", bins = 20) + \n    labs(title = \"Histogram of Pulse Rate by Sleep Trouble for NHANES subjects ages 21-64\",\n         x = \"Pulse Rate (beats per minute)\") + \n    guides(fill = \"none\") +\n    facet_grid(SleepTrouble ~ ., labeller = \"label_both\")\n\n\n\n\nHow many people fall into each of these SleepTrouble categories, and what is their “average” Pulse rate?\n\nnh_500cc |>\n    group_by(SleepTrouble) |>\n    summarise(count = n(), mean(Pulse), median(Pulse)) |>\n    kable(digits = 2) \n\n\n\n\nSleepTrouble\ncount\nmean(Pulse)\nmedian(Pulse)\n\n\n\n\nNo\n380\n73.45\n73\n\n\nYes\n120\n76.43\n76\n\n\n\n\n\n\n\n5.5.3 Pulse and HealthGen\nWe can compare the distribution of Pulse rate across groups by the subject’s self-reported overall health (HealthGen), as well.\n\nggplot(data = nh_500cc, aes(x = HealthGen, y = Pulse, fill = HealthGen)) + \n    geom_boxplot() +\n    labs(title = \"Pulse by Self-Reported Overall Health for NHANES ages 21-64\",\n         x = \"Self-Reported Overall Health\", y = \"Pulse Rate\") + \n    guides(fill = \"none\") \n\n\n\n\nHow many people fall into each of these HealthGen categories, and what is their “average” Pulse rate?\n\nnh_500cc |>\n    group_by(HealthGen) |>\n    summarise(count = n(), mean(Pulse), median(Pulse)) |>\n    kable(digits = 2) \n\n\n\n\nHealthGen\ncount\nmean(Pulse)\nmedian(Pulse)\n\n\n\n\nExcellent\n52\n72.08\n72\n\n\nVgood\n167\n71.78\n72\n\n\nGood\n204\n75.22\n74\n\n\nFair\n65\n76.55\n76\n\n\nPoor\n12\n85.50\n82\n\n\n\n\n\n\n\n5.5.4 Pulse Rate and Systolic Blood Pressure\n\nggplot(data = nh_500cc, aes(x = SBP, y = Pulse)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x) +\n    labs(title = \"Pulse Rate vs. SBP for NHANES subjects, ages 21-64\")\n\n\n\n\n\n\n5.5.5 Sleep Trouble vs. No Sleep Trouble?\nCould we see whether subjects who have described SleepTrouble show different SBP-pulse rate patterns than the subjects who haven’t?\n\nLet’s try doing this by changing the shape and the color of the points based on SleepTrouble.\n\n\nggplot(data = nh_500cc, \n       aes(x = SBP, y = Pulse, \n           color = SleepTrouble, shape = SleepTrouble)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x) +\n    labs(title = \"Pulse Rate vs. SBP for NHANES subjects, ages 21-64\")\n\n\n\n\nThis plot might be easier to interpret if we faceted by SleepTrouble, as well.\n\nggplot(data = nh_500cc, \n       aes(x = SBP, y = Pulse, \n           color = SleepTrouble, shape = SleepTrouble)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x) +\n    labs(title = \"Pulse Rate vs. SBP for NHANES subjects, ages 21-64\") +\n    facet_wrap(~ SleepTrouble, labeller = \"label_both\")"
  },
  {
    "objectID": "05-nhanes_cc.html#general-health-status",
    "href": "05-nhanes_cc.html#general-health-status",
    "title": "5  Visualizing NHANES Data",
    "section": "5.6 General Health Status",
    "text": "5.6 General Health Status\nHere’s a Table of the General Health Status results. Again, this is a self-reported rating of each subject’s health on a five point scale (Excellent, Very Good, Good, Fair, Poor.)\n\nnh_500cc |>\n    tabyl(HealthGen) \n\n HealthGen   n percent\n Excellent  52   0.104\n     Vgood 167   0.334\n      Good 204   0.408\n      Fair  65   0.130\n      Poor  12   0.024\n\n\nThe HealthGen data are categorical, which means that summarizing them with averages isn’t as appealing as looking at percentages, proportions and rates. The tabyl function comes from the janitor package in R.\n\nI don’t actually like the title of percent here, as it’s really a proportion, but that can be adjusted, and we can add a total.\n\n\nnh_500cc |>\n    tabyl(HealthGen) |>\n    adorn_totals() |>\n    adorn_pct_formatting()\n\n HealthGen   n percent\n Excellent  52   10.4%\n     Vgood 167   33.4%\n      Good 204   40.8%\n      Fair  65   13.0%\n      Poor  12    2.4%\n     Total 500  100.0%\n\n\nWhen working with an unordered categorical variable, like MaritalStatus, the same approach can work.\n\nnh_500cc |>\n    tabyl(MaritalStatus) |>\n    adorn_totals() |>\n    adorn_pct_formatting()\n\n MaritalStatus   n percent\n      Divorced  47    9.4%\n   LivePartner  46    9.2%\n       Married 256   51.2%\n  NeverMarried 125   25.0%\n     Separated  17    3.4%\n       Widowed   9    1.8%\n         Total 500  100.0%\n\n\n\n5.6.1 Bar Chart for Categorical Data\nUsually, a bar chart is the best choice for graphing a variable made up of categories.\n\nggplot(data = nh_500cc, aes(x = HealthGen)) + \n    geom_bar()\n\n\n\n\nThere are lots of things we can do to make this plot fancier.\n\nggplot(data = nh_500cc, aes(x = HealthGen, fill = HealthGen)) + \n    geom_bar() + \n    guides(fill = \"none\") +\n    labs(x = \"Self-Reported Health Status\",\n         y = \"Number of NHANES subjects\",\n         title = \"Self-Reported Health Status in NHANES subjects ages 21-64\")\n\n\n\n\nOr, we can really go crazy…\n\nnh_500cc |>\n    count(HealthGen) |>\n    mutate(pct = round_half_up(prop.table(n) * 100, 1)) |>\n    ggplot(aes(x = HealthGen, y = pct, fill = HealthGen)) + \n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    geom_text(aes(y = pct + 1,    # nudge above top of bar\n                  label = paste0(pct, '%')),  # prettify\n              position = position_dodge(width = .9), \n              size = 4) +\n    labs(x = \"Self-Reported Health Status\",\n         y = \"Percentage of NHANES subjects\",\n         title = \"Self-Reported Health Status in NHANES subjects ages 21-64\") +\n    theme_bw()\n\n\n\n\n\n\n5.6.2 Two-Way Tables\nWe can create cross-classifications of two categorical variables (for example HealthGen and Smoke100), adding both row and column marginal totals, and compare subjects by Sex, as follows…\n\nnh_500cc |>\n    tabyl(Smoke100, HealthGen) |>\n    adorn_totals(c(\"row\", \"col\")) \n\n Smoke100 Excellent Vgood Good Fair Poor Total\n       No        44   108  105   29    5   291\n      Yes         8    59   99   36    7   209\n    Total        52   167  204   65   12   500\n\n\nIf we like, we can make this look a little more polished with the knitr::kable function…\n\nnh_500cc |>\n    tabyl(Smoke100, HealthGen) |>\n    adorn_totals(c(\"row\", \"col\")) |>\n    knitr::kable()\n\n\n\n\nSmoke100\nExcellent\nVgood\nGood\nFair\nPoor\nTotal\n\n\n\n\nNo\n44\n108\n105\n29\n5\n291\n\n\nYes\n8\n59\n99\n36\n7\n209\n\n\nTotal\n52\n167\n204\n65\n12\n500\n\n\n\n\n\nOr, we can get a complete cross-tabulation, including (in this case) the percentages of people within each of the two categories of Smoke100 that fall in each HealthGen category (percentages within each row) like this.\n\nnh_500cc |>\n    tabyl(Smoke100, HealthGen) |>\n    adorn_totals(\"row\") |>\n    adorn_percentages(\"row\") |>\n    adorn_pct_formatting() |>\n    adorn_ns() |>\n    knitr::kable()\n\n\n\n\nSmoke100\nExcellent\nVgood\nGood\nFair\nPoor\n\n\n\n\nNo\n15.1% (44)\n37.1% (108)\n36.1% (105)\n10.0% (29)\n1.7% (5)\n\n\nYes\n3.8% (8)\n28.2% (59)\n47.4% (99)\n17.2% (36)\n3.3% (7)\n\n\nTotal\n10.4% (52)\n33.4% (167)\n40.8% (204)\n13.0% (65)\n2.4% (12)\n\n\n\n\n\nAnd, if we wanted the column percentages, to determine which sex had the higher rate of each HealthGen status level, we can get that by changing the adorn_percentages to describe results at the column level:\n\nnh_500cc |>\n    tabyl(Sex, HealthGen) |>\n    adorn_totals(\"col\") |>\n    adorn_percentages(\"col\") |>\n    adorn_pct_formatting() |>\n    adorn_ns() |>\n    knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\nSex\nExcellent\nVgood\nGood\nFair\nPoor\nTotal\n\n\n\n\nfemale\n63.5% (33)\n44.3% (74)\n43.6% (89)\n47.7% (31)\n75.0% (9)\n47.2% (236)\n\n\nmale\n36.5% (19)\n55.7% (93)\n56.4% (115)\n52.3% (34)\n25.0% (3)\n52.8% (264)\n\n\n\n\n\n\n\n5.6.3 SBP by General Health Status\nLet’s consider now the relationship between self-reported overall health and systolic blood pressure.\n\nggplot(data = nh_500cc, aes(x = HealthGen, y = SBP, \n                            fill = HealthGen)) + \n    geom_boxplot() + \n    labs(title = \"SBP by Health Status, Overall Health for NHANES ages 21-64\",\n         y = \"Systolic Blood Pressure\", \n         x = \"Self-Reported Overall Health\") + \n    guides(fill = \"none\") \n\n\n\n\nWe can see that not too many people self-identify with the “Poor” health category.\n\nnh_500cc |>\n    group_by(HealthGen) |>\n    summarise(count = n(), mean(SBP), median(SBP)) |>\n    knitr::kable() \n\n\n\n\nHealthGen\ncount\nmean(SBP)\nmedian(SBP)\n\n\n\n\nExcellent\n52\n113.9231\n113\n\n\nVgood\n167\n117.5928\n118\n\n\nGood\n204\n121.5931\n120\n\n\nFair\n65\n120.3846\n118\n\n\nPoor\n12\n122.8333\n124\n\n\n\n\n\n\n\n5.6.4 SBP by Physical Activity and General Health Status\nWe’ll build a panel of boxplots to try to understand the relationships between Systolic Blood Pressure, General Health Status and Physical Activity. Note the use of coord_flip to rotate the graph 90 degrees, and the use of labeller within facet_wrap to include both the name of the (Physical Activity) variable and its value.\n\nggplot(data = nh_500cc, aes(x = HealthGen, y = SBP, fill = HealthGen)) + \n    geom_boxplot() + \n    labs(title = \"SBP by Health Status, Overall Health for NHANES ages 21-64\",\n         y = \"Systolic BP\", x = \"Self-Reported Overall Health\") + \n    guides(fill = \"none\") +\n    facet_wrap(~ PhysActive, labeller = \"label_both\") + \n    coord_flip()\n\n\n\n\n\n\n5.6.5 SBP by Sleep Trouble and General Health Status\nHere’s a plot of faceted histograms, which might be used to address similar questions related to the relationship between Overall Health, Systolic Blood Pressure and whether someone has trouble sleeping.\n\nggplot(data = nh_500cc, aes(x = SBP, fill = HealthGen)) + \n    geom_histogram(color = \"white\", bins = 20) + \n    labs(title = \"SBP by Overall Health and Sleep Trouble for NHANES ages 21-64\",\n         x = \"Systolic BP\") + \n    guides(fill = \"none\") +\n    facet_grid(SleepTrouble ~ HealthGen, labeller = \"label_both\")"
  },
  {
    "objectID": "05-nhanes_cc.html#conclusions",
    "href": "05-nhanes_cc.html#conclusions",
    "title": "5  Visualizing NHANES Data",
    "section": "5.7 Conclusions",
    "text": "5.7 Conclusions\nThis is just a small piece of the toolbox for visualizations that we’ll create in this class. Many additional tools are on the way, but the main idea won’t change. Using the ggplot2 package, we can accomplish several critical tasks in creating a visualization, including:\n\nIdentifying (and labeling) the axes and titles\nIdentifying a type of geom to use, like a point, bar or histogram\nChanging fill, color, shape, size to facilitate comparisons\nBuilding “small multiples” of plots with faceting\n\nGood data visualizations make it easy to see the data, and ggplot2’s tools make it relatively difficult to make a really bad graph."
  },
  {
    "objectID": "06-summarizing_quantities.html",
    "href": "06-summarizing_quantities.html",
    "title": "6  Summarizing Quantities",
    "section": "",
    "text": "Most numerical summaries that might be new to you are applied most appropriately to quantitative variables. The measures that will interest us relate to:"
  },
  {
    "objectID": "06-summarizing_quantities.html#setup-packages-used-here",
    "href": "06-summarizing_quantities.html#setup-packages-used-here",
    "title": "6  Summarizing Quantities",
    "section": "6.1 Setup: Packages Used Here",
    "text": "6.1 Setup: Packages Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(patchwork)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\n\nThis chapter also requires that the knitr, Hmisc, mosaic, and psych packages are loaded on your machine, but are not included with library() above."
  },
  {
    "objectID": "06-summarizing_quantities.html#working-with-the-nh_750-data",
    "href": "06-summarizing_quantities.html#working-with-the-nh_750-data",
    "title": "6  Summarizing Quantities",
    "section": "6.2 Working with the nh_750 data",
    "text": "6.2 Working with the nh_750 data\nTo demonstrate key ideas in this Chapter, we will consider our sample of 750 adults ages 21-64 from NHANES 2011-12 which includes some missing values. We’ll load into the nh_750 data frame the information from the nh_adult750.Rds file we created in Section @ref(newNHANES).\n\nnh_750 <- read_rds(\"data/nh_adult750.Rds\")"
  },
  {
    "objectID": "06-summarizing_quantities.html#the-summary-function-for-quantitative-data",
    "href": "06-summarizing_quantities.html#the-summary-function-for-quantitative-data",
    "title": "6  Summarizing Quantities",
    "section": "6.3 The summary function for Quantitative data",
    "text": "6.3 The summary function for Quantitative data\nR provides a small sampling of numerical summaries with the summary function, for instance.\n\nnh_750 |>\n  select(Age, BMI, SBP, DBP, Pulse) |>\n  summary()\n\n      Age             BMI             SBP             DBP        \n Min.   :21.00   Min.   :16.70   Min.   : 83.0   Min.   :  0.00  \n 1st Qu.:30.00   1st Qu.:24.20   1st Qu.:108.0   1st Qu.: 66.00  \n Median :40.00   Median :27.90   Median :118.0   Median : 73.00  \n Mean   :40.82   Mean   :29.08   Mean   :118.8   Mean   : 72.69  \n 3rd Qu.:51.00   3rd Qu.:32.10   3rd Qu.:127.0   3rd Qu.: 80.00  \n Max.   :64.00   Max.   :80.60   Max.   :209.0   Max.   :108.00  \n                 NA's   :5       NA's   :33      NA's   :33      \n     Pulse       \n Min.   : 40.00  \n 1st Qu.: 66.00  \n Median : 72.00  \n Mean   : 73.53  \n 3rd Qu.: 80.00  \n Max.   :124.00  \n NA's   :32      \n\n\nThis basic summary includes a set of five quantiles1, plus the sample’s mean.\n\nMin. = the minimum value for each variable, so, for example, the youngest subject’s Age was 21.\n1st Qu. = the first quartile (25th percentile) for each variable - for example, 25% of the subjects were Age 30 or younger.\nMedian = the median (50th percentile) - half of the subjects were Age 40 or younger.\nMean = the mean, usually what one means by an average - the sum of the Ages divided by 750 is 40.8,\n3rd Qu. = the third quartile (75th percentile) - 25% of the subjects were Age 51 or older.\nMax. = the maximum value for each variable, so the oldest subject was Age 64.\n\nThe summary also specifies the number of missing values for each variable. Here, we are missing 5 of the BMI values, for example."
  },
  {
    "objectID": "06-summarizing_quantities.html#measuring-the-center-of-a-distribution",
    "href": "06-summarizing_quantities.html#measuring-the-center-of-a-distribution",
    "title": "6  Summarizing Quantities",
    "section": "6.4 Measuring the Center of a Distribution",
    "text": "6.4 Measuring the Center of a Distribution\n\n6.4.1 The Mean and The Median\nThe mean and median are the most commonly used measures of the center of a distribution for a quantitative variable. The median is the more generally useful value, as it is relevant even if the data have a shape that is not symmetric. We might also collect the sum of the observations, and the count of the number of observations, usually symbolized with n.\nFor variables without missing values, like Age, this is pretty straightforward.\n\nnh_750 |>\n    summarise(n = n(), Mean = mean(Age), Median = median(Age), Sum = sum(Age))\n\n# A tibble: 1 × 4\n      n  Mean Median   Sum\n  <int> <dbl>  <dbl> <int>\n1   750  40.8     40 30616\n\n\nAnd again, the Mean is just the Sum (30616), divided by the number of non-missing values of Age (750), or 40.8213333.\nThe Median is the middle value when the data are sorted in order. When we have an odd number of values, this is sufficient. When we have an even number, as in this case, we take the mean of the two middle values. We could sort and list all 500 Ages, if we wanted to do so.\n\nnh_750 |> select(Age) |> \n    arrange(Age)\n\n# A tibble: 750 × 1\n     Age\n   <int>\n 1    21\n 2    21\n 3    21\n 4    21\n 5    21\n 6    21\n 7    21\n 8    21\n 9    21\n10    21\n# … with 740 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nBut this data set figures we don’t want to output more than 10 observations to a table like this.\nIf we really want to see all of the data, we can use View(nh_750) to get a spreadsheet-style presentation, or use the sort command…\n\nsort(nh_750$Age)\n\n  [1] 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 22 22 22\n [26] 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 23 23 23 23 23 23\n [51] 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 24 24 24 24 24 24 24 24 24\n [76] 24 24 24 24 24 24 24 24 24 24 24 24 24 24 25 25 25 25 25 25 25 25 25 25 25\n[101] 25 25 25 26 26 26 26 26 26 26 26 26 26 26 26 26 26 27 27 27 27 27 27 27 27\n[126] 27 27 27 27 27 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28\n[151] 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 30 30 30 30 30\n[176] 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 31 31 31 31 31 31\n[201] 31 31 31 31 31 31 31 31 31 31 31 31 31 32 32 32 32 32 32 32 32 32 32 32 32\n[226] 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 33 33 33 33 33 33 33 33 33\n[251] 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 34 34 34 34 34 34 34 34 34 34\n[276] 34 34 34 34 35 35 35 35 35 35 35 35 35 35 35 36 36 36 36 36 36 36 36 36 36\n[301] 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 37 37 37 37 37 37 37 37 37\n[326] 37 37 37 37 37 37 37 37 37 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38\n[351] 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 40 40 40 40 40 40\n[376] 40 40 40 40 40 40 40 40 40 40 40 40 41 41 41 41 41 41 41 41 41 41 41 41 41\n[401] 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 43 43 43 43 43 43\n[426] 43 43 43 43 43 43 43 43 43 43 43 43 44 44 44 44 44 44 44 44 44 44 44 44 44\n[451] 44 44 44 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 46 46 46 46 46\n[476] 46 46 46 46 46 46 46 46 46 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47\n[501] 47 48 48 48 48 48 48 48 48 48 48 48 49 49 49 49 49 49 49 49 49 49 49 49 49\n[526] 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n[551] 50 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 52 52 52 52\n[576] 52 52 52 52 52 52 52 52 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 54 54\n[601] 54 54 54 54 54 54 54 54 54 54 54 54 54 54 55 55 55 55 55 55 55 55 55 55 56\n[626] 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 57 57 57 57 57 57 57\n[651] 57 57 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 59 59 59 59 59\n[676] 59 59 59 59 59 59 59 59 60 60 60 60 60 60 60 60 60 60 60 60 60 61 61 61 61\n[701] 61 61 61 61 61 61 61 61 61 61 62 62 62 62 62 62 62 62 62 62 62 62 62 63 63\n[726] 63 63 63 63 63 63 63 63 63 63 63 64 64 64 64 64 64 64 64 64 64 64 64 64 64\n\n\nAgain, to find the median, we would take the mean of the middle two observations in this sorted data set. That would be the 250th and 251st largest Ages.\n\nsort(nh_750$Age)[250:251]\n\n[1] 33 33\n\n\n\n\n6.4.2 Dealing with Missingness\nWhen calculating a mean, you may be tempted to try something like this…\n\nnh_750 |>\n    summarise(mean(Pulse), median(Pulse))\n\n# A tibble: 1 × 2\n  `mean(Pulse)` `median(Pulse)`\n          <dbl>           <int>\n1            NA              NA\n\n\nThis fails because we have some missing values in the Pulse data. We can address this by either omitting the data with missing values before we run the summarise() function, or tell the mean and median summary functions to remove missing values2.\n\nnh_750 |>\n    filter(complete.cases(Pulse)) |>\n    summarise(count = n(), mean(Pulse), median(Pulse))\n\n# A tibble: 1 × 3\n  count `mean(Pulse)` `median(Pulse)`\n  <int>         <dbl>           <dbl>\n1   718          73.5              72\n\n\nOr, we could tell the summary functions themselves to remove NA values.\n\nnh_750 |>\n    summarise(mean(Pulse, na.rm=TRUE), median(Pulse, na.rm=TRUE))\n\n# A tibble: 1 × 2\n  `mean(Pulse, na.rm = TRUE)` `median(Pulse, na.rm = TRUE)`\n                        <dbl>                         <dbl>\n1                        73.5                            72\n\n\nIn Chapter @ref(miss), we will discuss various assumptions we can make about missing data, and the importance of imputation when dealing with it in modeling or making inferences. For now, we will limit our descriptive summaries to observed values, in what are called complete case or available case analyses.\n\n\n6.4.3 The Mode of a Quantitative Variable\nOne other less common measure of the center of a quantitative variable’s distribution is its most frequently observed value, referred to as the mode. This measure is only appropriate for discrete variables, be they quantitative or categorical. To find the mode, we usually tabulate the data, and then sort by the counts of the numbers of observations.\n\nnh_750 |>\n    group_by(Age) |>\n    summarise(count = n()) |>\n    arrange(desc(count)) \n\n# A tibble: 44 × 2\n     Age count\n   <int> <int>\n 1    32    28\n 2    36    26\n 3    50    26\n 4    30    24\n 5    33    24\n 6    24    23\n 7    21    22\n 8    22    22\n 9    23    22\n10    28    20\n# … with 34 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThe mode is just the most common Age observed in the data.\nNote the use of three different “verbs” in our function there - for more explanation of this strategy, visit Wickham and Grolemund (2022). The group_by function here is very useful. It converts the nh_750 data frame into a new grouped tibble where operations are performed on the groups. Here, this means that it groups the data by Age before counting observations, and then sorting the groups (the Ages) by their frequencies.\nAs an alternative, the modeest package’s mfv function calculates the sample mode (or most frequent value) 3."
  },
  {
    "objectID": "06-summarizing_quantities.html#measuring-the-spread-of-a-distribution",
    "href": "06-summarizing_quantities.html#measuring-the-spread-of-a-distribution",
    "title": "6  Summarizing Quantities",
    "section": "6.5 Measuring the Spread of a Distribution",
    "text": "6.5 Measuring the Spread of a Distribution\nStatistics is all about variation, so spread or dispersion is an important fundamental concept in statistics. Measures of spread like the inter-quartile range and range (maximum - minimum) can help us understand and compare data sets. If the values in the data are close to the center, the spread will be small. If many of the values in the data are scattered far away from the center, the spread will be large.\n\n6.5.1 The Range and the Interquartile Range (IQR)\nThe range of a quantitative variable is sometimes interpreted as the difference between the maximum and the minimum, even though R presents the actual minimum and maximum values when you ask for a range…\n\nnh_750 |> \n    select(Age) |> \n    range()\n\n[1] 21 64\n\n\nAnd, for a variable with missing values, we can use…\n\nnh_750 |> \n    select(BMI) |> \n    filter(complete.cases(BMI)) |>\n    range()\n\n[1] 16.7 80.6\n\n\nA more interesting and useful statistic is the inter-quartile range, or IQR, which is the range of the middle half of the distribution, calculated by subtracting the 25th percentile value from the 75th percentile value.\n\nnh_750 |>\n    summarise(IQR(Age), quantile(Age, 0.25), quantile(Age, 0.75))\n\n# A tibble: 1 × 3\n  `IQR(Age)` `quantile(Age, 0.25)` `quantile(Age, 0.75)`\n       <dbl>                 <dbl>                 <dbl>\n1         21                    30                    51\n\n\nWe can calculate the range and IQR nicely from the summary information on quantiles, of course:\n\nnh_750 |>\n    select(Age, BMI, SBP, DBP, Pulse) |>\n    summary()\n\n      Age             BMI             SBP             DBP        \n Min.   :21.00   Min.   :16.70   Min.   : 83.0   Min.   :  0.00  \n 1st Qu.:30.00   1st Qu.:24.20   1st Qu.:108.0   1st Qu.: 66.00  \n Median :40.00   Median :27.90   Median :118.0   Median : 73.00  \n Mean   :40.82   Mean   :29.08   Mean   :118.8   Mean   : 72.69  \n 3rd Qu.:51.00   3rd Qu.:32.10   3rd Qu.:127.0   3rd Qu.: 80.00  \n Max.   :64.00   Max.   :80.60   Max.   :209.0   Max.   :108.00  \n                 NA's   :5       NA's   :33      NA's   :33      \n     Pulse       \n Min.   : 40.00  \n 1st Qu.: 66.00  \n Median : 72.00  \n Mean   : 73.53  \n 3rd Qu.: 80.00  \n Max.   :124.00  \n NA's   :32      \n\n\n\n\n6.5.2 The Variance and the Standard Deviation\nThe IQR is always a reasonable summary of spread, just as the median is always a reasonable summary of the center of a distribution. Yet, most people are inclined to summarize a batch of data using two numbers: the mean and the standard deviation. This is really only a sensible thing to do if you are willing to assume the data follow a Normal distribution: a bell-shaped, symmetric distribution without substantial outliers.\nBut most data do not (even approximately) follow a Normal distribution. Summarizing by the median and quartiles (25th and 75th percentiles) is much more robust, explaining R’s emphasis on them.\n\n\n6.5.3 Obtaining the Variance and Standard Deviation in R\nHere are the variances of the quantitative variables in the nh_750 data. Note the need to include na.rm = TRUE to deal with the missing values in some variables.\n\nnh_750 |>\n    select(Age, BMI, SBP, DBP, Pulse) |>\n    summarise_all(var, na.rm = TRUE)\n\n# A tibble: 1 × 5\n    Age   BMI   SBP   DBP Pulse\n  <dbl> <dbl> <dbl> <dbl> <dbl>\n1  157.  52.4  229.  128.  136.\n\n\nAnd here are the standard deviations of those same variables.\n\nnh_750 |>\n    select(Age, BMI, SBP, DBP, Pulse) |>\n    summarise_all(sd, na.rm = TRUE)\n\n# A tibble: 1 × 5\n    Age   BMI   SBP   DBP Pulse\n  <dbl> <dbl> <dbl> <dbl> <dbl>\n1  12.5  7.24  15.1  11.3  11.6\n\n\n\n\n6.5.4 Defining the Variance and Standard Deviation\nBock, Velleman, and De Veaux (2004) have lots of useful thoughts here, which are lightly edited here.\nIn thinking about spread, we might consider how far each data value is from the mean. Such a difference is called a deviation. We could just average the deviations, but the positive and negative differences always cancel out, leaving an average deviation of zero, so that’s not helpful. Instead, we square each deviation to obtain non-negative values, and to emphasize larger differences. When we add up these squared deviations and find their mean (almost), this yields the variance.\n\\[\n\\mbox{Variance} = s^2 = \\frac{\\Sigma (y - \\bar{y})^2}{n-1}\n\\]\nWhy almost? It would be the mean of the squared deviations only if we divided the sum by \\(n\\), but instead we divide by \\(n-1\\) because doing so produces an estimate of the true (population) variance that is unbiased4. If you’re looking for a more intuitive explanation, this Stack Exchange link awaits your attention.\n\nTo return to the original units of measurement, we take the square root of \\(s^2\\), and instead work with \\(s\\), the standard deviation, also abbreviated SD.\n\n\\[\n\\mbox{Standard Deviation} = s = \\sqrt{\\frac{\\Sigma (y - \\bar{y})^2}{n-1}}\n\\]\n\n\n6.5.5 Interpreting the SD when the data are Normally distributed\nFor a set of measurements that follow a Normal distribution, the interval:\n\nMean \\(\\pm\\) Standard Deviation contains approximately 68% of the measurements;\nMean \\(\\pm\\) 2(Standard Deviation) contains approximately 95% of the measurements;\nMean \\(\\pm\\) 3(Standard Deviation) contains approximately all (99.7%) of the measurements.\n\nWe often refer to the population or process mean of a distribution with \\(\\mu\\) and the standard deviation with \\(\\sigma\\), leading to the Figure below.\n\n\n\n\n\nThe Normal Distribution and the Empirical Rule\n\n\n\n\nBut if the data are not from an approximately Normal distribution, then this Empirical Rule is less helpful.\n\n\n6.5.6 Chebyshev’s Inequality: One Interpretation of the Standard Deviation\nChebyshev’s Inequality tells us that for any distribution, regardless of its relationship to a Normal distribution, no more than 1/k2 of the distribution’s values can lie more than k standard deviations from the mean. This implies, for instance, that for any distribution, at least 75% of the values must lie within two standard deviations of the mean, and at least 89% must lie within three standard deviations of the mean.\nAgain, most data sets do not follow a Normal distribution. We’ll return to this notion soon. But first, let’s try to draw some pictures that let us get a better understanding of the distribution of our data."
  },
  {
    "objectID": "06-summarizing_quantities.html#measuring-the-shape-of-a-distribution",
    "href": "06-summarizing_quantities.html#measuring-the-shape-of-a-distribution",
    "title": "6  Summarizing Quantities",
    "section": "6.6 Measuring the Shape of a Distribution",
    "text": "6.6 Measuring the Shape of a Distribution\nWhen considering the shape of a distribution, one is often interested in three key points.\n\nThe number of modes in the distribution, which I always assess through plotting the data.\nThe skewness, or symmetry that is present, which I typically assess by looking at a plot of the distribution of the data, but if required to, will summarize with a non-parametric measure of skewness.\nThe kurtosis, or heavy-tailedness (outlier-proneness) that is present, usually in comparison to a Normal distribution. Again, this is something I nearly inevitably assess graphically, but there are measures.\n\nA Normal distribution has a single mode, is symmetric and, naturally, is neither heavy-tailed nor light-tailed as compared to a Normal distribution (we call this mesokurtic).\n\n6.6.1 Multimodal vs. Unimodal distributions\nA unimodal distribution, on some level, is straightforward. It is a distribution with a single mode, or “peak” in the distribution. Such a distribution may be skewed or symmetric, light-tailed or heavy-tailed. We usually describe as multimodal distributions like the two on the right below, which have multiple local maxima, even though they have just a single global maximum peak.\n\n\n\n\n\nUnimodal and Multimodal Sketches\n\n\n\n\nTruly multimodal distributions are usually described that way in terms of shape. For unimodal distributions, skewness and kurtosis become useful ideas.\n\n\n6.6.2 Skew\nWhether or not a distribution is approximately symmetric is an important consideration in describing its shape. Graphical assessments are always most useful in this setting, particularly for unimodal data. My favorite measure of skew, or skewness if the data have a single mode, is:\n\\[\nskew_1 = \\frac{\\mbox{mean} - \\mbox{median}}{\\mbox{standard deviation}}\n\\]\n\nSymmetric distributions generally show values of \\(skew_1\\) near zero. If the distribution is actually symmetric, the mean should be equal to the median.\nDistributions with \\(skew_1\\) values above 0.2 in absolute value generally indicate meaningful skew.\nPositive skew (mean > median if the data are unimodal) is also referred to as right skew.\nNegative skew (mean < median if the data are unimodal) is referred to as left skew.\n\n\n\n\n\n\nNegative (Left) Skew and Positive (Right) Skew\n\n\n\n\n\n\n6.6.3 Kurtosis\nWhen we have a unimodal distribution that is symmetric, we will often be interested in the behavior of the tails of the distribution, as compared to a Normal distribution with the same mean and standard deviation. High values of kurtosis measures (and there are several) indicate data which has extreme outliers, or is heavy-tailed.\n\nA mesokurtic distribution has similar tail behavior to what we would expect from a Normal distribution.\nA leptokurtic distribution is a thinner, more slender distribution, with heavier tails than we’d expect from a Normal distribution. One example is the t distribution.\nA platykurtic distribution is a broader, flatter distribution, with thinner tails than we’d expect from a Normal distribution. One example is a uniform distribution.\n\n\nset.seed(431)\nsims_kurt <- tibble(meso = rnorm(n = 300, mean = 0, sd = 1),\n                    lepto = rt(n = 300, df = 4),\n                    platy = runif(n = 300, min = -2, max = 2))\n\np1 <- ggplot(sims_kurt, aes(x = meso)) +\n  geom_histogram(aes(y = stat(density)), \n                 bins = 25, fill = \"royalblue\", col = \"white\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(sims_kurt$meso), \n                            sd = sd(sims_kurt$meso)),\n                col = \"red\") +\n  labs(title = \"Normal (mesokurtic)\")\n\np1a <- ggplot(sims_kurt, aes(x = meso, y = \"\")) +\n  geom_violin() +\n  geom_boxplot(fill = \"royalblue\", outlier.color = \"royalblue\", width = 0.3) +\n  labs(y = \"\", x = \"Normal (mesokurtic)\")\n\np2 <- ggplot(sims_kurt, aes(x = lepto)) +\n  geom_histogram(aes(y = stat(density)), \n                 bins = 25, fill = \"tomato\", col = \"white\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(sims_kurt$lepto), \n                            sd = sd(sims_kurt$lepto)),\n                col = \"royalblue\") +\n  labs(title = \"t (leptokurtic)\")\n\np2a <- ggplot(sims_kurt, aes(x = lepto, y = \"\")) +\n  geom_violin() +\n  geom_boxplot(fill = \"tomato\", outlier.color = \"tomato\", width = 0.3) +\n  labs(y = \"\", x = \"t (slender with heavy tails)\")\n\np3 <- ggplot(sims_kurt, aes(x = platy)) +\n  geom_histogram(aes(y = stat(density)), \n                 bins = 25, fill = \"yellow\", col = \"black\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(sims_kurt$platy), \n                            sd = sd(sims_kurt$platy)),\n                col = \"royalblue\", lwd = 1.5) +\n  xlim(-3, 3) +\n  labs(title = \"Uniform (platykurtic)\")\n\np3a <- ggplot(sims_kurt, aes(x = platy, y = \"\")) +\n  geom_violin() +\n  geom_boxplot(fill = \"yellow\", width = 0.3) + \n  xlim(-3, 3) +\n  labs(y = \"\", x = \"Uniform (broad with thin tails)\")\n\n\n(p1 + p2 + p3) / (p1a + p2a + p3a) + \n  plot_layout(heights = c(3, 1))\n\n\n\n\nGraphical tools are in most cases the best way to identify issues related to kurtosis."
  },
  {
    "objectID": "06-summarizing_quantities.html#numerical-summaries-for-quantitative-variables",
    "href": "06-summarizing_quantities.html#numerical-summaries-for-quantitative-variables",
    "title": "6  Summarizing Quantities",
    "section": "6.7 Numerical Summaries for Quantitative Variables",
    "text": "6.7 Numerical Summaries for Quantitative Variables\n\n6.7.1 favstats in the mosaic package\nThe favstats function adds the standard deviation, and counts of overall and missing observations to our usual summary for a continuous variable. Let’s look at systolic blood pressure, because we haven’t yet.\n\nmosaic::favstats(~ SBP, data = nh_750)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\n\n min  Q1 median  Q3 max     mean       sd   n missing\n  83 108    118 127 209 118.7908 15.14329 717      33\n\n\nWe could, of course, duplicate these results with several summarise() pieces…\n\nnh_750 |>\n    filter(complete.cases(SBP)) |>\n    summarise(min = min(SBP), Q1 = quantile(SBP, 0.25), \n              median = median(SBP), Q3 = quantile(SBP, 0.75), \n              max = max(SBP),  mean = mean(SBP), \n              sd = sd(SBP), n = n(), miss = sum(is.na(SBP)))\n\n# A tibble: 1 × 9\n    min    Q1 median    Q3   max  mean    sd     n  miss\n  <int> <dbl>  <int> <dbl> <int> <dbl> <dbl> <int> <int>\n1    83   108    118   127   209  119.  15.1   717     0\n\n\nThe somewhat unusual structure of favstats (complete with an easy to forget ~) is actually helpful. It allows you to look at some interesting grouping approaches, like this:\n\nmosaic::favstats(SBP ~ Education, data = nh_750)\n\n       Education min     Q1 median     Q3 max     mean       sd   n missing\n1      8th Grade  96 110.25  119.5 129.75 167 122.4565 16.34993  46       4\n2 9 - 11th Grade  85 107.75  116.0 127.00 191 118.8026 15.79453  76       0\n3    High School  84 111.50  120.5 129.00 209 121.0882 16.52853 136       7\n4   Some College  85 108.00  117.0 126.00 186 118.6293 14.32736 232       9\n5   College Grad  83 107.00  117.0 125.00 171 116.8326 14.41202 227      13\n\n\nOf course, we could accomplish the same comparison with dplyr commands, too, but the favstats approach has much to offer.\n\nnh_750 |>\n    filter(complete.cases(SBP, Education)) |>\n    group_by(Education) |>\n    summarise(min = min(SBP), Q1 = quantile(SBP, 0.25), \n              median = median(SBP), Q3 = quantile(SBP, 0.75), \n              max = max(SBP), mean = mean(SBP), \n              sd = sd(SBP), n = n(), miss = sum(is.na(SBP)))\n\n# A tibble: 5 × 10\n  Education        min    Q1 median    Q3   max  mean    sd     n  miss\n  <fct>          <int> <dbl>  <dbl> <dbl> <int> <dbl> <dbl> <int> <int>\n1 8th Grade         96  110.   120.  130.   167  122.  16.3    46     0\n2 9 - 11th Grade    85  108.   116   127    191  119.  15.8    76     0\n3 High School       84  112.   120.  129    209  121.  16.5   136     0\n4 Some College      85  108    117   126    186  119.  14.3   232     0\n5 College Grad      83  107    117   125    171  117.  14.4   227     0\n\n\n\n\n6.7.2 describe in the psych package\nThe psych package has a more detailed list of numerical summaries for quantitative variables that lets us look at a group of observations at once.\n\npsych::describe(nh_750 |> select(Age, BMI, SBP, DBP, Pulse))\n\n      vars   n   mean    sd median trimmed   mad  min   max range  skew\nAge      1 750  40.82 12.54   40.0   40.53 14.83 21.0  64.0  43.0  0.16\nBMI      2 745  29.08  7.24   27.9   28.31  5.93 16.7  80.6  63.9  1.72\nSBP      3 717 118.79 15.14  118.0  117.88 13.34 83.0 209.0 126.0  0.96\nDBP      4 717  72.69 11.34   73.0   72.65 10.38  0.0 108.0 108.0 -0.28\nPulse    5 718  73.53 11.65   72.0   73.11 11.86 40.0 124.0  84.0  0.48\n      kurtosis   se\nAge      -1.15 0.46\nBMI       6.16 0.27\nSBP       3.10 0.57\nDBP       2.59 0.42\nPulse     0.73 0.43\n\n\nThe additional statistics presented here are:\n\ntrimmed = a trimmed mean (by default in this function, this removes the top and bottom 10% from the data, then computes the mean of the remaining values - the middle 80% of the full data set.)\nmad = the median absolute deviation (from the median), which can be used in a manner similar to the standard deviation or IQR to measure spread.\n\nIf the data are \\(Y_1, Y_2, ..., Y_n\\), then the mad is defined as \\(median(|Y_i - median(Y_i)|)\\).\nTo find the mad for a set of numbers, find the median, subtract the median from each value and find the absolute value of that difference, and then find the median of those absolute differences.\nFor non-normal data with a skewed shape but tails well approximated by the Normal, the mad is likely to be a better (more robust) estimate of the spread than is the standard deviation.\n\na measure of skew, which refers to how much asymmetry is present in the shape of the distribution. The measure is not the same as the nonparametric skew measure that we will usually prefer. The [Wikipedia page on skewness][https://en.wikipedia.org/wiki/Skewness] is very detailed.\na measure of excess kurtosis, which refers to how outlier-prone, or heavy-tailed the shape of the distribution is, as compared to a Normal distribution.\nse = the standard error of the sample mean, equal to the sample sd divided by the square root of the sample size.\n\n\n\n6.7.3 The Hmisc package’s version of describe\n\nHmisc::describe(nh_750 |> \n                  select(Age, BMI, SBP, DBP, Pulse))\n\nselect(nh_750, Age, BMI, SBP, DBP, Pulse) \n\n 5  Variables      750  Observations\n--------------------------------------------------------------------------------\nAge \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     750        0       44    0.999    40.82    14.46       22       24 \n     .25      .50      .75      .90      .95 \n      30       40       51       59       62 \n\nlowest : 21 22 23 24 25, highest: 60 61 62 63 64\n--------------------------------------------------------------------------------\nBMI \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     745        5      250        1    29.08    7.538    20.22    21.30 \n     .25      .50      .75      .90      .95 \n   24.20    27.90    32.10    37.60    41.28 \n\nlowest : 16.7 17.6 17.8 17.9 18.0, highest: 59.1 62.8 63.3 69.0 80.6\n--------------------------------------------------------------------------------\nSBP \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     717       33       81    0.999    118.8    16.36     98.0    102.0 \n     .25      .50      .75      .90      .95 \n   108.0    118.0    127.0    137.0    144.2 \n\nlowest :  83  84  85  86  89, highest: 171 179 186 191 209\n--------------------------------------------------------------------------------\nDBP \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     717       33       66    0.999    72.69    12.43       55       59 \n     .25      .50      .75      .90      .95 \n      66       73       80       86       91 \n\nlowest :   0  25  41  42  44, highest: 104 105 106 107 108\n--------------------------------------------------------------------------------\nPulse \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     718       32       37    0.997    73.53    12.95       56       60 \n     .25      .50      .75      .90      .95 \n      66       72       80       88       94 \n\nlowest :  40  44  46  48  50, highest: 108 112 114 118 124\n--------------------------------------------------------------------------------\n\n\nThe Hmisc package’s version of describe for a distribution of data presents three new ideas, in addition to a more comprehensive list of quartiles (the 5th, 10th, 25th, 50th, 75th, 90th and 95th are shown) and the lowest and highest few observations. These are:\n\ndistinct - the number of different values observed in the data.\nInfo - a measure of how “continuous” the variable is, related to how many “ties” there are in the data, with Info taking a higher value (closer to its maximum of one) if the data are more continuous.\nGmd - the Gini mean difference - a robust measure of spread that is calculated as the mean absolute difference between any pairs of observations. Larger values of Gmd indicate more spread-out distributions. (Gini is pronounced as either “Genie” or “Ginny”.)\n\n\n\n6.7.4 Other options\nThe package summarytools has a function called dfSummary which I like and Dominic Comtois has also published Recommendations for Using summarytools with R Markdown. Note that this isn’t really for Word documents.\nDataExplorer can be used for more automated exploratory data analyses (and some people also like skimr) and visdat, as well.\nThe df_stats function available when the mosaic package is loaded allows you to run favstats for multiple outcome variables simultaneously.\n\n\n\n\nBock, David E., Paul F. Velleman, and Richard D. De Veaux. 2004. Stats: Modelling the World. Boston MA: Pearson Addison-Wesley.\n\n\nWickham, Hadley, and Garrett Grolemund. 2022. R for Data Science. Second. O’Reilly. https://r4ds.hadley.nz/."
  },
  {
    "objectID": "07-summarizing_categories.html",
    "href": "07-summarizing_categories.html",
    "title": "7  Summarizing Categories",
    "section": "",
    "text": "knitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor)\nlibrary(gt)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "07-summarizing_categories.html#using-the-nh_adult750-data-again",
    "href": "07-summarizing_categories.html#using-the-nh_adult750-data-again",
    "title": "7  Summarizing Categories",
    "section": "7.2 Using the nh_adult750 data again",
    "text": "7.2 Using the nh_adult750 data again\nTo demonstrate key ideas in this Chapter, we will again consider our sample of 750 adults ages 21-64 from NHANES 2011-12 which includes some missing values. We’ll load into the nh_750 data frame the information from the nh_adult750.Rds file we created in Section @ref(newNHANES).\n\nnh_750 <- read_rds(\"data/nh_adult750.Rds\")\n\nSummarizing categorical variables numerically is mostly about building tables, and calculating percentages or proportions. We’ll save our discussion of modeling categorical data for later. Recall that in the nh_750 data set we built in Section @ref(newNHANES) we had the following categorical variables. The number of levels indicates the number of possible categories for each categorical variable.\n\n\n\nVariable\nDescription\nLevels\nType\n\n\n\n\nSex\nsex of subject\n2\nbinary\n\n\nRace\nsubject’s race\n6\nnominal\n\n\nEducation\nsubject’s educational level\n5\nordinal\n\n\nPhysActive\nParticipates in sports?\n2\nbinary\n\n\nSmoke100\nSmoked 100+ cigarettes?\n2\nbinary\n\n\nSleepTrouble\nTrouble sleeping?\n2\nbinary\n\n\nHealthGen\nSelf-report health\n5\nordinal"
  },
  {
    "objectID": "07-summarizing_categories.html#the-summary-function-for-categorical-data",
    "href": "07-summarizing_categories.html#the-summary-function-for-categorical-data",
    "title": "7  Summarizing Categories",
    "section": "7.3 The summary function for Categorical data",
    "text": "7.3 The summary function for Categorical data\nWhen R recognizes a variable as categorical, it stores it as a factor. Such variables get special treatment from the summary function, in particular a table of available values (so long as there aren’t too many.)\n\nnh_750 |>\n  select(Sex, Race, Education, PhysActive, Smoke100, \n         SleepTrouble, HealthGen, MaritalStatus) |>\n  summary()\n\n     Sex            Race              Education   PhysActive Smoke100 \n female:388   Asian   : 70   8th Grade     : 50   No :326    No :453  \n male  :362   Black   :128   9 - 11th Grade: 76   Yes:424    Yes:297  \n              Hispanic: 63   High School   :143                       \n              Mexican : 80   Some College  :241                       \n              White   :393   College Grad  :240                       \n              Other   : 16                                            \n SleepTrouble     HealthGen        MaritalStatus\n No :555      Excellent: 84   Divorced    : 78  \n Yes:195      Vgood    :197   LivePartner : 70  \n              Good     :252   Married     :388  \n              Fair     :104   NeverMarried:179  \n              Poor     : 14   Separated   : 19  \n              NA's     : 99   Widowed     : 16"
  },
  {
    "objectID": "07-summarizing_categories.html#tables-to-describe-one-categorical-variable",
    "href": "07-summarizing_categories.html#tables-to-describe-one-categorical-variable",
    "title": "7  Summarizing Categories",
    "section": "7.4 Tables to describe One Categorical Variable",
    "text": "7.4 Tables to describe One Categorical Variable\nSuppose we build a table (using the tabyl function from the janitor package) to describe the HealthGen distribution.\n\nnh_750 |>\n    tabyl(HealthGen) |>\n    adorn_pct_formatting()\n\n HealthGen   n percent valid_percent\n Excellent  84   11.2%         12.9%\n     Vgood 197   26.3%         30.3%\n      Good 252   33.6%         38.7%\n      Fair 104   13.9%         16.0%\n      Poor  14    1.9%          2.2%\n      <NA>  99   13.2%             -\n\n\nNote how the missing (<NA>) values are not included in the valid_percent calculation, but are in the percent calculation. Note also the use of percentage formatting.\nWhat if we want to add a total count, sometimes called the marginal total?\n\nnh_750 |>\n    tabyl(HealthGen) |>\n    adorn_totals() |>\n    adorn_pct_formatting()\n\n HealthGen   n percent valid_percent\n Excellent  84   11.2%         12.9%\n     Vgood 197   26.3%         30.3%\n      Good 252   33.6%         38.7%\n      Fair 104   13.9%         16.0%\n      Poor  14    1.9%          2.2%\n      <NA>  99   13.2%             -\n     Total 750  100.0%        100.0%\n\n\nWhat about marital status, which has no missing data in our sample?\n\nnh_750 |>\n    tabyl(MaritalStatus) |>\n    adorn_totals() |>\n    adorn_pct_formatting()\n\n MaritalStatus   n percent\n      Divorced  78   10.4%\n   LivePartner  70    9.3%\n       Married 388   51.7%\n  NeverMarried 179   23.9%\n     Separated  19    2.5%\n       Widowed  16    2.1%\n         Total 750  100.0%"
  },
  {
    "objectID": "07-summarizing_categories.html#constructing-tables-well",
    "href": "07-summarizing_categories.html#constructing-tables-well",
    "title": "7  Summarizing Categories",
    "section": "7.5 Constructing Tables Well",
    "text": "7.5 Constructing Tables Well\nThe prolific Howard Wainer is responsible for many interesting books on visualization and related issues, including Wainer (2005) and Wainer (2013). These rules come from Chapter 10 of Wainer (1997).\n\nOrder the rows and columns in a way that makes sense.\nRound, a lot!\nALL is different and important\n\n\n7.5.1 Alabama First!\nWhich of these Tables is more useful to you?\n2013 Percent of Students in grades 9-12 who are obese\n\n\n\nState\n% Obese\n95% CI\nSample Size\n\n\n\n\nAlabama\n17.1\n(14.6 - 19.9)\n1,499\n\n\nAlaska\n12.4\n(10.5-14.6)\n1,167\n\n\nArizona\n10.7\n(8.3-13.6)\n1,520\n\n\nArkansas\n17.8\n(15.7-20.1)\n1,470\n\n\nConnecticut\n12.3\n(10.2-14.7)\n2,270\n\n\nDelaware\n14.2\n(12.9-15.6)\n2,475\n\n\nFlorida\n11.6\n(10.5-12.8)\n5,491\n\n\n…\n\n\n\n\n\nWisconsin\n11.6\n(9.7-13.9)\n2,771\n\n\nWyoming\n10.7\n(9.4-12.2)\n2,910\n\n\n\nor …\n\n\n\nState\n% Obese\n95% CI\nSample Size\n\n\n\n\nKentucky\n18.0\n(15.7 - 20.6)\n1,537\n\n\nArkansas\n17.8\n(15.7 - 20.1)\n1,470\n\n\nAlabama\n17.1\n(14.6 - 19.9)\n1,499\n\n\nTennessee\n16.9\n(15.1 - 18.8)\n1,831\n\n\nTexas\n15.7\n(13.9 - 17.6)\n3,039\n\n\n…\n\n\n\n\n\nMassachusetts\n10.2\n(8.5 - 12.1)\n2,547\n\n\nIdaho\n9.6\n(8.2 - 11.1)\n1,841\n\n\nMontana\n9.4\n(8.4 - 10.5)\n4,679\n\n\nNew Jersey\n8.7\n(6.8 - 11.2)\n1,644\n\n\nUtah\n6.4\n(4.8 - 8.5)\n2,136\n\n\n\nIt is a rare event when Alabama first is the best choice.\n\n\n7.5.2 ALL is different and important\nSummaries of rows and columns provide a measure of what is typical or usual. Sometimes a sum is helpful, at other times, consider presenting a median or other summary. The ALL category, as Wainer (1997) suggests, should be both visually different from the individual entries and set spatially apart.\nOn the whole, it’s far easier to fall into a good graph in R (at least if you have some ggplot2 skills) than to produce a good table."
  },
  {
    "objectID": "07-summarizing_categories.html#the-mode-of-a-categorical-variable",
    "href": "07-summarizing_categories.html#the-mode-of-a-categorical-variable",
    "title": "7  Summarizing Categories",
    "section": "7.6 The Mode of a Categorical Variable",
    "text": "7.6 The Mode of a Categorical Variable\nA common measure applied to a categorical variable is to identify the mode, the most frequently observed value. To find the mode for variables with lots of categories (so that the summary may not be sufficient), we usually tabulate the data, and then sort by the counts of the numbers of observations, as we did with discrete quantitative variables.\n\nnh_750 |>\n    group_by(HealthGen) |>\n    summarise(count = n()) |>\n    arrange(desc(count)) \n\n# A tibble: 6 × 2\n  HealthGen count\n  <fct>     <int>\n1 Good        252\n2 Vgood       197\n3 Fair        104\n4 <NA>         99\n5 Excellent    84\n6 Poor         14"
  },
  {
    "objectID": "07-summarizing_categories.html#describe-in-the-hmisc-package",
    "href": "07-summarizing_categories.html#describe-in-the-hmisc-package",
    "title": "7  Summarizing Categories",
    "section": "7.7 describe in the Hmisc package",
    "text": "7.7 describe in the Hmisc package\n\nHmisc::describe(nh_750 |> \n                    select(Sex, Race, Education, PhysActive, \n                           Smoke100, SleepTrouble, \n                           HealthGen, MaritalStatus))\n\nselect(nh_750, Sex, Race, Education, PhysActive, Smoke100, SleepTrouble, HealthGen, MaritalStatus) \n\n 8  Variables      750  Observations\n--------------------------------------------------------------------------------\nSex \n       n  missing distinct \n     750        0        2 \n                        \nValue      female   male\nFrequency     388    362\nProportion  0.517  0.483\n--------------------------------------------------------------------------------\nRace \n       n  missing distinct \n     750        0        6 \n\nlowest : Asian    Black    Hispanic Mexican  White   \nhighest: Black    Hispanic Mexican  White    Other   \n                                                                \nValue         Asian    Black Hispanic  Mexican    White    Other\nFrequency        70      128       63       80      393       16\nProportion    0.093    0.171    0.084    0.107    0.524    0.021\n--------------------------------------------------------------------------------\nEducation \n       n  missing distinct \n     750        0        5 \n\nlowest : 8th Grade      9 - 11th Grade High School    Some College   College Grad  \nhighest: 8th Grade      9 - 11th Grade High School    Some College   College Grad  \n                                                                      \nValue           8th Grade 9 - 11th Grade    High School   Some College\nFrequency              50             76            143            241\nProportion          0.067          0.101          0.191          0.321\n                         \nValue        College Grad\nFrequency             240\nProportion          0.320\n--------------------------------------------------------------------------------\nPhysActive \n       n  missing distinct \n     750        0        2 \n                      \nValue         No   Yes\nFrequency    326   424\nProportion 0.435 0.565\n--------------------------------------------------------------------------------\nSmoke100 \n       n  missing distinct \n     750        0        2 \n                      \nValue         No   Yes\nFrequency    453   297\nProportion 0.604 0.396\n--------------------------------------------------------------------------------\nSleepTrouble \n       n  missing distinct \n     750        0        2 \n                    \nValue        No  Yes\nFrequency   555  195\nProportion 0.74 0.26\n--------------------------------------------------------------------------------\nHealthGen \n       n  missing distinct \n     651       99        5 \n\nlowest : Excellent Vgood     Good      Fair      Poor     \nhighest: Excellent Vgood     Good      Fair      Poor     \n                                                            \nValue      Excellent     Vgood      Good      Fair      Poor\nFrequency         84       197       252       104        14\nProportion     0.129     0.303     0.387     0.160     0.022\n--------------------------------------------------------------------------------\nMaritalStatus \n       n  missing distinct \n     750        0        6 \n\nlowest : Divorced     LivePartner  Married      NeverMarried Separated   \nhighest: LivePartner  Married      NeverMarried Separated    Widowed     \n                                                                           \nValue          Divorced  LivePartner      Married NeverMarried    Separated\nFrequency            78           70          388          179           19\nProportion        0.104        0.093        0.517        0.239        0.025\n                       \nValue           Widowed\nFrequency            16\nProportion        0.021\n--------------------------------------------------------------------------------"
  },
  {
    "objectID": "07-summarizing_categories.html#cross-tabulations-of-two-variables",
    "href": "07-summarizing_categories.html#cross-tabulations-of-two-variables",
    "title": "7  Summarizing Categories",
    "section": "7.8 Cross-Tabulations of Two Variables",
    "text": "7.8 Cross-Tabulations of Two Variables\nIt is very common for us to want to describe the association of one categorical variable with another. For instance, is there a relationship between Education and SleepTrouble in these data?\n\nnh_750 |>\n    tabyl(Education, SleepTrouble) |>\n    adorn_totals(where = c(\"row\", \"col\")) \n\n      Education  No Yes Total\n      8th Grade  40  10    50\n 9 - 11th Grade  52  24    76\n    High School 102  41   143\n   Some College 173  68   241\n   College Grad 188  52   240\n          Total 555 195   750\n\n\nNote the use of adorn_totals to get the marginal counts, and how we specify that we want both the row and column totals. We can add a title for the columns with…\n\nnh_750 |>\n    tabyl(Education, SleepTrouble) |>\n    adorn_totals(where = c(\"row\", \"col\")) |>\n    adorn_title(placement = \"combined\")\n\n Education/SleepTrouble  No Yes Total\n              8th Grade  40  10    50\n         9 - 11th Grade  52  24    76\n            High School 102  41   143\n           Some College 173  68   241\n           College Grad 188  52   240\n                  Total 555 195   750\n\n\nOften, we’ll want to show percentages in a cross-tabulation like this. To get row percentages so that we can directly see the probability of SleepTrouble = Yes for each level of Education, we can use:\n\nnh_750 |>\n    tabyl(Education, SleepTrouble) |>\n    adorn_totals(where = \"row\") |>\n    adorn_percentages(denominator = \"row\") |>\n    adorn_pct_formatting() |>\n    adorn_title(placement = \"combined\")\n\n Education/SleepTrouble    No   Yes\n              8th Grade 80.0% 20.0%\n         9 - 11th Grade 68.4% 31.6%\n            High School 71.3% 28.7%\n           Some College 71.8% 28.2%\n           College Grad 78.3% 21.7%\n                  Total 74.0% 26.0%\n\n\nIf we want to compare the distribution of Education between the two levels of SleepTrouble with column percentages, we can use the following…\n\nnh_750 |>\n    tabyl(Education, SleepTrouble) |>\n    adorn_totals(where = \"col\") |>\n    adorn_percentages(denominator = \"col\") |>\n    adorn_pct_formatting() |>\n    adorn_title(placement = \"combined\") \n\n Education/SleepTrouble    No   Yes Total\n              8th Grade  7.2%  5.1%  6.7%\n         9 - 11th Grade  9.4% 12.3% 10.1%\n            High School 18.4% 21.0% 19.1%\n           Some College 31.2% 34.9% 32.1%\n           College Grad 33.9% 26.7% 32.0%\n\n\nIf we want overall percentages in the cells of the table, so that the total across all combinations of Education and SleepTrouble is 100%, we can use:\n\nnh_750 |>\n    tabyl(Education, SleepTrouble) |>\n    adorn_totals(where = c(\"row\", \"col\")) |>\n    adorn_percentages(denominator = \"all\") |>\n    adorn_pct_formatting() |>\n    adorn_title(placement = \"combined\") \n\n Education/SleepTrouble    No   Yes  Total\n              8th Grade  5.3%  1.3%   6.7%\n         9 - 11th Grade  6.9%  3.2%  10.1%\n            High School 13.6%  5.5%  19.1%\n           Some College 23.1%  9.1%  32.1%\n           College Grad 25.1%  6.9%  32.0%\n                  Total 74.0% 26.0% 100.0%\n\n\nAnother common approach is to include both counts and percentages in a cross-tabulation. Let’s look at the breakdown of HealthGen by MaritalStatus.\n\nnh_750 |>\n    tabyl(MaritalStatus, HealthGen) |>\n    adorn_totals(where = c(\"row\")) |>\n    adorn_percentages(denominator = \"row\") |>\n    adorn_pct_formatting() |>\n    adorn_ns(position = \"front\") |>\n    adorn_title(placement = \"combined\") |>\n    knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaritalStatus/HealthGen\nExcellent\nVgood\nGood\nFair\nPoor\nNA_\n\n\n\n\nDivorced\n7 (9.0%)\n19 (24.4%)\n29 (37.2%)\n11 (14.1%)\n3 (3.8%)\n9 (11.5%)\n\n\nLivePartner\n4 (5.7%)\n19 (27.1%)\n25 (35.7%)\n18 (25.7%)\n0 (0.0%)\n4 (5.7%)\n\n\nMarried\n46 (11.9%)\n101 (26.0%)\n130 (33.5%)\n41 (10.6%)\n6 (1.5%)\n64 (16.5%)\n\n\nNeverMarried\n25 (14.0%)\n52 (29.1%)\n56 (31.3%)\n24 (13.4%)\n3 (1.7%)\n19 (10.6%)\n\n\nSeparated\n2 (10.5%)\n3 (15.8%)\n4 (21.1%)\n8 (42.1%)\n0 (0.0%)\n2 (10.5%)\n\n\nWidowed\n0 (0.0%)\n3 (18.8%)\n8 (50.0%)\n2 (12.5%)\n2 (12.5%)\n1 (6.2%)\n\n\nTotal\n84 (11.2%)\n197 (26.3%)\n252 (33.6%)\n104 (13.9%)\n14 (1.9%)\n99 (13.2%)\n\n\n\n\n\nWhat if we wanted to ignore the missing HealthGen values? Most often, I filter down to the complete observations.\n\nnh_750 |>\n    filter(complete.cases(MaritalStatus, HealthGen)) |>\n    tabyl(MaritalStatus, HealthGen) |>\n    adorn_totals(where = c(\"row\")) |>\n    adorn_percentages(denominator = \"row\") |>\n    adorn_pct_formatting() |>\n    adorn_ns(position = \"front\") |>\n    adorn_title(placement = \"combined\")\n\n MaritalStatus/HealthGen  Excellent       Vgood        Good        Fair\n                Divorced  7 (10.1%)  19 (27.5%)  29 (42.0%)  11 (15.9%)\n             LivePartner  4  (6.1%)  19 (28.8%)  25 (37.9%)  18 (27.3%)\n                 Married 46 (14.2%) 101 (31.2%) 130 (40.1%)  41 (12.7%)\n            NeverMarried 25 (15.6%)  52 (32.5%)  56 (35.0%)  24 (15.0%)\n               Separated  2 (11.8%)   3 (17.6%)   4 (23.5%)   8 (47.1%)\n                 Widowed  0  (0.0%)   3 (20.0%)   8 (53.3%)   2 (13.3%)\n                   Total 84 (12.9%) 197 (30.3%) 252 (38.7%) 104 (16.0%)\n       Poor\n  3  (4.3%)\n  0  (0.0%)\n  6  (1.9%)\n  3  (1.9%)\n  0  (0.0%)\n  2 (13.3%)\n 14  (2.2%)\n\n\nFor more on working with tabyls, see the vignette in the janitor package. There you’ll find a complete list of all of the adorn functions, for example.\nHere’s another approach, to look at the cross-classification of Race and HealthGen:\n\nxtabs(~ Race + HealthGen, data = nh_750)\n\n          HealthGen\nRace       Excellent Vgood Good Fair Poor\n  Asian           10    17   24    6    1\n  Black           15    28   40   24    4\n  Hispanic         4     9   24   13    2\n  Mexican          6    12   25   21    2\n  White           48   128  131   37    5\n  Other            1     3    8    3    0"
  },
  {
    "objectID": "07-summarizing_categories.html#cross-classifying-three-categorical-variables",
    "href": "07-summarizing_categories.html#cross-classifying-three-categorical-variables",
    "title": "7  Summarizing Categories",
    "section": "7.9 Cross-Classifying Three Categorical Variables",
    "text": "7.9 Cross-Classifying Three Categorical Variables\nSuppose we are interested in Smoke100 and its relationship to PhysActive and SleepTrouble.\n\nnh_750 |>\n    tabyl(Smoke100, PhysActive, SleepTrouble) |>\n    adorn_title(placement = \"top\")\n\n$No\n          PhysActive    \n Smoke100         No Yes\n       No        137 219\n      Yes         93 106\n\n$Yes\n          PhysActive    \n Smoke100         No Yes\n       No         41  56\n      Yes         55  43\n\n\nThe result here is a tabyl of Smoke100 (rows) by PhysActive (columns), split into a list by SleepTrouble.\nThere are several alternative approaches for doing this, although I expect us to stick with tabyl for our work in 431. These alternatives include the use of the xtabs function:\n\nxtabs(~ Smoke100 + PhysActive + SleepTrouble, data = nh_750)\n\n, , SleepTrouble = No\n\n        PhysActive\nSmoke100  No Yes\n     No  137 219\n     Yes  93 106\n\n, , SleepTrouble = Yes\n\n        PhysActive\nSmoke100  No Yes\n     No   41  56\n     Yes  55  43\n\n\nWe can also build a flat version of this table, as follows:\n\nftable(Smoke100 ~ PhysActive + SleepTrouble, data = nh_750)\n\n                        Smoke100  No Yes\nPhysActive SleepTrouble                 \nNo         No                    137  93\n           Yes                    41  55\nYes        No                    219 106\n           Yes                    56  43\n\n\nAnd we can do this with dplyr functions and the table() function, as well, for example…\n\nnh_750 |>\n    select(Smoke100, PhysActive, SleepTrouble) |>\n    table() \n\n, , SleepTrouble = No\n\n        PhysActive\nSmoke100  No Yes\n     No  137 219\n     Yes  93 106\n\n, , SleepTrouble = Yes\n\n        PhysActive\nSmoke100  No Yes\n     No   41  56\n     Yes  55  43"
  },
  {
    "objectID": "07-summarizing_categories.html#gaining-control-over-tables-in-r-the-gt-package",
    "href": "07-summarizing_categories.html#gaining-control-over-tables-in-r-the-gt-package",
    "title": "7  Summarizing Categories",
    "section": "7.10 Gaining Control over Tables in R: the gt package",
    "text": "7.10 Gaining Control over Tables in R: the gt package\nWith the gt package, anyone can make wonderful-looking tables using the R programming language. The gt package allows you to start with a tibble or data frame, and use it to make very detailed tables that look professional, and includes tools that enable you to include titles and subtitles, all sorts of labels, as well as footnotes and source notes.\nHere’s a fairly simple example of a cross-tabulation of part of the nh_750 data built using a few tools from the gt package.\n\ntemp_tbl <- nh_750 |> filter(complete.cases(PhysActive, HealthGen)) |>\n  tabyl(PhysActive, HealthGen) |>\n  tibble() \n\ngt(temp_tbl) |>\n  tab_header(title = md(\"**Cross-Tabulation from nh_750**\"),\n             subtitle = md(\"Physical Activity vs. Overall Health\"))\n\n\n\n\n\n  \n    \n      Cross-Tabulation from nh_750\n    \n    \n      Physical Activity vs. Overall Health\n    \n  \n  \n    \n      PhysActive\n      Excellent\n      Vgood\n      Good\n      Fair\n      Poor\n    \n  \n  \n    No\n24\n66\n126\n59\n10\n    Yes\n60\n131\n126\n45\n4\n  \n  \n  \n\n\n\n\nThe gt package and its usage is described in detail at https://gt.rstudio.com/.\n\n\n\n\nWainer, Howard. 1997. Visual Revelations: Graphical Tales of Fate and Deception from Napoleon Bonaparte to Ross Perot. New York: Springer-Verlag.\n\n\n———. 2005. Graphic Discovery: A Trout in the Milk and Other Visual Adventures. Princeton, NJ: Princeton University Press.\n\n\n———. 2013. Medical Illuminations: Using Evidence, Visualization and Statistical Thinking to Improve Healthcare. New York: Oxford University Press."
  },
  {
    "objectID": "08-missing_data.html",
    "href": "08-missing_data.html",
    "title": "8  Missing Data and Single Imputation",
    "section": "",
    "text": "Almost all serious statistical analyses have to deal with missing data. Data values that are missing are indicated in R, and to R, by the symbol NA."
  },
  {
    "objectID": "08-missing_data.html#setup-packages-used-here",
    "href": "08-missing_data.html#setup-packages-used-here",
    "title": "8  Missing Data and Single Imputation",
    "section": "8.1 Setup: Packages Used Here",
    "text": "8.1 Setup: Packages Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor)\nlibrary(naniar)\nlibrary(simputation)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\n\nWe’ll focus on tools from the naniar and simputation packages in this chapter. This chapter also requires that the mosaic package is loaded on your machine, but that package is not included with library() above."
  },
  {
    "objectID": "08-missing_data.html#a-simulated-example-with-15-subjects",
    "href": "08-missing_data.html#a-simulated-example-with-15-subjects",
    "title": "8  Missing Data and Single Imputation",
    "section": "8.2 A Simulated Example with 15 subjects",
    "text": "8.2 A Simulated Example with 15 subjects\nIn the following tiny data set called sbp_example, we have four variables for a set of 15 subjects. In addition to a subject id, we have:\n\nthe treatment this subject received (A, B or C are the treatments),\nan indicator (1 = yes, 0 = no) of whether the subject has diabetes,\nthe subject’s systolic blood pressure at baseline\nthe subject’s systolic blood pressure after the application of the treatment\n\n\n# create some temporary variables\nsubject <- 101:115\nx1 <- c(\"A\", \"B\", \"C\", \"A\", \"C\", \"A\", \"A\", NA, \"B\", \"C\", \"A\", \"B\", \"C\", \"A\", \"B\")\nx2 <- c(1, 0, 0, 1, NA, 1, 0, 1, NA, 1, 0, 0, 1, 1, NA)\nx3 <- c(120, 145, 150, NA, 155, NA, 135, NA, 115, 170, 150, 145, 140, 160, 135)\nx4 <- c(105, 135, 150, 120, 135, 115, 160, 150, 130, 155, 140, 140, 150, 135, 120)\n\nsbp_example <- \n  tibble(subject, treat = factor(x1), diabetes = x2, \n         sbp.before = x3, sbp.after = x4) \n\nrm(subject, x1, x2, x3, x4) # just cleaning up\n\nsbp_example\n\n# A tibble: 15 × 5\n   subject treat diabetes sbp.before sbp.after\n     <int> <fct>    <dbl>      <dbl>     <dbl>\n 1     101 A            1        120       105\n 2     102 B            0        145       135\n 3     103 C            0        150       150\n 4     104 A            1         NA       120\n 5     105 C           NA        155       135\n 6     106 A            1         NA       115\n 7     107 A            0        135       160\n 8     108 <NA>         1         NA       150\n 9     109 B           NA        115       130\n10     110 C            1        170       155\n11     111 A            0        150       140\n12     112 B            0        145       140\n13     113 C            1        140       150\n14     114 A            1        160       135\n15     115 B           NA        135       120"
  },
  {
    "objectID": "08-missing_data.html#identifying-missingness-with-naniar-functions",
    "href": "08-missing_data.html#identifying-missingness-with-naniar-functions",
    "title": "8  Missing Data and Single Imputation",
    "section": "8.3 Identifying missingness with naniar functions",
    "text": "8.3 Identifying missingness with naniar functions\nThe naniar package has many useful functions.\n\nHow many missing values do we have, overall?\n\n\nn_miss(sbp_example)\n\n[1] 7\n\n\n\nHow many variables have missing values, overall?\n\n\nn_var_miss(sbp_example)\n\n[1] 3\n\n\n\nWhich variables contain missing values?\n\n\nmiss_var_which(sbp_example)\n\n[1] \"treat\"      \"diabetes\"   \"sbp.before\"\n\n\n\nHow many missing values do we have in each variable?\n\n\nmiss_var_summary(sbp_example)\n\n# A tibble: 5 × 3\n  variable   n_miss pct_miss\n  <chr>       <int>    <dbl>\n1 diabetes        3    20   \n2 sbp.before      3    20   \n3 treat           1     6.67\n4 subject         0     0   \n5 sbp.after       0     0   \n\n\nWe are missing one treat, 3 diabetes and 3 sbp.before values.\n\nCan we plot missingness, by variable?\n\n\ngg_miss_var(sbp_example)\n\n\n\n\n\nHow many of the cases (rows) have missing values?\n\n\nn_case_miss(sbp_example)\n\n[1] 6\n\n\n\nHow many cases have complete data, with no missing values?\n\n\nn_case_complete(sbp_example)\n\n[1] 9\n\n\n\nCan we tabulate missingness by case?\n\n\nmiss_case_table(sbp_example)\n\n# A tibble: 3 × 3\n  n_miss_in_case n_cases pct_cases\n           <int>   <int>     <dbl>\n1              0       9     60   \n2              1       5     33.3 \n3              2       1      6.67\n\n\n\nWhich cases have missing values?\n\n\nmiss_case_summary(sbp_example)\n\n# A tibble: 15 × 3\n    case n_miss pct_miss\n   <int>  <int>    <dbl>\n 1     8      2       40\n 2     4      1       20\n 3     5      1       20\n 4     6      1       20\n 5     9      1       20\n 6    15      1       20\n 7     1      0        0\n 8     2      0        0\n 9     3      0        0\n10     7      0        0\n11    10      0        0\n12    11      0        0\n13    12      0        0\n14    13      0        0\n15    14      0        0\n\n\n\nHow can we identify the subjects with missing data?\n\n\nsbp_example |> filter(!complete.cases(sbp_example))\n\n# A tibble: 6 × 5\n  subject treat diabetes sbp.before sbp.after\n    <int> <fct>    <dbl>      <dbl>     <dbl>\n1     104 A            1         NA       120\n2     105 C           NA        155       135\n3     106 A            1         NA       115\n4     108 <NA>         1         NA       150\n5     109 B           NA        115       130\n6     115 B           NA        135       120\n\n\nWe have nine subjects with complete data, three subjects with missing diabetes (only), two subjects with missing sbp.before (only), and 1 subject who is missing both treat and sbp.before."
  },
  {
    "objectID": "08-missing_data.html#missing-data-mechanisms",
    "href": "08-missing_data.html#missing-data-mechanisms",
    "title": "8  Missing Data and Single Imputation",
    "section": "8.4 Missing-data mechanisms",
    "text": "8.4 Missing-data mechanisms\nMy source for this description of mechanisms is Chapter 25 of Gelman and Hill (2007), and that chapter is available at this link.\n\nMCAR = Missingness completely at random. A variable is missing completely at random if the probability of missingness is the same for all units, for example, if for each subject, we decide whether to collect the diabetes status by rolling a die and refusing to answer if a “6” shows up. If data are missing completely at random, then throwing out cases with missing data does not bias your inferences.\nMissingness that depends only on observed predictors. A more general assumption, called missing at random or MAR, is that the probability a variable is missing depends only on available information. Here, we would have to be willing to assume that the probability of nonresponse to diabetes depends only on the other, fully recorded variables in the data. It is often reasonable to model this process as a logistic regression, where the outcome variable equals 1 for observed cases and 0 for missing. When an outcome variable is missing at random, it is acceptable to exclude the missing cases (that is, to treat them as NA), as long as the regression controls for all the variables that affect the probability of missingness.\nMissingness that depends on unobserved predictors. Missingness is no longer “at random” if it depends on information that has not been recorded and this information also predicts the missing values. If a particular treatment causes discomfort, a patient is more likely to drop out of the study. This missingness is not at random (unless “discomfort” is measured and observed for all patients). If missingness is not at random, it must be explicitly modeled, or else you must accept some bias in your inferences.\nMissingness that depends on the missing value itself. Finally, a particularly difficult situation arises when the probability of missingness depends on the (potentially missing) variable itself. For example, suppose that people with higher earnings are less likely to reveal them.\n\nEssentially, situations 3 and 4 are referred to collectively as non-random missingness, and cause more trouble for us than 1 and 2."
  },
  {
    "objectID": "08-missing_data.html#options-for-dealing-with-missingness",
    "href": "08-missing_data.html#options-for-dealing-with-missingness",
    "title": "8  Missing Data and Single Imputation",
    "section": "8.5 Options for Dealing with Missingness",
    "text": "8.5 Options for Dealing with Missingness\nThere are several available methods for dealing with missing data that are MCAR or MAR, but they basically boil down to:\n\nComplete Case (or Available Case) analyses\nSingle Imputation\nMultiple Imputation"
  },
  {
    "objectID": "08-missing_data.html#complete-case-and-available-case-analyses",
    "href": "08-missing_data.html#complete-case-and-available-case-analyses",
    "title": "8  Missing Data and Single Imputation",
    "section": "8.6 Complete Case (and Available Case) analyses",
    "text": "8.6 Complete Case (and Available Case) analyses\nIn Complete Case analyses, rows containing NA values are omitted from the data before analyses commence. This is the default approach for many statistical software packages, and may introduce unpredictable bias and fail to include some useful, often hard-won information.\n\nA complete case analysis can be appropriate when the number of missing observations is not large, and the missing pattern is either MCAR (missing completely at random) or MAR (missing at random.)\nTwo problems arise with complete-case analysis:\n\nIf the units with missing values differ systematically from the completely observed cases, this could bias the complete-case analysis.\nIf many variables are included in a model, there may be very few complete cases, so that most of the data would be discarded for the sake of a straightforward analysis.\n\nA related approach is available-case analysis where different aspects of a problem are studied with different subsets of the data, perhaps identified on the basis of what is missing in them."
  },
  {
    "objectID": "08-missing_data.html#single-imputation",
    "href": "08-missing_data.html#single-imputation",
    "title": "8  Missing Data and Single Imputation",
    "section": "8.7 Single Imputation",
    "text": "8.7 Single Imputation\nIn single imputation analyses, NA values are estimated/replaced one time with one particular data value for the purpose of obtaining more complete samples, at the expense of creating some potential bias in the eventual conclusions or obtaining slightly less accurate estimates than would be available if there were no missing values in the data.\n\nA single imputation can be just a replacement with the mean or median (for a quantity) or the mode (for a categorical variable.) However, such an approach, though easy to understand, underestimates variance and ignores the relationship of missing values to other variables.\nSingle imputation can also be done using a variety of models to try to capture information about the NA values that are available in other variables within the data set.\nThe simputation package can help us execute single imputations using a wide variety of techniques, within the pipe approach used by the tidyverse. Another approach I have used in the past is the mice package, which can also perform single imputations."
  },
  {
    "objectID": "08-missing_data.html#multiple-imputation",
    "href": "08-missing_data.html#multiple-imputation",
    "title": "8  Missing Data and Single Imputation",
    "section": "8.8 Multiple Imputation",
    "text": "8.8 Multiple Imputation\nMultiple imputation, where NA values are repeatedly estimated/replaced with multiple data values, for the purpose of obtaining mode complete samples and capturing details of the variation inherent in the fact that the data have missingness, so as to obtain more accurate estimates than are possible with single imputation.\n\nWe’ll postpone further discussion of multiple imputation to later in the semester."
  },
  {
    "objectID": "08-missing_data.html#building-a-complete-case-analysis",
    "href": "08-missing_data.html#building-a-complete-case-analysis",
    "title": "8  Missing Data and Single Imputation",
    "section": "8.9 Building a Complete Case Analysis",
    "text": "8.9 Building a Complete Case Analysis\nWe can drop all of the missing values from a data set with drop_na or with na.omit or by filtering for complete.cases. Any of these approaches produces the same result - a new data set with 9 rows (after dropping the six subjects with any NA values) and 5 columns.\n\ncc.1 <- na.omit(sbp_example)\ncc.2 <- sbp_example |> drop_na()\ncc.3 <- sbp_example |> filter(complete.cases(sbp_example))"
  },
  {
    "objectID": "08-missing_data.html#single-imputation-with-the-mean-or-mode",
    "href": "08-missing_data.html#single-imputation-with-the-mean-or-mode",
    "title": "8  Missing Data and Single Imputation",
    "section": "8.10 Single Imputation with the Mean or Mode",
    "text": "8.10 Single Imputation with the Mean or Mode\nThe most straightforward approach to single imputation is to impute a single summary of the variable, such as the mean, median or mode.\n\nmosaic::favstats(~ sbp.before, data = sbp_example)\n\n min  Q1 median     Q3 max     mean       sd  n missing\n 115 135    145 151.25 170 143.3333 15.71527 12       3\n\n\n\nsbp_example |> tabyl(diabetes, treat) |>\n  adorn_totals(where = c(\"row\", \"col\"))\n\n diabetes A B C NA_ Total\n        0 2 2 1   0     5\n        1 4 0 2   1     7\n     <NA> 0 2 1   0     3\n    Total 6 4 4   1    15\n\n\nHere, suppose we decide to impute\n\nsbp.before with the mean (143.3) among non-missing values,\ndiabetes with its more common value, 1, and\ntreat with its more common value, or mode (A)\n\n\nsi.1 <- sbp_example |>\n    replace_na(list(sbp.before = 143.33,\n                    diabetes = 1,\n                    treat = \"A\"))\nsi.1\n\n# A tibble: 15 × 5\n   subject treat diabetes sbp.before sbp.after\n     <int> <fct>    <dbl>      <dbl>     <dbl>\n 1     101 A            1       120        105\n 2     102 B            0       145        135\n 3     103 C            0       150        150\n 4     104 A            1       143.       120\n 5     105 C            1       155        135\n 6     106 A            1       143.       115\n 7     107 A            0       135        160\n 8     108 A            1       143.       150\n 9     109 B            1       115        130\n10     110 C            1       170        155\n11     111 A            0       150        140\n12     112 B            0       145        140\n13     113 C            1       140        150\n14     114 A            1       160        135\n15     115 B            1       135        120"
  },
  {
    "objectID": "08-missing_data.html#doing-single-imputation-with-simputation",
    "href": "08-missing_data.html#doing-single-imputation-with-simputation",
    "title": "8  Missing Data and Single Imputation",
    "section": "8.11 Doing Single Imputation with simputation",
    "text": "8.11 Doing Single Imputation with simputation\nSingle imputation is a potentially appropriate method when missingness can be assumed to be either completely at random (MCAR) or dependent only on observed predictors (MAR). We’ll use the simputation package to accomplish it.\n\nThe simputation vignette is available at https://cran.r-project.org/web/packages/simputation/vignettes/intro.html\nThe simputation reference manual is available at https://cran.r-project.org/web/packages/simputation/simputation.pdf\n\nSuppose we wanted to use:\n\na robust linear model to predict sbp.before missing values, on the basis of sbp.after and diabetes status, and\na predictive mean matching approach (which, unlike the robust linear model, will ensure that only values of diabetes that we’ve seen before will be imputed) to predict diabetes status, on the basis of sbp.after, and\na decision tree approach to predict treat status, using all other variables in the data\n\n\nset.seed(50001)\n\nimp.2 <- sbp_example |>\n    impute_rlm(sbp.before ~ sbp.after + diabetes) |>\n    impute_pmm(diabetes ~ sbp.after) |>\n    impute_cart(treat ~ .)\n\nimp.2\n\n# A tibble: 15 × 5\n   subject treat diabetes sbp.before sbp.after\n *   <int> <fct>    <dbl>      <dbl>     <dbl>\n 1     101 A            1       120        105\n 2     102 B            0       145        135\n 3     103 C            0       150        150\n 4     104 A            1       139.       120\n 5     105 C            1       155        135\n 6     106 A            1       136.       115\n 7     107 A            0       135        160\n 8     108 A            1       155.       150\n 9     109 B            1       115        130\n10     110 C            1       170        155\n11     111 A            0       150        140\n12     112 B            0       145        140\n13     113 C            1       140        150\n14     114 A            1       160        135\n15     115 B            1       135        120\n\n\nDetails on the many available methods in simputation are provided in its manual. These include:\n\nimpute_cart uses a Classification and Regression Tree approach for numerical or categorical data. There is also an impute_rf command which uses Random Forests for imputation.\nimpute_pmm is one of several “hot deck” options for imputation, this one is predictive mean matching, which can be used with numeric data (only). Missing values are first imputed using a predictive model. Next, these predictions are replaced with the observed values which are nearest to the prediction. Other imputation options in this group include random hot deck, sequential hot deck and k-nearest neighbor imputation.\nimpute_rlm is one of several regression imputation methods, including linear models, robust linear models (which use what is called M-estimation to impute numerical variables) and lasso/elastic net/ridge regression models.\n\nThe simputation package can also do EM-based multivariate imputation, and multivariate random forest imputation, and several other approaches.\n\n\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel-Hierarchical Models. New York: Cambridge University Press. http://www.stat.columbia.edu/~gelman/arm/."
  },
  {
    "objectID": "09-nnyfs_foundations.html",
    "href": "09-nnyfs_foundations.html",
    "title": "9  National Youth Fitness Survey",
    "section": "",
    "text": "knitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\n\nWe also use functions from the Hmisc and mosaic packages in this chapter, but do not load the whole packages."
  },
  {
    "objectID": "09-nnyfs_foundations.html#what-is-the-nhanes-nyfs",
    "href": "09-nnyfs_foundations.html#what-is-the-nhanes-nyfs",
    "title": "9  National Youth Fitness Survey",
    "section": "9.2 What is the NHANES NYFS?",
    "text": "9.2 What is the NHANES NYFS?\nThe nnyfs.csv and the nnyfs.Rds data files were built by Professor Love using data from the 2012 National Youth Fitness Survey.\n\nThe NHANES National Youth Fitness Survey (NNYFS) was conducted in 2012 to collect data on physical activity and fitness levels in order to provide an evaluation of the health and fitness of children in the U.S. ages 3 to 15. The NNYFS collected data on physical activity and fitness levels of our youth through interviews and fitness tests.\n\nIn the nnyfs data file (either .csv or .Rds), I’m only providing a modest fraction of the available information. More on the NNYFS (including information I’m not using) is available at https://wwwn.cdc.gov/nchs/nhanes/search/nnyfs12.aspx.\nThe data elements I’m using fall into four main groups, or components:\n\nDemographics\nDietary\nExamination and\nQuestionnaire\n\nWhat I did was merge a few elements from each of the available components of the NHANES National Youth Fitness Survey, reformulated (and in some cases simplified) some variables, and restricted the sample to kids who had completed elements of each of the four components."
  },
  {
    "objectID": "09-nnyfs_foundations.html#the-variables-included-in-nnyfs",
    "href": "09-nnyfs_foundations.html#the-variables-included-in-nnyfs",
    "title": "9  National Youth Fitness Survey",
    "section": "9.3 The Variables included in nnyfs",
    "text": "9.3 The Variables included in nnyfs\nThis section tells you where the data come from, and briefly describe what is collected.\n\n9.3.1 From the NNYFS Demographic Component\nAll of these come from the Y_DEMO file.\n\n\n\n\n\n\n\n\nIn nnyfs\nIn Y_DEMO\nDescription\n\n\n\n\nSEQN\nSEQN\nSubject ID, connects all of the files\n\n\nsex\nRIAGENDR\nReally, this is sex, not gender\n\n\nage_child\nRIDAGEYR\nAge in years at screening\n\n\nrace_eth\nRIDRETH1\nRace/Hispanic origin (collapsed to 4 levels)\n\n\neduc_child\nDMDEDUC3\nEducation Level (for children ages 6-15). 0 = Kindergarten, 9 = Ninth grade or higher\n\n\nlanguage\nSIALANG\nLanguage in which the interview was conducted\n\n\nsampling_wt\nWTMEC\nFull-sample MEC exam weight (for inference)\n\n\nincome_pov\nINDFMPIR\nRatio of family income to poverty (ceiling is 5.0)\n\n\nage_adult\nDMDHRAGE\nAge of adult who brought child to interview\n\n\neduc_adult\nDMDHREDU\nEducation level of adult who brought child\n\n\n\n\n\n9.3.2 From the NNYFS Dietary Component\nFrom the Y_DR1TOT file, we have a number of variables related to the child’s diet, with the following summaries mostly describing consumption “yesterday” in a dietary recall questionnaire.\n\n\n\n\n\n\n\n\nIn nnyfs\nIn Y_DR1TOT\nDescription\n\n\n\n\nrespondent\nDR1MNRSP\nwho responded to interview (child, Mom, someone else)\n\n\nsalt_used\nDBQ095Z\nuses salt, lite salt or salt substitute at the table\n\n\nenergy\nDR1TKCAL\nenergy consumed (kcal)\n\n\nprotein\nDR1TPROT\nprotein consumed (g)\n\n\nsugar\nDR1TSUGR\ntotal sugar consumed (g)\n\n\nfat\nDR1TTFAT\ntotal fat consumed (g)\n\n\ndiet_yesterday\nDR1_300\ncompare food consumed yesterday to usual amount\n\n\nwater\nDR1_320Z\ntotal plain water drank (g)\n\n\n\n\n\n9.3.3 From the NNYFS Examination Component\nFrom the Y_BMX file of Body Measures:\n\n\n\nIn nnyfs\nIn Y_BMX\nDescription\n\n\n\n\nheight\nBMXHT\nstanding height (cm)\n\n\nweight\nBMXWT\nweight (kg)\n\n\nbmi\nBMXBMI\nbody mass index (\\(kg/m^2\\))\n\n\nbmi_cat\nBMDBMIC\nBMI category (4 levels)\n\n\narm_length\nBMXARML\nUpper arm length (cm)\n\n\nwaist\nBMXWAIST\nWaist circumference (cm)\n\n\narm_circ\nBMXARMC\nArm circumference (cm)\n\n\ncalf_circ\nBMXCALF\nMaximal calf circumference (cm)\n\n\ncalf_skinfold\nBMXCALFF\nCalf skinfold (mm)\n\n\ntriceps_skinfold\nBMXTRI\nTriceps skinfold (mm)\n\n\nsubscapular_skinfold\nBMXSUB\nSubscapular skinfold (mm)\n\n\n\nFrom the Y_PLX file of Plank test results:\n\n\n\nIn nnyfs\nIn Y_PLX\nDescription\n\n\n\n\nplank_time\nMPXPLANK\n# of seconds plank position is held\n\n\n\n\n\n9.3.4 From the NNYFS Questionnaire Component\nFrom the Y_PAQ file of Physical Activity questions:\n\n\n\n\n\n\n\n\nIn nnyfs\nIn Y_PAQ\nDescription\n\n\n\n\nactive_days\nPAQ706\nDays physically active (\\(\\geq 60\\) min.) in past week\n\n\ntv_hours\nPAQ710\nAverage hours watching TV/videos past 30d\n\n\ncomputer_hours\nPAQ715\nAverage hours on computer past 30d\n\n\nphysical_last_week\nPAQ722\nAny physical activity outside of school past week\n\n\nenjoy_recess\nPAQ750\nEnjoy participating in PE/recess\n\n\n\nFrom the Y_DBQ file of Diet Behavior and Nutrition questions:\n\n\n\nIn nnyfs\nIn Y_DBQ\nDescription\n\n\n\n\nmeals_out\nDBD895\n# meals not home-prepared in past 7 days\n\n\n\nFrom the Y_HIQ file of Health Insurance questions:\n\n\n\nIn nnyfs\nIn Y_HIQ\nDescription\n\n\n\n\ninsured\nHIQ011\nCovered by Health Insurance?\n\n\ninsurance\nHIQ031\nType of Health Insurance coverage\n\n\n\nFrom the Y_HUQ file of Access to Care questions:\n\n\n\nIn nnyfs\nIn Y_HUQ\nDescription\n\n\n\n\nphys_health\nHUQ010\nGeneral health condition (Excellent - Poor)\n\n\naccess_to_care\nHUQ030\nRoutine place to get care?\n\n\ncare_source\nHUQ040\nType of place most often goes to for care\n\n\n\nFrom the Y_MCQ file of Medical Conditions questions:\n\n\n\nIn nnyfs\nIn Y_MCQ\nDescription\n\n\n\n\nasthma_ever\nMCQ010\nEver told you have asthma?\n\n\nasthma_now\nMCQ035\nStill have asthma?\n\n\n\nFrom the Y_RXQ_RX file of Prescription Medication questions:\n\n\n\nIn nnyfs\nIn Y_RXQ_RX\nDescription\n\n\n\n\nmed_use\nRXDUSE\nTaken prescription medication in last month?\n\n\nmed_count\nRXDCOUNT\n# of prescription meds taken in past month"
  },
  {
    "objectID": "09-nnyfs_foundations.html#looking-over-a-few-variables",
    "href": "09-nnyfs_foundations.html#looking-over-a-few-variables",
    "title": "9  National Youth Fitness Survey",
    "section": "9.4 Looking over A Few Variables",
    "text": "9.4 Looking over A Few Variables\nNow, I’ll take a look at the nnyfs data, which I’ve made available in a comma-separated version (nnyfs.csv), if you prefer, as well as in an R data set (nnyfs.Rds) which loads a bit faster. After loading the file, let’s get a handle on its size and contents. In my R Project for these notes, the data are contained in a separate data subdirectory.\n\nnnyfs <- readRDS(\"data/nnyfs.Rds\")\n\n## size of the tibble\ndim(nnyfs)\n\n[1] 1518   45\n\n\nThere are 1518 rows (subjects) and 45 columns (variables), by which I mean that there are 1518 kids in the nnyfs data frame, and we have 45 pieces of information on each subject. So, what do we have, exactly?\n\nnnyfs # this is a tibble, has some nice features in a print-out like this\n\n# A tibble: 1,518 × 45\n    SEQN sex    age_ch…¹ race_…² educ_…³ langu…⁴ sampl…⁵ incom…⁶ age_a…⁷ educ_…⁸\n   <dbl> <fct>     <dbl> <fct>     <dbl> <fct>     <dbl>   <dbl>   <dbl> <fct>  \n 1 71917 Female       15 3_Blac…       9 English  28299.    0.21      46 2_9-11…\n 2 71918 Female        8 3_Blac…       2 English  15127.    5         46 3_High…\n 3 71919 Female       14 2_Whit…       8 English  29977.    5         42 5_Coll…\n 4 71920 Female       15 2_Whit…       8 English  80652.    0.87      53 3_High…\n 5 71921 Male          3 2_Whit…      NA English  55592.    4.34      31 3_High…\n 6 71922 Male         12 1_Hisp…       6 English  27365.    5         42 4_Some…\n 7 71923 Male         12 2_Whit…       5 English  86673.    5         39 2_9-11…\n 8 71924 Female        8 4_Othe…       2 English  39549.    2.74      31 3_High…\n 9 71925 Male          7 1_Hisp…       0 English  42333.    0.46      45 2_9-11…\n10 71926 Male          8 3_Blac…       2 English  15307.    1.57      56 3_High…\n# … with 1,508 more rows, 35 more variables: respondent <fct>, salt_used <fct>,\n#   energy <dbl>, protein <dbl>, sugar <dbl>, fat <dbl>, diet_yesterday <fct>,\n#   water <dbl>, plank_time <dbl>, height <dbl>, weight <dbl>, bmi <dbl>,\n#   bmi_cat <fct>, arm_length <dbl>, waist <dbl>, arm_circ <dbl>,\n#   calf_circ <dbl>, calf_skinfold <dbl>, triceps_skinfold <dbl>,\n#   subscapular_skinfold <dbl>, active_days <dbl>, tv_hours <dbl>,\n#   computer_hours <dbl>, physical_last_week <fct>, enjoy_recess <fct>, …\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nTibbles are a modern reimagining of the main way in which people have stored data in R, called a data frame. Tibbles were developed to keep what time has proven to be effective, and throwing out what is not. We can learn something about the structure of the tibble from such functions as str or glimpse.\n\nstr(nnyfs)\n\ntibble [1,518 × 45] (S3: tbl_df/tbl/data.frame)\n $ SEQN                : num [1:1518] 71917 71918 71919 71920 71921 ...\n $ sex                 : Factor w/ 2 levels \"Female\",\"Male\": 1 1 1 1 2 2 2 1 2 2 ...\n $ age_child           : num [1:1518] 15 8 14 15 3 12 12 8 7 8 ...\n $ race_eth            : Factor w/ 4 levels \"1_Hispanic\",\"2_White Non-Hispanic\",..: 3 3 2 2 2 1 2 4 1 3 ...\n $ educ_child          : num [1:1518] 9 2 8 8 NA 6 5 2 0 2 ...\n $ language            : Factor w/ 2 levels \"English\",\"Spanish\": 1 1 1 1 1 1 1 1 1 1 ...\n $ sampling_wt         : num [1:1518] 28299 15127 29977 80652 55592 ...\n $ income_pov          : num [1:1518] 0.21 5 5 0.87 4.34 5 5 2.74 0.46 1.57 ...\n $ age_adult           : num [1:1518] 46 46 42 53 31 42 39 31 45 56 ...\n $ educ_adult          : Factor w/ 5 levels \"1_Less than 9th Grade\",..: 2 3 5 3 3 4 2 3 2 3 ...\n $ respondent          : Factor w/ 3 levels \"Child\",\"Mom\",..: 1 2 1 1 2 1 1 1 2 1 ...\n $ salt_used           : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 1 2 2 2 1 2 ...\n $ energy              : num [1:1518] 2844 1725 2304 1114 1655 ...\n $ protein             : num [1:1518] 169.1 55.2 199.3 14 50.6 ...\n $ sugar               : num [1:1518] 128.2 118.7 81.4 119.2 90.3 ...\n $ fat                 : num [1:1518] 127.9 63.7 86.1 36 53.3 ...\n $ diet_yesterday      : Factor w/ 3 levels \"1_Much more than usual\",..: 2 2 2 2 2 2 1 2 2 3 ...\n $ water               : num [1:1518] 607 178 503 859 148 ...\n $ plank_time          : num [1:1518] NA 45 121 45 11 107 127 44 184 58 ...\n $ height              : num [1:1518] NA 131.6 172 167.1 90.2 ...\n $ weight              : num [1:1518] NA 38.6 58.7 92.5 12.4 66.4 56.7 22.2 20.9 28.3 ...\n $ bmi                 : num [1:1518] NA 22.3 19.8 33.1 15.2 25.9 22.5 14.4 15.9 17 ...\n $ bmi_cat             : Factor w/ 4 levels \"1_Underweight\",..: NA 4 2 4 2 4 3 2 2 2 ...\n $ arm_length          : num [1:1518] NA 27.7 38.4 35.9 18.3 34.2 33 26.5 24.2 26 ...\n $ waist               : num [1:1518] NA 71.9 79.4 96.4 46.8 90 72.3 56.1 54.5 59.7 ...\n $ arm_circ            : num [1:1518] NA 25.4 26 37.9 15.1 29.5 27.9 17.6 17.7 19.9 ...\n $ calf_circ           : num [1:1518] NA 32.3 35.3 46.8 19.4 36.9 36.8 24 24.3 27.3 ...\n $ calf_skinfold       : num [1:1518] NA 22 18.4 NA 8.4 22 18.3 7 7.2 8.2 ...\n $ triceps_skinfold    : num [1:1518] NA 19.9 15 20.6 8.6 22.8 20.5 12.9 6.9 8.8 ...\n $ subscapular_skinfold: num [1:1518] NA 17.4 9.8 22.8 5.7 24.4 12.6 6.8 4.8 6.1 ...\n $ active_days         : num [1:1518] 3 5 3 3 7 2 5 3 7 7 ...\n $ tv_hours            : num [1:1518] 2 2 1 3 2 3 0 4 2 2 ...\n $ computer_hours      : num [1:1518] 1 2 3 3 0 1 0 3 1 1 ...\n $ physical_last_week  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 2 2 2 2 2 2 2 ...\n $ enjoy_recess        : Factor w/ 5 levels \"1_Strongly Agree\",..: 1 1 3 2 NA 2 2 NA 1 1 ...\n $ meals_out           : num [1:1518] 0 2 3 2 1 1 2 1 0 2 ...\n $ insured             : Factor w/ 2 levels \"Has Insurance\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ phys_health         : Factor w/ 5 levels \"1_Excellent\",..: 1 3 1 3 1 1 3 1 2 1 ...\n $ access_to_care      : Factor w/ 2 levels \"Has Usual Care Source\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ care_source         : Factor w/ 6 levels \"Clinic or Health Center\",..: 1 2 2 2 2 2 2 2 2 2 ...\n $ asthma_ever         : Factor w/ 2 levels \"History of Asthma\",..: 2 1 2 1 2 2 2 2 2 2 ...\n $ asthma_now          : Factor w/ 2 levels \"Asthma Now\",\"No Asthma Now\": 2 1 2 1 2 2 2 2 2 2 ...\n $ med_use             : Factor w/ 2 levels \"Had Medication\",..: 2 1 2 1 2 2 2 2 2 2 ...\n $ med_count           : num [1:1518] 0 1 0 2 0 0 0 0 0 0 ...\n $ insurance           : Factor w/ 10 levels \"Medicaid\",\"Medicare\",..: 8 8 5 8 5 5 5 5 8 1 ...\n\n\nThere are a lot of variables here. Let’s run through the first few in a little detail.\n\n9.4.1 SEQN\nThe first variable, SEQN is just a (numerical) identifying code attributable to a given subject of the survey. This is nominal data, which will be of little interest down the line. On some occasions, as in this case, the ID numbers are sequential, in the sense that subject 71919 was included in the data base after subject 71918, but this fact isn’t particularly interesting here, because the protocol remained unchanged throughout the study.\n\n\n9.4.2 sex\nThe second variable, sex, is listed as a factor variable (R uses factor and character to refer to categorical, especially non-numeric information). Here, as we can see below, we have two levels, Female and Male.\n\nnnyfs |>\n  tabyl(sex) |>\n  adorn_totals() |>\n  adorn_pct_formatting()\n\n    sex    n percent\n Female  760   50.1%\n   Male  758   49.9%\n  Total 1518  100.0%\n\n\n\n\n9.4.3 age_child\nThe third variable, age_child, is the age of the child at the time of their screening to be in the study, measured in years. Note that age is a continuous concept, but the measure used here (number of full years alive) is a common discrete approach to measurement. Age, of course, has a meaningful zero point, so this can be thought of as a ratio variable; a child who is 6 is half as old as one who is 12. We can tabulate the observed values, since there are only a dozen or so.\n\nnnyfs |> tabyl(age_child) |>\n  adorn_pct_formatting()\n\n age_child   n percent\n         3 110    7.2%\n         4 112    7.4%\n         5 114    7.5%\n         6 129    8.5%\n         7 123    8.1%\n         8 112    7.4%\n         9  99    6.5%\n        10 124    8.2%\n        11 111    7.3%\n        12 137    9.0%\n        13 119    7.8%\n        14 130    8.6%\n        15  98    6.5%\n\n\nAt the time of initial screening, these children should have been between 3 and 15 years of age, so things look reasonable. Since this is a meaningful quantitative variable, we may be interested in a more descriptive summary.\n\nnnyfs |> select(age_child) |> \n  summary()\n\n   age_child     \n Min.   : 3.000  \n 1st Qu.: 6.000  \n Median : 9.000  \n Mean   : 9.033  \n 3rd Qu.:12.000  \n Max.   :15.000  \n\n\nThese six numbers provide a nice, if incomplete, look at the ages.\n\nMin. = the minimum, or youngest age at the examination was 3 years old.\n1st Qu. = the first quartile (25th percentile) of the ages was 6. This means that 25 percent of the subjects were age 6 or less.\nMedian = the second quartile (50th percentile) of the ages was 9. This is often used to describe the center of the data. Half of the subjects were age 9 or less.\n3rd Qu. = the third quartile (75th percentile) of the ages was 12\nMax. = the maximum, or oldest age at the examination was 15 years.\n\nWe could get the standard deviation and a count of missing and non-missing observations with favstats from the mosaic package.\n\nmosaic::favstats(~ age_child, data = nnyfs) |>\n  kable(digits = 1)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\n\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\n\n3\n6\n9\n12\n15\n9\n3.7\n1518\n0\n\n\n\n\n\n\n\n9.4.4 race_eth\nThe fourth variable in the data set is race_eth, which is a multi-categorical variable describing the child’s race and ethnicity.\n\nnnyfs |> tabyl(race_eth) |> \n  adorn_pct_formatting() |>\n  knitr::kable()\n\n\n\n\nrace_eth\nn\npercent\n\n\n\n\n1_Hispanic\n450\n29.6%\n\n\n2_White Non-Hispanic\n610\n40.2%\n\n\n3_Black Non-Hispanic\n338\n22.3%\n\n\n4_Other Race/Ethnicity\n120\n7.9%\n\n\n\n\n\nAnd now, we get the idea of looking at whether our numerical summaries of the children’s ages varies by their race/ethnicity…\n\nmosaic::favstats(age_child ~ race_eth, data = nnyfs)\n\n                race_eth min   Q1 median Q3 max     mean       sd   n missing\n1             1_Hispanic   3 5.25    9.0 12  15 8.793333 3.733846 450       0\n2   2_White Non-Hispanic   3 6.00    9.0 12  15 9.137705 3.804421 610       0\n3   3_Black Non-Hispanic   3 6.00    9.0 12  15 9.038462 3.576423 338       0\n4 4_Other Race/Ethnicity   3 7.00    9.5 12  15 9.383333 3.427970 120       0\n\n\n\n\n9.4.5 income_pov\nSkipping down a bit, let’s look at the family income as a multiple of the poverty level. Here’s the summary.\n\nnnyfs |> select(income_pov) |> summary()\n\n   income_pov   \n Min.   :0.000  \n 1st Qu.:0.870  \n Median :1.740  \n Mean   :2.242  \n 3rd Qu.:3.520  \n Max.   :5.000  \n NA's   :89     \n\n\nWe see there is some missing data here. Let’s ignore that for the moment and concentrate on interpreting the results for the children with actual data. We should start with a picture.\n\nggplot(nnyfs, aes(x = income_pov)) +\n  geom_histogram(bins = 30, fill = \"white\", col = \"blue\")\n\nWarning: Removed 89 rows containing non-finite values (stat_bin).\n\n\n\n\n\nThe histogram shows us that the values are truncated at 5, so that children whose actual family income is above 5 times the poverty line are listed as 5. We also see a message reminding us that some of the data are missing for this variable.\nIs there a relationship between income_pov and race_eth in these data?\n\nmosaic::favstats(income_pov ~ race_eth, data = nnyfs) |>\n  kable(digits = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrace_eth\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\n1_Hispanic\n0\n0.6\n1.0\n1.7\n5\n1.3\n1.1\n409\n41\n\n\n2_White Non-Hispanic\n0\n1.5\n3.0\n4.5\n5\n2.9\n1.6\n588\n22\n\n\n3_Black Non-Hispanic\n0\n0.8\n1.6\n2.8\n5\n2.0\n1.5\n328\n10\n\n\n4_Other Race/Ethnicity\n0\n1.2\n2.7\n4.6\n5\n2.8\n1.7\n104\n16\n\n\n\n\n\nThis deserves a picture. Let’s try a boxplot.\n\nggplot(nnyfs, aes(x = race_eth, y = income_pov)) +\n  geom_boxplot()\n\nWarning: Removed 89 rows containing non-finite values (stat_boxplot).\n\n\n\n\n\n\n\n9.4.6 bmi\nMoving into the body measurement data, bmi is the body-mass index of the child. The BMI is a person’s weight in kilograms divided by his or her height in meters squared. Symbolically, BMI = weight in kg / (height in m)2. This is a continuous concept, measured to as many decimal places as you like, and it has a meaningful zero point, so it’s a ratio variable.\n\nnnyfs |> select(bmi) |> summary()\n\n      bmi       \n Min.   :11.90  \n 1st Qu.:15.90  \n Median :18.10  \n Mean   :19.63  \n 3rd Qu.:21.90  \n Max.   :48.30  \n NA's   :4      \n\n\nWhy would a table of these BMI values not be a great idea, for these data? A hint is that R represents this variable as num or numeric in its depiction of the data structure, and this implies that R has some decimal values stored. Here, I’ll use the head() function and the tail() function to show the first few and the last few values of what would prove to be a very long table of bmi values.\n\nnnyfs |> tabyl(bmi) |> \n  adorn_pct_formatting() |> \n  head()\n\n  bmi n percent valid_percent\n 11.9 1    0.1%          0.1%\n 12.6 1    0.1%          0.1%\n 12.7 1    0.1%          0.1%\n 12.9 1    0.1%          0.1%\n 13.0 2    0.1%          0.1%\n 13.1 1    0.1%          0.1%\n\n\n\nnnyfs |> tabyl(bmi) |> \n  adorn_pct_formatting() |> \n  tail()\n\n  bmi n percent valid_percent\n 42.8 1    0.1%          0.1%\n 43.0 1    0.1%          0.1%\n 46.9 1    0.1%          0.1%\n 48.2 1    0.1%          0.1%\n 48.3 1    0.1%          0.1%\n   NA 4    0.3%             -\n\n\n\n\n9.4.7 bmi_cat\nNext I’ll look at the bmi_cat information. This is a four-category ordinal variable, which divides the sample according to BMI into four groups. The BMI categories use sex-specific 2000 BMI-for-age (in months) growth charts prepared by the Centers for Disease Control for the US. We can get the breakdown from a table of the variable’s values.\n\nnnyfs |> tabyl(bmi_cat) |> adorn_pct_formatting()\n\n       bmi_cat   n percent valid_percent\n 1_Underweight  41    2.7%          2.7%\n      2_Normal 920   60.6%         60.8%\n  3_Overweight 258   17.0%         17.0%\n       4_Obese 295   19.4%         19.5%\n          <NA>   4    0.3%             -\n\n\nIn terms of percentiles by age and sex from the growth charts, the meanings of the categories are:\n\nUnderweight (BMI < 5th percentile)\nNormal weight (BMI 5th to < 85th percentile)\nOverweight (BMI 85th to < 95th percentile)\nObese (BMI \\(\\geq\\) 95th percentile)\n\nNote how I’ve used labels in the bmi_cat variable that include a number at the start so that the table results are sorted in a rational way. R sorts tables alphabetically, in general. We’ll use the forcats package to work with categorical variables that we store as factors eventually, but for now, we’ll keep things relatively simple.\nNote that the bmi_cat data don’t completely separate out the raw bmi data, because the calculation of percentiles requires different tables for each combination of age and sex.\n\nmosaic::favstats(bmi ~ bmi_cat, data = nnyfs) |>\n  kable(digits = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbmi_cat\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\n1_Underweight\n11.9\n13.4\n13.7\n15.0\n16.5\n14.1\n1.1\n41\n0\n\n\n2_Normal\n13.5\n15.4\n16.5\n18.7\n24.0\n17.2\n2.3\n920\n0\n\n\n3_Overweight\n16.9\n18.3\n21.4\n23.4\n27.9\n21.2\n2.9\n258\n0\n\n\n4_Obese\n17.9\n22.3\n26.2\n30.2\n48.3\n26.7\n5.7\n295\n0\n\n\n\n\n\n\n\n9.4.8 waist\nLet’s also look briefly at waist, which is the circumference of the child’s waist, in centimeters. Again, this is a numeric variable, so perhaps we’ll stick to the simple summary, rather than obtaining a table of observed values.\n\nmosaic::favstats(~ waist, data = nnyfs) \n\n  min   Q1 median   Q3   max     mean       sd    n missing\n 42.5 55.6   64.8 76.6 144.7 67.70536 15.19809 1512       6\n\n\nHere’s a histogram of the waist circumference data.\n\nggplot(nnyfs, aes(x = waist)) +\n  geom_histogram(bins = 25, fill = \"tomato\", color = \"cyan\")\n\nWarning: Removed 6 rows containing non-finite values (stat_bin).\n\n\n\n\n\n\n\n9.4.9 triceps_skinfold\nThe last variable I’ll look at for now is triceps_skinfold, which is measured in millimeters. This is one of several common locations used for the assessment of body fat using skinfold calipers, and is a frequent part of growth assessments in children. Again, this is a numeric variable according to R.\n\nmosaic::favstats(~ triceps_skinfold, data = nnyfs)\n\n min  Q1 median Q3  max     mean       sd    n missing\n   4 9.1   12.4 18 38.8 14.35725 6.758825 1497      21\n\n\nAnd here’s a histogram of the triceps skinfold data, with the fill and color flipped from what we saw in the plot of the waist circumference data a moment ago.\n\nggplot(nnyfs, aes(x = triceps_skinfold)) +\n  geom_histogram(bins = 25, fill = \"cyan\", color = \"tomato\")\n\nWarning: Removed 21 rows containing non-finite values (stat_bin).\n\n\n\n\n\nOK. We’ve seen a few variables, and we’ll move on now to look more seriously at the data."
  },
  {
    "objectID": "09-nnyfs_foundations.html#additional-numeric-summaries",
    "href": "09-nnyfs_foundations.html#additional-numeric-summaries",
    "title": "9  National Youth Fitness Survey",
    "section": "9.5 Additional Numeric Summaries",
    "text": "9.5 Additional Numeric Summaries\n\n9.5.1 The Five Number Summary, Quantiles and IQR\nThe five number summary is most famous when used to form a box plot - it’s the minimum, 25th percentile, median, 75th percentile and maximum. For numerical and integer variables, the summary function produces the five number summary, plus the mean, and a count of any missing values (NA’s).\n\nnnyfs |> \n  select(waist, energy, sugar) |>\n  summary()\n\n     waist            energy         sugar       \n Min.   : 42.50   Min.   : 257   Min.   :  1.00  \n 1st Qu.: 55.60   1st Qu.:1368   1st Qu.: 82.66  \n Median : 64.80   Median :1794   Median :116.92  \n Mean   : 67.71   Mean   :1877   Mean   :124.32  \n 3rd Qu.: 76.60   3rd Qu.:2306   3rd Qu.:157.05  \n Max.   :144.70   Max.   :5265   Max.   :405.49  \n NA's   :6                                       \n\n\nAs an alternative, we can use the $ notation to indicate the variable we wish to study inside a data set, and we can use the fivenum function to get the five numbers used in developing a box plot. We’ll focus for a little while on the number of kilocalories consumed by each child, according to the dietary recall questionnaire. That’s the energy variable.\n\nfivenum(nnyfs$energy)\n\n[1]  257.0 1367.0 1794.5 2306.0 5265.0\n\n\n\nAs mentioned in @ref(rangeandiqr), the inter-quartile range, or IQR, is sometimes used as a competitor for the standard deviation. It’s the difference between the 75th percentile and the 25th percentile. The 25th percentile, median, and 75th percentile are referred to as the quartiles of the data set, because, together, they split the data into quarters.\n\n\nIQR(nnyfs$energy)\n\n[1] 938.5\n\n\nWe can obtain quantiles (percentiles) as we like - here, I’m asking for the 1st and 99th:\n\nquantile(nnyfs$energy, probs=c(0.01, 0.99))\n\n     1%     99% \n 566.85 4051.75"
  },
  {
    "objectID": "09-nnyfs_foundations.html#additional-summaries-from-favstats",
    "href": "09-nnyfs_foundations.html#additional-summaries-from-favstats",
    "title": "9  National Youth Fitness Survey",
    "section": "9.6 Additional Summaries from favstats",
    "text": "9.6 Additional Summaries from favstats\nIf we’re focusing on a single variable, the favstats function in the mosaic package can be very helpful. Rather than calling up the entire mosaic library here, I’ll just specify the function within the library.\n\nmosaic::favstats(~ energy, data = nnyfs)\n\n min     Q1 median   Q3  max     mean       sd    n missing\n 257 1367.5 1794.5 2306 5265 1877.157 722.3537 1518       0\n\n\nThis adds three useful results to the base summary - the standard deviation, the sample size and the number of missing observations."
  },
  {
    "objectID": "09-nnyfs_foundations.html#the-histogram",
    "href": "09-nnyfs_foundations.html#the-histogram",
    "title": "9  National Youth Fitness Survey",
    "section": "9.7 The Histogram",
    "text": "9.7 The Histogram\nObtaining a basic histogram of, for example, the energy (kilocalories consumed) in the nnyfs data is pretty straightforward.\n\nggplot(data = nnyfs, aes(x = energy)) +\n    geom_histogram(binwidth = 100, col = \"white\")\n\n\n\n\n\n9.7.1 Freedman-Diaconis Rule to select bin width\nIf we like, we can suggest a particular number of cells for the histogram, instead of accepting the defaults. In this case, we have \\(n\\) = 1518 observations. The Freedman-Diaconis rule can be helpful here. That rule suggests that we set the bin-width to\n\\[\nh = \\frac{2*IQR}{n^{1/3}}\n\\]\nso that the number of bins is equal to the range of the data set (maximum - minimum) divided by \\(h\\).\nFor the energy data in the nnyfs tibble, we have\n\nIQR of 938.5, \\(n\\) = 1518 and range = 5008\nThus, by the Freedman-Diaconis rule, the optimal binwidth \\(h\\) is 163.3203676, or, realistically, 163.\nAnd so the number of bins would be 30.6636586, or, realistically 31.\n\nHere, we’ll draw the graph again, using the Freedman-Diaconis rule to identify the number of bins, and also play around a bit with the fill and color of the bars.\n\nbw <- 2 * IQR(nnyfs$energy) / length(nnyfs$energy)^(1/3)\nggplot(data = nnyfs, aes(x = energy)) +\n    geom_histogram(binwidth=bw, color = \"white\", fill = \"black\")\n\n\n\n\nThis is a nice start, but it is by no means a finished graph.\nLet’s improve the axis labels, add a title, and fill in the bars with a distinctive blue and use a black outline around each bar. I’ll just use 25 bars, because I like how that looks in this case, and optimizing the number of bins is rarely important.\n\nggplot(data = nnyfs, aes(x = energy)) +\n    geom_histogram(bins=25, color = \"black\", fill = \"dodgerblue\") + \n    labs(title = \"Histogram of Body-Mass Index Results in the nnyfs data\",\n         x = \"Energy Consumed (kcal)\", y = \"# of Subjects\")\n\n\n\n\n\n\n9.7.2 A Note on Colors\nThe simplest way to specify a color is with its name, enclosed in parentheses. My favorite list of R colors is http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf. In a pinch, you can usually find it by googling Colors in R. You can also type colors() in the R console to obtain a list of the names of the same 657 colors.\nWhen using colors to make comparisons, you may be interested in using a scale that has some nice properties. The viridis package vignette describes four color scales (viridis, magma, plasma and inferno) that are designed to be colorful, robust to colorblindness and gray scale printing, and perceptually uniform, which means (as the package authors describe it) that values close to each other have similar-appearing colors and values far away from each other have more different-appearing colors, consistently across the range of values. We can apply these colors with special functions within ggplot.\nHere’s a comparison of several histograms, looking at energy consumed as a function of whether yesterday was typical in terms of food consumption.\n\nggplot(data = nnyfs, aes(x = energy, fill = diet_yesterday)) +\n  geom_histogram(bins = 20, col = \"white\") +\n  scale_fill_viridis_d() +\n  facet_wrap(~ diet_yesterday)\n\n\n\n\nWe don’t really need the legend here, and perhaps we should restrict the plot to participants who responded to the diet_yesterday question, and put in a title and better axis labels?\n\nnnyfs |> filter(!is.na(energy), !is.na(diet_yesterday)) %>%\n  ggplot(data = ., aes(x = energy, fill = diet_yesterday)) +\n  geom_histogram(bins = 20, col = \"white\") +\n  scale_fill_viridis_d() +\n  guides(fill = \"none\") +\n  facet_wrap(~ diet_yesterday) +\n  labs(x = \"Energy consumed, in kcal\",\n       title = \"Energy Consumption and How Typical Was Yesterday's Eating\",\n       subtitle = \"NHANES National Youth Fitness Survey, no survey weighting\")\n\n\n\n\nNote the use of the %>% pipe here. I need to write more about this."
  },
  {
    "objectID": "09-nnyfs_foundations.html#the-frequency-polygon",
    "href": "09-nnyfs_foundations.html#the-frequency-polygon",
    "title": "9  National Youth Fitness Survey",
    "section": "9.8 The Frequency Polygon",
    "text": "9.8 The Frequency Polygon\nAs we’ve seen, we can also plot the distribution of a single continuous variable using the freqpoly geom. We can also add a rug plot, which places a small vertical line on the horizontal axis everywhere where an observation appears in the data.\n\nggplot(data = nnyfs, aes(x = energy)) +\n    geom_freqpoly(binwidth = 150, color = \"dodgerblue\") + \n    geom_rug(color = \"red\") +\n    labs(title = \"Frequency Polygon of nnyfs Energy data\",\n         x = \"Energy (kcal)\", y = \"# of Patients\")"
  },
  {
    "objectID": "09-nnyfs_foundations.html#plotting-the-probability-density-function",
    "href": "09-nnyfs_foundations.html#plotting-the-probability-density-function",
    "title": "9  National Youth Fitness Survey",
    "section": "9.9 Plotting the Probability Density Function",
    "text": "9.9 Plotting the Probability Density Function\nWe can also produce a density function, which has the effect of smoothing out the bumps in a histogram or frequency polygon, while also changing what is plotted on the y-axis.\n\nggplot(data = nnyfs, aes(x = energy)) +\n    geom_density(kernel = \"gaussian\", color = \"dodgerblue\") + \n    labs(title = \"Density of nnyfs Energy data\",\n         x = \"Energy (kcal)\", y = \"Probability Density function\")\n\n\n\n\nSo, what’s a density function?\n\nA probability density function is a function of a continuous variable, x, that represents the probability of x falling within a given range. Specifically, the integral over the interval (a,b) of the density function gives the probability that the value of x is within (a,b).\nIf you’re interested in exploring more on the notion of density functions for continuous (and discrete) random variables, some nice elementary material is available at Khan Academy."
  },
  {
    "objectID": "09-nnyfs_foundations.html#the-boxplot",
    "href": "09-nnyfs_foundations.html#the-boxplot",
    "title": "9  National Youth Fitness Survey",
    "section": "9.10 The Boxplot",
    "text": "9.10 The Boxplot\nSometimes, it’s helpful to picture the five-number summary of the data in such a way as to get a general sense of the distribution. One approach is a boxplot, sometimes called a box-and-whisker plot.\n\n9.10.1 Drawing a Boxplot for One Variable in ggplot2\nThe ggplot2 library easily handles comparison boxplots for multiple distributions, as we’ll see in a moment. However, building a boxplot for a single distribution requires a little trickiness.\n\nggplot(nnyfs, aes(x = 1, y = energy)) + \n    geom_boxplot(fill = \"deepskyblue\") + \n    coord_flip() + \n    labs(title = \"Boxplot of Energy for kids in the NNYFS\",\n         y = \"Energy (kcal)\",\n         x = \"\") +\n    theme(axis.text.y = element_blank(),\n          axis.ticks.y = element_blank())\n\n\n\n\n\n\n9.10.2 About the Boxplot\nThe boxplot is another John Tukey invention.\n\nR draws the box (here in yellow) so that its edges of the box fall at the 25th and 75th percentiles of the data, and the thick line inside the box falls at the median (50th percentile).\nThe whiskers then extend out to the largest and smallest values that are not classified by the plot as candidate outliers.\nAn outlier is an unusual point, far from the center of a distribution.\nNote that I’ve used the horizontal option to show this boxplot in this direction. Most comparison boxplots, as we’ll see below, are oriented vertically.\n\nThe boxplot’s whiskers that are drawn from the first and third quartiles (i.e. the 25th and 75th percentiles) out to the most extreme points in the data that do not meet the standard of ``candidate outliers.’’ An outlier is simply a point that is far away from the center of the data - which may be due to any number of reasons, and generally indicates a need for further investigation.\nMost software, including R, uses a standard proposed by Tukey which describes a ``candidate outlier’’ as any point above the upper fence or below the lower fence. The definitions of the fences are based on the inter-quartile range (IQR).\nIf IQR = 75th percentile - 25th percentile, then the upper fence is 75th percentile + 1.5 IQR, and the lower fence is 25th percentile - 1.5 IQR.\nSo for these energy data,\n\nthe upper fence is located at 2306 + 1.5(938.5) = 3713.75\nthe lower fence is located at 1367 - 1.5(938.5) = -40.75\n\nIn this case, we see no points identified as outliers in the low part of the distribution, but quite a few identified that way on the high side. This tends to identify about 5% of the data as a candidate outlier, if the data follow a Normal distribution.\n\nThis plot is indicating clearly that there is some asymmetry (skew) in the data, specifically right skew.\nThe standard R uses is to indicate as outliers any points that are more than 1.5 inter-quartile ranges away from the edges of the box.\n\nThe horizontal orientation I’ve chosen here clarifies the relationship of direction of skew to the plot. A plot like this, with multiple outliers on the right side is indicative of a long right tail in the distribution, and hence, positive or right skew - with the mean being larger than the median. Other indications of skew include having one side of the box being substantially wider than the other, or one side of the whiskers being substantially longer than the other. More on skew later."
  },
  {
    "objectID": "09-nnyfs_foundations.html#a-simple-comparison-boxplot",
    "href": "09-nnyfs_foundations.html#a-simple-comparison-boxplot",
    "title": "9  National Youth Fitness Survey",
    "section": "9.11 A Simple Comparison Boxplot",
    "text": "9.11 A Simple Comparison Boxplot\nBoxplots are most often used for comparison. We can build boxplots using ggplot2, as well, and we’ll discuss that in detail later. For now, here’s a boxplot built to compare the energy results by the subject’s race/ethnicity.\n\nggplot(nnyfs, aes(x = factor(race_eth), y = energy, fill=factor(race_eth))) +\n  geom_boxplot() + \n  guides(fill = \"none\") +\n  labs(y = \"Energy consumed (kcal)\", x = \"Race/Ethnicity\")\n\n\n\n\nLet’s look at the comparison of observed energy levels across the five categories in our phys_health variable, now making use of the viridis color scheme.\n\nggplot(nnyfs, aes(x = factor(phys_health), y = energy, fill = factor(phys_health))) +\n  geom_boxplot() + \n  scale_fill_viridis_d() + \n  labs(title = \"Energy by Self-Reported Physical Health, in nnyfs data\")\n\n\n\n\nAs a graph, that’s not bad, but what if we want to improve it further?\nLet’s turn the boxes in the horizontal direction, and get rid of the perhaps unnecessary phys_health labels.\n\nggplot(nnyfs, aes(x = factor(phys_health), y = energy, fill = factor(phys_health))) +\n    geom_boxplot() + \n    scale_fill_viridis_d() + \n    coord_flip() + \n    guides(fill = \"none\") +\n    labs(title = \"Energy Consumed by Self-Reported Physical Health\", \n         subtitle = \"NHANES National Youth Fitness Survey, unweighted\", \n         x = \"\")"
  },
  {
    "objectID": "09-nnyfs_foundations.html#using-describe-in-the-psych-library",
    "href": "09-nnyfs_foundations.html#using-describe-in-the-psych-library",
    "title": "9  National Youth Fitness Survey",
    "section": "9.12 Using describe in the psych library",
    "text": "9.12 Using describe in the psych library\nFor additional numerical summaries, one option would be to consider using the describe function from the psych library.\n\npsych::describe(nnyfs$energy)\n\n   vars    n    mean     sd median trimmed    mad min  max range skew kurtosis\nX1    1 1518 1877.16 722.35 1794.5  1827.1 678.29 257 5265  5008  0.8     1.13\n      se\nX1 18.54\n\n\nThis package provides, in order, the following…\n\nn = the sample size\nmean = the sample mean\nsd = the sample standard deviation\nmedian = the median, or 50th percentile\ntrimmed = mean of the middle 80% of the data\nmad = median absolute deviation\nmin = minimum value in the sample\nmax = maximum value in the sample\nrange = max - min\nskew = skewness measure, described below (indicates degree of asymmetry)\nkurtosis = kurtosis measure, described below (indicates heaviness of tails, degree of outlier-proneness)\nse = standard error of the sample mean = sd / square root of sample size, useful in inference\n\n\n9.12.1 The Trimmed Mean\nThe trimmed mean trim value in R indicates proportion of observations to be trimmed from each end of the outcome distribution before the mean is calculated. The trimmed value provided by the psych::describe package describes what this particular package calls a 20% trimmed mean (bottom and top 10% of energy values are removed before taking the mean - it’s the mean of the middle 80% of the data.) I might call that a 10% trimmed mean in some settings, but that’s just me.\n\nmean(nnyfs$energy, trim=.1) \n\n[1] 1827.1\n\n\n\n\n9.12.2 The Median Absolute Deviation\nAn alternative to the IQR that is fancier, and a bit more robust, is the median absolute deviation, which, in large sample sizes, for data that follow a Normal distribution, will be (in expectation) equal to the standard deviation. The MAD is the median of the absolute deviations from the median, multiplied by a constant (1.4826) to yield asymptotically normal consistency.\n\nmad(nnyfs$energy)\n\n[1] 678.2895"
  },
  {
    "objectID": "09-nnyfs_foundations.html#assessing-skew",
    "href": "09-nnyfs_foundations.html#assessing-skew",
    "title": "9  National Youth Fitness Survey",
    "section": "9.13 Assessing Skew",
    "text": "9.13 Assessing Skew\nA relatively common idea is to assess skewness, several measures of which are available. Many models assume a Normal distribution, where, among other things, the data are symmetric around the mean.\nSkewness measures asymmetry in the distribution, where left skew (mean < median) is indicated by negative skewness values, while right skew (mean > median) is indicated by positive values. The skew value will be near zero for data that follow a symmetric distribution.\n\n9.13.1 Non-parametric Skewness\nA simpler measure of skew, sometimes called the nonparametric skew and closely related to Pearson’s notion of median skewness, falls between -1 and +1 for any distribution. It is just the difference between the mean and the median, divided by the standard deviation.\n\nValues greater than +0.2 are sometimes taken to indicate fairly substantial right skew, while values below -0.2 indicate fairly substantial left skew.\n\n\n(mean(nnyfs$energy) - median(nnyfs$energy))/sd(nnyfs$energy)\n\n[1] 0.114427\n\n\nThe Wikipedia page on skewness, from which some of this material is derived, provides definitions for several other skewness measures."
  },
  {
    "objectID": "09-nnyfs_foundations.html#assessing-kurtosis-heavy-tailedness",
    "href": "09-nnyfs_foundations.html#assessing-kurtosis-heavy-tailedness",
    "title": "9  National Youth Fitness Survey",
    "section": "9.14 Assessing Kurtosis (Heavy-Tailedness)",
    "text": "9.14 Assessing Kurtosis (Heavy-Tailedness)\nAnother measure of a distribution’s shape that can be found in the psych library is the kurtosis. Kurtosis is an indicator of whether the distribution is heavy-tailed or light-tailed as compared to a Normal distribution. Positive kurtosis means more of the variance is due to outliers - unusual points far away from the mean relative to what we might expect from a Normally distributed data set with the same standard deviation.\n\nA Normal distribution will have a kurtosis value near 0, a distribution with similar tail behavior to what we would expect from a Normal is said to be mesokurtic\nHigher kurtosis values (meaningfully higher than 0) indicate that, as compared to a Normal distribution, the observed variance is more the result of extreme outliers (i.e. heavy tails) as opposed to being the result of more modest sized deviations from the mean. These heavy-tailed, or outlier prone, distributions are sometimes called leptokurtic.\nKurtosis values meaningfully lower than 0 indicate light-tailed data, with fewer outliers than we’d expect in a Normal distribution. Such distributions are sometimes referred to as platykurtic, and include distributions without outliers, like the Uniform distribution.\n\nHere’s a table:\n\n\n\n\n\n\n\n\nFewer outliers than a Normal\nApproximately Normal\nMore outliers than a Normal\n\n\n\n\nLight-tailed\n“Normalish”\nHeavy-tailed\n\n\nplatykurtic (kurtosis < 0)\nmesokurtic (kurtosis = 0)\nleptokurtic (kurtosis > 0)\n\n\n\n\npsych::kurtosi(nnyfs$energy)\n\n[1] 1.130539\n\n\nNote that the kurtosi() function is strangely named, and is part of the psych package.\n\n9.14.1 The Standard Error of the Sample Mean\nThe standard error of the sample mean, which is the standard deviation divided by the square root of the sample size:\n\nsd(nnyfs$energy)/sqrt(length(nnyfs$energy))\n\n[1] 18.54018"
  },
  {
    "objectID": "09-nnyfs_foundations.html#the-describe-function-in-the-hmisc-package",
    "href": "09-nnyfs_foundations.html#the-describe-function-in-the-hmisc-package",
    "title": "9  National Youth Fitness Survey",
    "section": "9.15 The describe function in the Hmisc package",
    "text": "9.15 The describe function in the Hmisc package\nThe Hmisc package has lots of useful functions. It’s named for its main developer, Frank Harrell. The describe function in Hmisc knows enough to separate numerical from categorical variables, and give you separate (and detailed) summaries for each.\n\nFor a categorical variable, it provides counts of total observations (n), the number of missing values, and the number of unique categories, along with counts and percentages falling in each category.\nFor a numerical variable, it provides:\ncounts of total observations (n), the number of missing values, and the number of unique values\nan Info value for the data, which indicates how continuous the variable is (a score of 1 is generally indicative of a completely continuous variable with no ties, while scores near 0 indicate lots of ties, and very few unique values)\nthe sample Mean\nGini’s mean difference, which is a robust measure of spread, with larger values indicating greater dispersion in the data. It is defined as the mean absolute difference between any pairs of observations.\nmany sample percentiles (quantiles) of the data, specifically (5, 10, 25, 50, 75, 90, 95, 99)\neither a complete table of all observed values, with counts and percentages (if there are a modest number of unique values), or\na table of the five smallest and five largest values in the data set, which is useful for range checking\n\n\nnnyfs |> \n  select(waist, energy, bmi) |>\n  Hmisc::describe()\n\nselect(nnyfs, waist, energy, bmi) \n\n 3  Variables      1518  Observations\n--------------------------------------------------------------------------------\nwaist \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    1512        6      510        1    67.71     16.6    49.40    51.40 \n     .25      .50      .75      .90      .95 \n   55.60    64.80    76.60    88.70    96.84 \n\nlowest :  42.5  43.4  44.1  44.4  44.5, highest: 125.8 126.0 127.0 132.3 144.7\n--------------------------------------------------------------------------------\nenergy \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    1518        0     1137        1     1877    796.1      849     1047 \n     .25      .50      .75      .90      .95 \n    1368     1794     2306     2795     3195 \n\nlowest :  257  260  326  349  392, highest: 4382 4529 5085 5215 5265\n--------------------------------------------------------------------------------\nbmi \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    1514        4      225        1    19.63    5.269    14.30    14.90 \n     .25      .50      .75      .90      .95 \n   15.90    18.10    21.90    26.27    30.20 \n\nlowest : 11.9 12.6 12.7 12.9 13.0, highest: 42.8 43.0 46.9 48.2 48.3\n--------------------------------------------------------------------------------\n\n\nMore on the Info value in Hmisc::describe is available here"
  },
  {
    "objectID": "09-nnyfs_foundations.html#summarizing-data-within-subgroups",
    "href": "09-nnyfs_foundations.html#summarizing-data-within-subgroups",
    "title": "9  National Youth Fitness Survey",
    "section": "9.16 Summarizing data within subgroups",
    "text": "9.16 Summarizing data within subgroups\nSuppose we want to understand how the subjects whose diet involved consuming much more than usual yesterday compare to those who consumer their usual amount, or to those who consumed much less than usual, in terms of the energy they consumed, as well as the protein. We might start by looking at the medians and means.\n\nnnyfs |>\n    group_by(diet_yesterday) |>\n    select(diet_yesterday, energy, protein) |>\n    summarise_all(list(median = median, mean = mean))\n\n# A tibble: 4 × 5\n  diet_yesterday         energy_median protein_median energy_mean protein_mean\n  <fct>                          <dbl>          <dbl>       <dbl>        <dbl>\n1 1_Much more than usual          2098           69.4       2150.         75.1\n2 2_Usual                         1794           61.3       1858.         67.0\n3 3_Much less than usual          1643           53.9       1779.         60.1\n4 <NA>                            4348          155.        4348         155. \n\n\nPerhaps we should restrict ourselves to the people who were not missing the diet_yesterday category, and look now at their sugar and water consumption.\n\nnnyfs |>\n    filter(complete.cases(diet_yesterday)) |>\n    group_by(diet_yesterday) |>\n    select(diet_yesterday, energy, protein, sugar, water) |>\n    summarise_all(list(median))\n\n# A tibble: 3 × 5\n  diet_yesterday         energy protein sugar water\n  <fct>                   <dbl>   <dbl> <dbl> <dbl>\n1 1_Much more than usual   2098    69.4  137.  500 \n2 2_Usual                  1794    61.3  114.  385.\n3 3_Much less than usual   1643    53.9  115.  311.\n\n\nIt looks like the children in the “Much more than usual” category consumed more energy, protein, sugar and water than the children in the other two categories. Let’s draw a picture of this.\n\ntemp_dat <- nnyfs |>\n    filter(complete.cases(diet_yesterday)) |>\n    mutate(diet_yesterday = fct_recode(diet_yesterday,\n        \"Much more\" = \"1_Much more than usual\",\n        \"Usual diet\" = \"2_Usual\",\n        \"Much less\" = \"3_Much less than usual\"))\n\np1 <- ggplot(temp_dat, aes(x = diet_yesterday, y = energy)) +\n    geom_violin() +\n    geom_boxplot(aes(fill = diet_yesterday), width = 0.2) +\n    theme_light() + \n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Energy Comparison\")\n\np2 <- ggplot(temp_dat, aes(x = diet_yesterday, y = protein)) +\n    geom_violin() +\n    geom_boxplot(aes(fill = diet_yesterday), width = 0.2) +\n    theme_light() + \n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Protein Comparison\")\n\np3 <- ggplot(temp_dat, aes(x = diet_yesterday, y = sugar)) +\n    geom_violin() +\n    geom_boxplot(aes(fill = diet_yesterday), width = 0.2) +\n    theme_light() + \n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Sugar Comparison\")\n\np4 <- ggplot(temp_dat, aes(x = diet_yesterday, y = water)) +\n    geom_violin() +\n    geom_boxplot(aes(fill = diet_yesterday), width = 0.2) +\n    theme_light() + \n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Water Comparison\")\n\np1 + p2 + p3 + p4\n\n\n\n\nWe can see that there is considerable overlap in these distributions, regardless of what we’re measuring."
  },
  {
    "objectID": "09-nnyfs_foundations.html#another-example",
    "href": "09-nnyfs_foundations.html#another-example",
    "title": "9  National Youth Fitness Survey",
    "section": "9.17 Another Example",
    "text": "9.17 Another Example\nSuppose now that we ask a different question. Do kids in larger categories of BMI have larger waist circumferences?\n\nnnyfs |>\n    group_by(bmi_cat) |>\n    summarise(mean = mean(waist), sd = sd(waist), \n              median = median(waist), \n              skew_1 = round((mean(waist) - median(waist)) / \n                                 sd(waist),2))\n\n# A tibble: 5 × 5\n  bmi_cat        mean    sd median skew_1\n  <fct>         <dbl> <dbl>  <dbl>  <dbl>\n1 1_Underweight  55.2  7.58   54.5   0.09\n2 2_Normal       NA   NA      NA    NA   \n3 3_Overweight   72.3 11.9    74    -0.14\n4 4_Obese        NA   NA      NA    NA   \n5 <NA>           NA   NA      NA    NA   \n\n\nOops. Looks like we need to filter for cases with complete data on both BMI category and waist circumference in order to get meaningful results. We should add a count, too.\n\nnnyfs |>\n    filter(complete.cases(bmi_cat, waist)) |>\n    group_by(bmi_cat) |>\n    summarise(count = n(), mean = mean(waist), \n              sd = sd(waist), median = median(waist), \n       skew_1 = \n         round((mean(waist) - median(waist)) / sd(waist),2))\n\n# A tibble: 4 × 6\n  bmi_cat       count  mean    sd median skew_1\n  <fct>         <int> <dbl> <dbl>  <dbl>  <dbl>\n1 1_Underweight    41  55.2  7.58   54.5   0.09\n2 2_Normal        917  61.2  9.35   59.5   0.19\n3 3_Overweight    258  72.3 11.9    74    -0.14\n4 4_Obese         294  85.6 17.1    86.8  -0.07\n\n\nOr, we could use something like favstats from the mosaic package, which automatically accounts for missing data, and omits it when calculating summary statistics within each group.\n\nmosaic::favstats(waist ~ bmi_cat, data = nnyfs) |>\n    kable(digits = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbmi_cat\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\n1_Underweight\n42.5\n49.3\n54.5\n62.4\n68.5\n55.2\n7.6\n41\n0\n\n\n2_Normal\n44.1\n53.9\n59.5\n68.4\n89.2\n61.2\n9.4\n917\n3\n\n\n3_Overweight\n49.3\n62.3\n74.0\n81.2\n105.3\n72.3\n11.9\n258\n0\n\n\n4_Obese\n52.1\n72.7\n86.8\n96.8\n144.7\n85.6\n17.1\n294\n1\n\n\n\n\n\nWhile patients in the heavier groups generally had higher waist circumferences, the standard deviations suggest there may be some meaningful overlap. Let’s draw the picture, in this case a comparison boxplot accompanying a violin plot.\n\nnnyfs |>\n    filter(complete.cases(bmi_cat, waist)) %>%\n    ggplot(., aes(x = bmi_cat, y = waist)) +\n    geom_violin() +\n    geom_boxplot(aes(fill = bmi_cat), width = 0.2) +\n    theme_light() + \n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Waist Circumference by BMI Category\")\n\n\n\n\nNote the use of the %>% pipe here. I need to write more about this.\nThe data transformation with dplyr cheat sheet found under the Help menu in RStudio is a great resource. And, of course, for more details, visit Wickham and Grolemund (2022)."
  },
  {
    "objectID": "09-nnyfs_foundations.html#boxplots-to-relate-an-outcome-to-a-categorical-predictor",
    "href": "09-nnyfs_foundations.html#boxplots-to-relate-an-outcome-to-a-categorical-predictor",
    "title": "9  National Youth Fitness Survey",
    "section": "9.18 Boxplots to Relate an Outcome to a Categorical Predictor",
    "text": "9.18 Boxplots to Relate an Outcome to a Categorical Predictor\nBoxplots are much more useful when comparing samples of data. For instance, consider this comparison boxplot describing the triceps skinfold results across the four levels of BMI category.\n\nggplot(nnyfs, aes(x = bmi_cat, y = triceps_skinfold,\n                  fill = bmi_cat)) +\n    geom_boxplot() +\n    scale_fill_viridis_d() +\n    theme_light()\n\nWarning: Removed 21 rows containing non-finite values (stat_boxplot).\n\n\n\n\n\nAgain, we probably want to omit those missing values (both in bmi_cat and triceps_skinfold) and also eliminate the repetitive legend (guides) on the right.\n\nnnyfs |> \n    filter(complete.cases(bmi_cat, triceps_skinfold)) %>%\n    ggplot(., aes(x = bmi_cat, y = triceps_skinfold,\n                  fill = bmi_cat)) +\n    geom_boxplot() +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    theme_light() +\n    labs(x = \"BMI Category\", y = \"Triceps Skinfold in mm\",\n         title = \"Triceps Skinfold increases with BMI category\",\n         subtitle = \"NNYFS children\")\n\n\n\n\nAs always, the boxplot shows the five-number summary (minimum, 25th percentile, median, 75th percentile and maximum) in addition to highlighting candidate outliers.\n\n9.18.1 Augmenting the Boxplot with the Sample Mean\nOften, we want to augment such a plot, perhaps by adding a little diamond to show the sample mean within each category, so as to highlight skew (in terms of whether the mean is meaningfully different from the median.)\n\nnnyfs |> \n    filter(complete.cases(bmi_cat, triceps_skinfold)) %>%\n    ggplot(., aes(x = bmi_cat, y = triceps_skinfold,\n                  fill = bmi_cat)) +\n    geom_boxplot() +\n    stat_summary(fun=\"mean\", geom=\"point\", \n                 shape=23, size=3, fill=\"white\") +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    theme_light() +\n    labs(x = \"BMI Category\", y = \"Triceps Skinfold in mm\",\n         title = \"Triceps Skinfold increases with BMI category\",\n         subtitle = \"NNYFS children\")"
  },
  {
    "objectID": "09-nnyfs_foundations.html#building-a-violin-plot",
    "href": "09-nnyfs_foundations.html#building-a-violin-plot",
    "title": "9  National Youth Fitness Survey",
    "section": "9.19 Building a Violin Plot",
    "text": "9.19 Building a Violin Plot\nThere are a number of other plots which compare distributions of data sets. An interesting one is called a violin plot. A violin plot is a kernel density estimate, mirrored to form a symmetrical shape.\n\nnnyfs |>\n    filter(complete.cases(triceps_skinfold, bmi_cat)) %>%\n    ggplot(., aes(x=bmi_cat, y=triceps_skinfold, \n                  fill = bmi_cat)) + \n    geom_violin(trim=FALSE) +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Triceps Skinfold by BMI Category\")\n\n\n\n\nTraditionally, these plots are shown with overlaid boxplots and a white dot at the median, like this example, now looking at waist circumference again.\n\nnnyfs |>\n    filter(complete.cases(waist, bmi_cat)) %>%\n    ggplot(., aes(x = bmi_cat, y = waist, \n                  fill = bmi_cat)) + \n    geom_violin(trim=FALSE) +\n    geom_boxplot(width=.1, outlier.colour=NA, \n                 color = c(rep(\"white\",2), rep(\"black\",2))) +\n    stat_summary(fun=median, geom=\"point\", \n                 fill=\"white\", shape=21, size=3) + \n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Waist Circumference by BMI Category\")\n\n\n\n\n\n9.19.1 Adding Notches to a Boxplot\nNotches are used in boxplots to help visually assess whether the medians of the distributions across the various groups actually differ to a statistically detectable extent. Think of them as confidence regions around the medians. If the notches do not overlap, as in this situation, this provides some evidence that the medians in the populations represented by these samples may be different.\n\nnnyfs |> \n    filter(complete.cases(bmi_cat, triceps_skinfold)) %>%\n    ggplot(., aes(x = bmi_cat, y = triceps_skinfold)) +\n    geom_violin() +\n    geom_boxplot(aes(fill = bmi_cat), width = 0.3, notch = TRUE) +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    theme_light() +\n    labs(x = \"BMI Category\", y = \"Triceps Skinfold in mm\",\n         title = \"Triceps Skinfold increases with BMI category\",\n         subtitle = \"NNYFS children\")\n\n\n\n\nThere is no overlap between the notches for each of the four categories, so we might reasonably conclude that the true median triceps skinfold values across the four categories are statistically significantly different.\nFor an example where the notches do overlap, consider the comparison of plank times by BMI category.\n\nnnyfs |> \n    filter(complete.cases(bmi_cat, plank_time)) %>%\n    ggplot(., aes(x=bmi_cat, y=plank_time)) +\n    geom_violin(aes(fill = bmi_cat)) +\n    geom_boxplot(width = 0.3, notch=TRUE) +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") + \n    theme_light() +\n    labs(title = \"Plank Times by BMI category\", \n         x = \"\", y = \"Plank Time (in seconds)\")\n\n\n\n\nThe overlap in the notches (for instance between Underweight and Normal) suggests that the median plank times in the population of interest don’t necessarily differ in a meaningful way by BMI category, other than perhaps the Obese group which may have a shorter time.\nThese data are somewhat right skewed. Would a logarithmic transformation in the plot help us see the patterns more clearly?\n\nnnyfs |> \n    filter(complete.cases(bmi_cat, plank_time)) %>%\n    ggplot(., aes(x=bmi_cat, y = log(plank_time))) +\n    geom_violin() +\n    geom_boxplot(aes(fill = bmi_cat), width = 0.3, notch=TRUE) +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") + \n    theme_light() +\n    labs(title = \"log(Plank Times) by BMI category\", \n         x = \"\", y = \"Natural Log of Plank Time\")"
  },
  {
    "objectID": "09-nnyfs_foundations.html#using-multiple-histograms-to-make-comparisons",
    "href": "09-nnyfs_foundations.html#using-multiple-histograms-to-make-comparisons",
    "title": "9  National Youth Fitness Survey",
    "section": "9.20 Using Multiple Histograms to Make Comparisons",
    "text": "9.20 Using Multiple Histograms to Make Comparisons\nWe can make an array of histograms to describe multiple groups of data, using ggplot2 and the notion of faceting our plot.\n\nnnyfs |> \n    filter(complete.cases(triceps_skinfold, bmi_cat)) %>%\n    ggplot(., aes(x=triceps_skinfold, fill = bmi_cat)) +\n    geom_histogram(binwidth = 2, color = \"black\") + \n    facet_wrap(~ bmi_cat) +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Triceps Skinfold by BMI Category\")"
  },
  {
    "objectID": "09-nnyfs_foundations.html#using-multiple-density-plots-to-make-comparisons",
    "href": "09-nnyfs_foundations.html#using-multiple-density-plots-to-make-comparisons",
    "title": "9  National Youth Fitness Survey",
    "section": "9.21 Using Multiple Density Plots to Make Comparisons",
    "text": "9.21 Using Multiple Density Plots to Make Comparisons\nOr, we can make a series of density plots to describe multiple groups of data.\n\nnnyfs |> \n    filter(complete.cases(triceps_skinfold, bmi_cat)) %>%\n    ggplot(., aes(x=triceps_skinfold, fill = bmi_cat)) +\n    geom_density(color = \"black\") + \n    facet_wrap(~ bmi_cat) +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Triceps Skinfold by BMI Category\")\n\n\n\n\nOr, we can plot all of the densities on top of each other with semi-transparent fills.\n\nnnyfs |> \n    filter(complete.cases(triceps_skinfold, bmi_cat)) %>%\n    ggplot(., aes(x=triceps_skinfold, fill = bmi_cat)) +\n    geom_density(alpha=0.3) + \n    scale_fill_viridis_d() + \n    labs(title = \"Triceps Skinfold by BMI Category\")\n\n\n\n\nThis really works better when we are comparing only two groups, like females to males.\n\nnnyfs |> \n    filter(complete.cases(triceps_skinfold, sex)) %>%\n    ggplot(., aes(x=triceps_skinfold, fill = sex)) +\n    geom_density(alpha=0.5) + \n    labs(title = \"Triceps Skinfold by Sex\")"
  },
  {
    "objectID": "09-nnyfs_foundations.html#a-ridgeline-plot",
    "href": "09-nnyfs_foundations.html#a-ridgeline-plot",
    "title": "9  National Youth Fitness Survey",
    "section": "9.22 A Ridgeline Plot",
    "text": "9.22 A Ridgeline Plot\nSome people don’t like violin plots - for example, see https://simplystatistics.org/2017/07/13/the-joy-of-no-more-violin-plots/. An alternative plot is available as part of the ggridges package. This shows the distribution of several groups simultaneously, especially when you have lots of subgroup categories, and is called a ridgeline plot.\n\nnnyfs |> \n    filter(complete.cases(waist, bmi_cat)) %>%\n    ggplot(., aes(x = waist, y = bmi_cat, height = ..density..)) +\n    ggridges::geom_density_ridges(scale = 0.85) + \n    theme_light() +\n    labs(title = \"Ridgeline Plot of Waist Circumference by BMI category (nnyfs)\",\n         x = \"Waist Circumference\", y = \"BMI Category\")\n\nPicking joint bandwidth of 3.47\n\n\n\n\n\nAnd here’s a ridgeline plot for the triceps skinfolds. We’ll start by sorting the subgroups by the median value of our outcome (triceps skinfold) in this case, though it turns out not to matter. We’ll also add some color.\n\nnnyfs |>\n    filter(complete.cases(bmi_cat, triceps_skinfold)) |>\n    mutate(bmi_cat = fct_reorder(bmi_cat,\n                                 triceps_skinfold, \n                                 .fun = median)) %>%\n    ggplot(., aes(x = triceps_skinfold, y = bmi_cat, \n                  fill = bmi_cat, height = ..density..)) +\n    ggridges::geom_density_ridges(scale = 0.85) + \n    scale_fill_viridis_d(option = \"magma\") +\n    guides(fill = \"none\") +\n    labs(title = \"Ridgeline Plot of Triceps Skinfold by BMI Category (nnyfs)\",\n         x = \"Triceps Skinfold\", y = \"BMI Category\") +\n    theme_light()\n\nPicking joint bandwidth of 1.37\n\n\n\n\n\nFor one last example, we’ll look at age by BMI category, so that sorting the BMI subgroups by the median matters, and we’ll try an alternate color scheme, and a theme specially designed for the ridgeline plot.\n\nnnyfs |>\n    filter(complete.cases(bmi_cat, age_child)) |>\n    mutate(bmi_cat = reorder(bmi_cat, age_child, median)) %>%\n    ggplot(aes(x = age_child, y = bmi_cat, fill = bmi_cat, height = ..density..)) +\n    ggridges::geom_density_ridges(scale = 0.85) + \n    scale_fill_brewer(palette = \"YlOrRd\") +\n    guides(fill = \"none\") +\n    labs(title = \"Ridgeline Plot of Age at Exam by BMI category (nnyfs)\",\n         x = \"Age of Child at Exam\", y = \"BMI Category\") +\n    ggridges::theme_ridges()\n\nPicking joint bandwidth of 1.15"
  },
  {
    "objectID": "09-nnyfs_foundations.html#what-summaries-to-report",
    "href": "09-nnyfs_foundations.html#what-summaries-to-report",
    "title": "9  National Youth Fitness Survey",
    "section": "9.23 What Summaries to Report",
    "text": "9.23 What Summaries to Report\nIt is usually helpful to focus on the shape, center and spread of a distribution. Bock, Velleman and DeVeaux provide some useful advice:\n\nIf the data are skewed, report the median and IQR (or the three middle quantiles). You may want to include the mean and standard deviation, but you should point out why the mean and median differ. The fact that the mean and median do not agree is a sign that the distribution may be skewed. A histogram will help you make that point.\nIf the data are symmetric, report the mean and standard deviation, and possibly the median and IQR as well.\nIf there are clear outliers and you are reporting the mean and standard deviation, report them with the outliers present and with the outliers removed. The differences may be revealing. The median and IQR are not likely to be seriously affected by outliers.\n\n\n\n\n\nWickham, Hadley, and Garrett Grolemund. 2022. R for Data Science. Second. O’Reilly. https://r4ds.hadley.nz/."
  },
  {
    "objectID": "10-assessingnormality.html",
    "href": "10-assessingnormality.html",
    "title": "10  Assessing Normality",
    "section": "",
    "text": "knitr::opts_chunk$set(comment = NA)\n\nlibrary(patchwork)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\n\nWe also use the favstat function from the mosaic package in this chapter, but do not load the whole packages."
  },
  {
    "objectID": "10-assessingnormality.html#introduction",
    "href": "10-assessingnormality.html#introduction",
    "title": "10  Assessing Normality",
    "section": "10.2 Introduction",
    "text": "10.2 Introduction\nData are well approximated by a Normal distribution if the shape of the data’s distribution is a good match for a Normal distribution with mean and standard deviation equal to the sample statistics.\n\nthe data are symmetrically distributed about a single peak, located at the sample mean\nthe spread of the distribution is well characterized by a Normal distribution with standard deviation equal to the sample standard deviation\nthe data show outlying values (both in number of candidate outliers, and size of the distance between the outliers and the center of the distribution) that are similar to what would be predicted by a Normal model.\n\nWe have several tools for assessing Normality of a single batch of data, including:\n\na histogram with superimposed Normal distribution\nhistogram variants (like the boxplot) which provide information on the center, spread and shape of a distribution\nthe Empirical Rule for interpretation of a standard deviation\na specialized normal Q-Q plot (also called a normal probability plot or normal quantile-quantile plot) designed to reveal differences between a sample distribution and what we might expect from a normal distribution of a similar number of values with the same mean and standard deviation"
  },
  {
    "objectID": "10-assessingnormality.html#empirical-rule-interpretation-of-the-standard-deviation",
    "href": "10-assessingnormality.html#empirical-rule-interpretation-of-the-standard-deviation",
    "title": "10  Assessing Normality",
    "section": "10.3 Empirical Rule Interpretation of the Standard Deviation",
    "text": "10.3 Empirical Rule Interpretation of the Standard Deviation\nFor a set of measurements that follows a Normal distribution, the interval:\n\nMean \\(\\pm\\) Standard Deviation contains approximately 68% of the measurements;\nMean \\(\\pm\\) 2(Standard Deviation) contains approximately 95% of the measurements;\nMean \\(\\pm\\) 3(Standard Deviation) contains approximately all (99.7%) of the measurements.\n\nAgain, most data sets do not follow a Normal distribution. We will occasionally think about transforming or re-expressing our data to obtain results which are better approximated by a Normal distribution, in part so that a standard deviation can be more meaningful.\nFor the energy data we have been studying, here again are some summary statistics…\n\nnnyfs <- read_rds(\"data/nnyfs.Rds\")\n\n\nmosaic::favstats(nnyfs$energy)\n\n min     Q1 median   Q3  max     mean       sd    n missing\n 257 1367.5 1794.5 2306 5265 1877.157 722.3537 1518       0\n\n\nThe mean is 1877 and the standard deviation is 722, so if the data really were Normally distributed, we’d expect to see:\n\nAbout 68% of the data in the range (1155, 2600). In fact, 1085 of the 1518 energy values are in this range, or 71.5%.\nAbout 95% of the data in the range (432, 3322). In fact, 1450 of the 1518 energy values are in this range, or 95.5%.\nAbout 99.7% of the data in the range (-290, 4044). In fact, 1502 of the 1518 energy values are in this range, or 98.9%.\n\nSo, based on this Empirical Rule approximation, do the energy data seem to be well approximated by a Normal distribution?"
  },
  {
    "objectID": "10-assessingnormality.html#describing-outlying-values-with-z-scores",
    "href": "10-assessingnormality.html#describing-outlying-values-with-z-scores",
    "title": "10  Assessing Normality",
    "section": "10.4 Describing Outlying Values with Z Scores",
    "text": "10.4 Describing Outlying Values with Z Scores\nThe maximum energy consumption value here is 5265. One way to gauge how extreme this is (or how much of an outlier it is) uses that observation’s Z score, the number of standard deviations away from the mean that the observation falls.\nHere, the maximum value, 5265 is 4.69 standard deviations above the mean, and thus has a Z score of 4.7.\nA negative Z score would indicate a point below the mean, while a positive Z score indicates, as we’ve seen, a point above the mean. The minimum body-mass index, 257 is 2.24 standard deviations below the mean, so it has a Z score of -2.2.\nRecall that the Empirical Rule suggests that if a variable follows a Normal distribution, it would have approximately 95% of its observations falling inside a Z score of (-2, 2), and 99.74% falling inside a Z score range of (-3, 3).\n\n10.4.1 Fences and Z Scores\nNote the relationship between the fences (Tukey’s approach to identifying points which fall within the whiskers of a boxplot, as compared to candidate outliers) and the Z scores.\nThe upper inner fence in this case falls at 3713.75, which indicates a Z score of 2.5, while the lower inner fence falls at -40.25, which indicates a Z score of -2.7. It is neither unusual nor inevitable for the inner fences to fall at Z scores near -2.0 and +2.0."
  },
  {
    "objectID": "10-assessingnormality.html#comparing-a-histogram-to-a-normal-distribution",
    "href": "10-assessingnormality.html#comparing-a-histogram-to-a-normal-distribution",
    "title": "10  Assessing Normality",
    "section": "10.5 Comparing a Histogram to a Normal Distribution",
    "text": "10.5 Comparing a Histogram to a Normal Distribution\nMost of the time, when we want to understand whether our data are well approximated by a Normal distribution, we will use a graph to aid in the decision.\nOne option is to build a histogram with a Normal density function (with the same mean and standard deviation as our data) superimposed. This is one way to help visualize deviations between our data and what might be expected from a Normal distribution.\n\nres <- mosaic::favstats(~ energy, data = nnyfs)\nbin_w <- 50 # specify binwidth\n\nggplot(nnyfs, aes(x=energy)) +\n    geom_histogram(aes(y = ..density..), binwidth = bin_w, \n                   fill = \"papayawhip\", color = \"seagreen\") +\n    stat_function(fun = dnorm, \n                  args = list(mean = res$mean, sd = res$sd), \n                  lwd = 1.5, col = \"blue\") +\n    geom_text(aes(label = paste(\"Mean\", round(res$mean,1), \n                                \", SD\", round(res$sd,1))),\n              x = 4000, y = 0.0006, \n              color=\"blue\", fontface = \"italic\") + \n    labs(title = \"nnyfs energy values with Normal Density Superimposed\", \n         x = \"Energy (kcal)\", y = \"Probability Density Function\")\n\n\n\n\nDoes it seem as though the Normal model (as shown in the blue density curve) is an effective approximation to the observed distribution shown in the bars of the histogram?\nWe’ll return shortly to the questions:\n\nDoes a Normal distribution model fit our data well? and\nIf the data aren’t Normal, but we want to use a Normal model anyway, what should we do?\n\n\n10.5.1 Histogram of energy with Normal model (with Counts)\nBut first, we’ll demonstrate an approach to building a histogram of counts (rather than a probability density) and then superimposing a Normal model.\n\nres <- mosaic::favstats(~ energy, data = nnyfs)\nbin_w <- 50 # specify binwidth\n\nggplot(nnyfs, aes(x = energy)) +\n  geom_histogram(binwidth = bin_w, \n                 fill = \"papayawhip\", \n                 col = \"navy\") +\n  theme_bw() +\n  stat_function(\n    fun = function(x) dnorm(x, mean = res$mean, \n                            sd = res$sd) * res$n * bin_w,\n    col = \"blue\", size = 2) +\n    geom_text(aes(label = paste(\"Mean\", round(res$mean,1), \n                                \", SD\", round(res$sd,1))),\n              x = 4000, y = 50, \n              color=\"blue\", fontface = \"italic\") + \n    labs(title = \"Histogram of energy, with Normal Model\", \n         x = \"Energy consumed (kcal)\", y = \"# of subjects\")"
  },
  {
    "objectID": "10-assessingnormality.html#does-a-normal-model-work-well-for-the-waist-circumference",
    "href": "10-assessingnormality.html#does-a-normal-model-work-well-for-the-waist-circumference",
    "title": "10  Assessing Normality",
    "section": "10.6 Does a Normal model work well for the waist circumference?",
    "text": "10.6 Does a Normal model work well for the waist circumference?\nNow, suppose we instead look at the waist data, remembering to filter the data to the complete cases before plotting. Do these data appear to follow a Normal distribution?\n\nres <- mosaic::favstats(~ waist, data = nnyfs)\nbin_w <- 5 # specify binwidth\n\nnnyfs |> filter(complete.cases(waist)) %>%\n    ggplot(., aes(x = waist)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"antiquewhite\", \n                   col = \"navy\") +\n    theme_bw() +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"darkred\", size = 2) +\n    geom_text(aes(label = paste(\"Mean\", round(res$mean,1), \n                                \", SD\", round(res$sd,1))),\n              x = 100, y = 200, \n              color=\"darkred\", fontface = \"italic\") + \n    labs(title = \"Histogram of waist, with Normal Model\", \n         x = \"Waist Circumference (cm)\", y = \"# of subjects\")\n\n\n\n\n\nmosaic::favstats(~ waist, data = nnyfs)\n\n  min   Q1 median   Q3   max     mean       sd    n missing\n 42.5 55.6   64.8 76.6 144.7 67.70536 15.19809 1512       6\n\n\n\n\n\nThe mean is 67.71 and the standard deviation is 15.2 so if the waist data really were Normally distributed, we’d expect to see:\n\nAbout 68% of the data in the range (52.51, 82.9). In fact, 1076 of the 1512 Age values are in this range, or 71.2%.\nAbout 95% of the data in the range (37.31, 98.1). In fact, 1443 of the 1512 Age values are in this range, or 95.4%.\nAbout 99.7% of the data in the range (22.11, 113.3). In fact, 1500 of the 1512 Age values are in this range, or 99.2%.\n\nHow does the Normal approximation work for waist circumference, according to the Empirical Rule?"
  },
  {
    "objectID": "10-assessingnormality.html#the-normal-q-q-plot",
    "href": "10-assessingnormality.html#the-normal-q-q-plot",
    "title": "10  Assessing Normality",
    "section": "10.7 The Normal Q-Q Plot",
    "text": "10.7 The Normal Q-Q Plot\nA normal probability plot (or normal quantile-quantile plot) of the energy results from the nnyfs data, developed using ggplot2 is shown below. In this case, this is a picture of 1518 energy consumption assessments. The idea of a normal Q-Q plot is that it plots the observed sample values (on the vertical axis) and then, on the horizontal, the expected or theoretical quantiles that would be observed in a standard normal distribution (a Normal distribution with mean 0 and standard deviation 1) with the same number of observations.\nA Normal Q-Q plot will follow a straight line when the data are (approximately) Normally distributed. When the data have a different shape, the plot will reflect that.\n\nggplot(nnyfs, aes(sample = energy)) +\n    geom_qq() + geom_qq_line(col = \"red\") +\n    theme_light() +\n    labs(title = \"Normal Q-Q plot for energy data\")"
  },
  {
    "objectID": "10-assessingnormality.html#interpreting-the-normal-q-q-plot",
    "href": "10-assessingnormality.html#interpreting-the-normal-q-q-plot",
    "title": "10  Assessing Normality",
    "section": "10.8 Interpreting the Normal Q-Q Plot",
    "text": "10.8 Interpreting the Normal Q-Q Plot\nThe purpose of a Normal Q-Q plot is to help point out distinctions from a Normal distribution. A Normal distribution is symmetric and has certain expectations regarding its tails. The Normal Q-Q plot can help us identify data as well approximated by a Normal distribution, or not, because of:\n\nskew (including distinguishing between right skew and left skew)\nbehavior in the tails (which could be heavy-tailed [more outliers than expected] or light-tailed)\n\n\n10.8.1 Data from a Normal distribution shows up as a straight line in a Normal Q-Q plot\nWe’ll demonstrate the looks that we can obtain from a Normal Q-Q plot in some simulations. First, here is an example of a Normal Q-Q plot, and its associated histogram, for a sample of 200 observations simulated from a Normal distribution.\n\nset.seed(123431) # so the results can be replicated\n                                          \n# simulate 200 observations from a Normal(20, 5) distribution and place them \n# in the d variable within the temp.1 data frame\ntemp.1 <- data.frame(d = rnorm(200, mean = 20, sd = 5)) \n                                          \n# left plot - basic Normal Q-Q plot of simulated data\np1 <- ggplot(temp.1, aes(sample = d)) +\n    geom_qq() + geom_qq_line(col = \"red\") +\n    theme_light() +\n    labs(y = \"Ordered Simulated Sample Data\")\n\n# right plot - histogram with superimposed normal distribution\nres <- mosaic::favstats(~ d, data = temp.1)\nbin_w <- 2 # specify binwidth\n\np2 <- ggplot(temp.1, aes(x = d)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"papayawhip\", \n                   col = \"seagreen\") +\n    theme_bw() +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"blue\", size = 1.5) +\n    geom_text(aes(label = paste(\"Mean\", round(res$mean,1), \n                                \", SD\", round(res$sd,1))),\n              x = 25, y = 35, \n              color=\"blue\", fontface = \"italic\") + \n    labs(x = \"Simulated Sample Data\", y = \"\")\n\np1 + p2 + \n  plot_annotation(title = \"200 observations from a simulated Normal distribution\") \n\n\n\n# uses patchwork package to combine plots\n\nThese simulated data appear to be well-modeled by the Normal distribution, because the points on the Normal Q-Q plot follow the diagonal reference line. In particular,\n\nthere is no substantial curve (such as we’d see with data that were skewed)\nthere is no particularly surprising behavior (curves away from the line) at either tail, so there’s no obvious problem with outliers\n\n\n\n10.8.2 Skew is indicated by monotonic curves in the Normal Q-Q plot\nData that come from a skewed distribution appear to curve away from a straight line in the Q-Q plot.\n\nset.seed(123431) # so the results can be replicated\n\n# simulate 200 observations from a beta(5, 2) distribution into the e1 variable\n# simulate 200 observations from a beta(1, 5) distribution into the e2 variable\ntemp.2 <- data.frame(e1 = rbeta(200, 5, 2), e2 = rbeta(200, 1, 5)) \n\np1 <- ggplot(temp.2, aes(sample = e1)) +\n    geom_qq(col = \"orchid\") + geom_qq_line(col = \"blue\") +\n    theme_light() +\n    labs(y = \"Ordered Sample e1\",\n         title = \"Beta(5, 2) sample: Left Skewed\")\n\np2 <- ggplot(temp.2, aes(sample = e2)) +\n    geom_qq(col = \"darkorange\") + geom_qq_line(col = \"blue\") +\n    theme_light() +\n    labs(y = \"Ordered Sample e2\",\n         title = \"Beta(1, 5) sample: Right Skewed\")\n\np1 + p2 + plot_annotation(title = \"200 observations from simulated Beta distributions\")\n\n\n\n\nNote the bends away from a straight line in each sample. The non-Normality may be easier to see in a histogram.\n\nres1 <- mosaic::favstats(~ e1, data = temp.2)\nbin_w1 <- 0.025 # specify binwidth\n\np1 <- ggplot(temp.2, aes(x = e1)) +\n    geom_histogram(binwidth = bin_w1, \n                   fill = \"orchid\", \n                   col = \"black\") +\n    theme_bw() +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res1$mean, \n                                sd = res1$sd) * \n            res1$n * bin_w1,\n        col = \"blue\", size = 1.5) +\nlabs(x = \"Sample e1\", y = \"\",\n     title = \"Beta(5,2) sample: Left Skew\")\n\nres2 <- mosaic::favstats(~ e2, data = temp.2)\nbin_w2 <- 0.025 # specify binwidth\n\np2 <- ggplot(temp.2, aes(x = e2)) +\n    geom_histogram(binwidth = bin_w2, \n                   fill = \"darkorange\", \n                   col = \"black\") +\n    theme_bw() +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res2$mean, \n                                sd = res2$sd) * \n            res2$n * bin_w2,\n        col = \"blue\", size = 1.5) +\nlabs(x = \"Sample e1\", y = \"\",\n     title = \"Beta(1,5) sample: Right Skew\")\n\np1 + p2 + plot_annotation(caption = \"Histograms with Normal curve superimposed\")\n\n\n\n\n\n\n10.8.3 Direction of Skew\nIn each of these pairs of plots, we see the same basic result.\n\nThe left plot (for data e1) shows left skew, with a longer tail on the left hand side and more clustered data at the right end of the distribution.\nThe right plot (for data e2) shows right skew, with a longer tail on the right hand side, the mean larger than the median, and more clustered data at the left end of the distribution.\n\n\n\n10.8.4 Outlier-proneness is indicated by “s-shaped” curves in a Normal Q-Q plot\n\nHeavy-tailed but symmetric distributions are indicated by reverse “S”-shapes, as shown on the left below.\nLight-tailed but symmetric distributions are indicated by “S” shapes in the plot, as shown on the right below.\n\n\nset.seed(4311) # so the results can be replicated\n\n# sample 200 observations from each of two probability distributions\ntemp.3 <- data.frame(s1 = rcauchy(200, location=10, scale = 1),\n                     s2 = runif(200, -30, 30)) \n\np1 <- ggplot(temp.3, aes(sample = s1)) +\n    geom_qq(col = \"slateblue\") + geom_qq_line(col = \"black\") +\n    theme_light() +\n    labs(y = \"Ordered Sample s1\",\n         title = \"Heavy-Tailed Symmetric Sample s1\")\n\np2 <- ggplot(temp.3, aes(sample = s2)) +\n    geom_qq(col = \"dodgerblue\") + geom_qq_line(col = \"black\") +\n    theme_light() +\n    labs(y = \"Ordered Sample s2\",\n         title = \"Light-Tailed Symmetric Sample s2\")\n\np1 + p2 + plot_annotation(title = \"200 observations from simulated distributions\")\n\n\n\n\nAnd, we can also visualize these simulations with histograms, although they’re less helpful for understanding tail behavior than they are for skew.\n\nres1 <- mosaic::favstats(~ s1, data = temp.3)\nbin_w1 <- 20 # specify binwidth\n\np1 <- ggplot(temp.3, aes(x = s1)) +\n    geom_histogram(binwidth = bin_w1, \n                   fill = \"slateblue\", \n                   col = \"white\") +\n    theme_bw() +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res1$mean, \n                                sd = res1$sd) * \n            res1$n * bin_w1,\n        col = \"blue\") +\nlabs(x = \"Sample s1\", y = \"\",\n     title = \"Cauchy sample: Heavy Tails\")\n\nres2 <- mosaic::favstats(~ s2, data = temp.3)\nbin_w2 <- 2 # specify binwidth\n\np2 <- ggplot(temp.3, aes(x = s2)) +\n    geom_histogram(binwidth = bin_w2, \n                   fill = \"dodgerblue\", \n                   col = \"white\") +\n    theme_bw() +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res2$mean, \n                                sd = res2$sd) * \n            res2$n * bin_w2,\n        col = \"blue\") +\nlabs(x = \"Sample s2\", y = \"\",\n     title = \"Uniform sample: Light Tails\")\n\np1 + p2 + plot_annotation(caption = \"Histograms with Normal curve superimposed\")\n\n\n\n\nInstead, boxplots (here augmented with violin plots) can be more helpful when thinking about light-tailed vs. heavy-tailed distributions.\n\np1 <- ggplot(temp.3, aes(x = \"s1\", y = s1)) +\n    geom_violin(col = \"slateblue\") + \n    geom_boxplot(fill = \"slateblue\", width = 0.2) +\n    theme_light() +\n    coord_flip() +\n    labs(y = \"Ordered Sample s1\", x = \"\",\n         title = \"Heavy-Tailed Symmetric Sample s1\")\n\np2 <- ggplot(temp.3, aes(x = \"s2\", y = s2)) +\n    geom_violin(col = \"dodgerblue\") + \n    geom_boxplot(fill = \"dodgerblue\", width = 0.2) +\n    theme_light() +\n    coord_flip() +\n    labs(y = \"Ordered Sample s2\", x = \"\",\n         title = \"Light-Tailed Symmetric Sample s2\")\n\np1 + p2 + plot_annotation(title = \"200 observations from simulated distributions\")\n\n\n\n\n\nrm(temp.1, temp.2, temp.3, p1, p2, res, res1, res2, bin_w, bin_w1, bin_w2) # cleaning up"
  },
  {
    "objectID": "10-assessingnormality.html#can-a-normal-distribution-fit-the-nnyfs-energy-data-well",
    "href": "10-assessingnormality.html#can-a-normal-distribution-fit-the-nnyfs-energy-data-well",
    "title": "10  Assessing Normality",
    "section": "10.9 Can a Normal Distribution Fit the nnyfs energy data Well?",
    "text": "10.9 Can a Normal Distribution Fit the nnyfs energy data Well?\nThe energy data we’ve been studying shows meaningful signs of right skew.\n\np1 <- ggplot(nnyfs, aes(sample = energy)) +\n    geom_qq(col = \"coral\", size = 2) + \n    geom_qq_line(col = \"blue\") +\n    theme_light() +\n    labs(title = \"Energy Consumed\",\n         y = \"Sorted Energy data\")\n\nres <- mosaic::favstats(~ energy, data = nnyfs)\nbin_w <- 250 # specify binwidth\n\np2 <- ggplot(nnyfs, aes(x = energy)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"coral\", \n                   col = \"white\") +\n    theme_bw() +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"blue\", size = 1.5) +\nlabs(x = \"Energy (kcal consumed)\", y = \"\",\n     title = \"Energy Consumed\")\n\n\n\np1 + p2\n\n\n\n\n\nSkewness is indicated by the curve in the Normal Q-Q plot. Curving up and away from the line in both tails suggests right skew, as does the histogram.\n\nWhat if we plotted not the original energy values (all of which are positive) but instead plotted the square roots of the energy values?\n\nCompare these two plots - the left describes the distribution of the original energy data from the NNYFS data frame, and the right plot shows the distribution of the square root of those values.\n\n\np1 <- ggplot(nnyfs, aes(sample = energy)) +\n    geom_qq(col = \"coral\", size = 2) + \n    geom_qq_line(col = \"blue\") +\n    theme_light() +\n    labs(title = \"Energy Consumed\",\n         y = \"Sorted Energy data\")\n\np2 <- ggplot(nnyfs, aes(sample = sqrt(energy))) +\n    geom_qq(col = \"darkcyan\", size = 2) + \n    geom_qq_line(col = \"blue\") +\n    theme_light() +\n    labs(title = \"Square Root of Energy\",\n         y = \"Sorted Square Root of Energy\")\n\np1 + p2\n\n\n\n\n\nThe left plot shows substantial right or positive skew\nThe right plot shows there’s much less skew after the square root has been taken.\n\nOur conclusion is that a Normal model is a far better fit to the square root of the energy values than it is to the raw energy values.\nThe effect of taking the square root may be clearer from the histograms below, with Normal models superimposed.\n\nres <- mosaic::favstats(~ energy, data = nnyfs)\nbin_w <- 250 # specify binwidth\n\np1 <- ggplot(nnyfs, aes(x = energy)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"coral\", \n                   col = \"white\") +\n    theme_bw() +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"black\", size = 1.5) +\nlabs(x = \"Energy (kcal consumed)\", y = \"\",\n     title = \"Energy Consumed\")\n\n\nres2 <- mosaic::favstats(~ sqrt(energy), data = nnyfs)\nbin_w2 <- 5 # specify binwidth\n\np2 <- ggplot(nnyfs, aes(x = sqrt(energy))) +\n    geom_histogram(binwidth = bin_w2, \n                   fill = \"darkcyan\", \n                   col = \"white\") +\n    theme_bw() +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res2$mean, \n                                sd = res2$sd) * \n            res2$n * bin_w2,\n        col = \"black\", size = 1.5) +\nlabs(x = \"Square Root of Energy\", y = \"\",\n     title = \"Square Root of Energy\")\n\n\np1 + p2 + plot_annotation(title = \"Comparing energy to sqrt(energy)\")\n\n\n\nrm(p1, p2, bin_w, bin_w2, res, res2) # cleanup\n\nWhen we are confronted with a variable that is not Normally distributed but that we wish was Normally distributed, it is sometimes useful to consider whether working with a transformation of the data will yield a more helpful result, as the square root does in this instance.\nThe rest of this Chapter provides some guidance about choosing from a class of power transformations that can reduce the impact of non-Normality in unimodal data.\n\nWhen we are confronted with a variable that is not Normally distributed but that we wish was Normally distributed, it is sometimes useful to consider whether working with a transformation of the data will yield a more helpful result.\nMany statistical methods, including t tests and analyses of variance, assume Normal distributions.\nWe’ll discuss using R to assess a range of what are called Box-Cox power transformations, via plots, mainly."
  },
  {
    "objectID": "10-assessingnormality.html#the-ladder-of-power-transformations",
    "href": "10-assessingnormality.html#the-ladder-of-power-transformations",
    "title": "10  Assessing Normality",
    "section": "10.10 The Ladder of Power Transformations",
    "text": "10.10 The Ladder of Power Transformations\nThe key notion in re-expression of a single variable to obtain a distribution better approximated by the Normal or re-expression of an outcome in a simple regression model is that of a ladder of power transformations, which applies to any unimodal data.\n\n\n\nPower\nTransformation\n\n\n\n\n3\nx3\n\n\n2\nx2\n\n\n1\nx (unchanged)\n\n\n0.5\nx0.5 = \\(\\sqrt{x}\\)\n\n\n0\nln x\n\n\n-0.5\nx-0.5 = 1/\\(\\sqrt{x}\\)\n\n\n-1\nx-1 = 1/x\n\n\n-2\nx-2 = 1/x2"
  },
  {
    "objectID": "10-assessingnormality.html#using-the-ladder",
    "href": "10-assessingnormality.html#using-the-ladder",
    "title": "10  Assessing Normality",
    "section": "10.11 Using the Ladder",
    "text": "10.11 Using the Ladder\nAs we move further away from the identity function (power = 1) we change the shape more and more in the same general direction.\n\nFor instance, if we try a logarithm, and this seems like too much of a change, we might try a square root instead.\nNote that this ladder (which like many other things is due to John Tukey) uses the logarithm for the “power zero” transformation rather than the constant, which is what x0 actually is.\nIf the variable x can take on negative values, we might take a different approach. If x is a count of something that could be zero, we often simply add 1 to x before transformation.\n\nThe ladder of power transformations is particularly helpful when we are confronted with data that shows skew.\n\nTo handle right skew (where the mean exceeds the median) we usually apply powers below 1.\nTo handle left skew (where the median exceeds the mean) we usually apply powers greater than 1.\n\nThe most common transformations are the square (power 2), the square root (power 1/2), the logarithm (power 0) and the inverse (power -1), and I usually restrict myself to those options in practical work."
  },
  {
    "objectID": "10-assessingnormality.html#protein-consumption-in-the-nnyfs-data",
    "href": "10-assessingnormality.html#protein-consumption-in-the-nnyfs-data",
    "title": "10  Assessing Normality",
    "section": "10.12 Protein Consumption in the NNYFS data",
    "text": "10.12 Protein Consumption in the NNYFS data\nHere are the protein consumption (in grams) results from the NNYFS data.\n\nmosaic::favstats(~ protein, data = nnyfs)\n\n  min    Q1 median     Q3    max     mean       sd    n missing\n 4.18 45.33 61.255 82.565 241.84 66.90148 30.96319 1518       0\n\n\n\np1 <- ggplot(nnyfs, aes(x = \"Protein\", y = protein)) +\n    geom_violin() +\n    geom_boxplot(width = 0.2, fill = \"salmon\", \n                 outlier.color = \"red\") +\n    \n    labs(title = \"NNYFS Protein consumption\",\n         x = \"\", y = \"Protein Consumption (g)\")\n\np2 <- ggplot(nnyfs, aes(sample = protein)) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    \n    labs(title = \"NNYFS Protein Consumption\",\n         y = \"Protein Consumption (g)\")\n\np1 + p2\n\n\n\n\nThe key point here is that we see several signs of meaningful right skew, and we’ll want to consider a transformation that might make a Normal model more plausible.\n\n10.12.1 Using patchwork to compose plots\nAs we mentioned previously, I feel that the slickest approach to composing how a series of plots are placed together is available in the patchwork package. Here’s another example.\n\nres <- mosaic::favstats(~ protein, data = nnyfs)\nbin_w <- 5 # specify binwidth\n\np1 <- ggplot(nnyfs, aes(x = protein)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"salmon\", \n                   col = \"white\") +\n    \n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"darkred\", size = 2) +\n    labs(title = \"Histogram with Normal fit\", \n         x = \"Protein Consumption (g)\", y = \"# of subjects\")\n\n\np2 <- ggplot(nnyfs, aes(sample = protein)) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    \n    labs(title = \"Normal Q-Q plot\",\n         y = \"Protein Consumption (g)\")\n\np3 <- ggplot(nnyfs, aes(x = \"\", y = protein)) +\n    geom_violin() +\n    geom_boxplot(width = 0.2, fill = \"salmon\", \n                 outlier.color = \"red\") +\n    \n    coord_flip() +\n    labs(title = \"Boxplot with Violin\",\n         x = \"\", y = \"Protein Consumption (g)\")\n\np1 + p2 - p3 + plot_layout(ncol = 1, height = c(3, 1)) +\n    plot_annotation(title = \"NNYFS Protein Consumption\")\n\n\n\n\nAgain, the patchwork package repository at https://patchwork.data-imaginist.com/index.html has lots of nice examples to work from."
  },
  {
    "objectID": "10-assessingnormality.html#can-we-transform-the-protein-data",
    "href": "10-assessingnormality.html#can-we-transform-the-protein-data",
    "title": "10  Assessing Normality",
    "section": "10.13 Can we transform the protein data?",
    "text": "10.13 Can we transform the protein data?\nAs we’ve seen, the protein data are right skewed, and all of the values are strictly positive. If we want to use the tools of the Normal distribution to describe these data, we might try taking a step “down” our ladder from power 1 (raw data) to lower powers.\n\n10.13.1 The Square Root\nWould a square root applied to the protein data help alleviate that right skew?\n\nres <- mosaic::favstats(~ sqrt(protein), data = nnyfs)\nbin_w <- 1 # specify binwidth\n\np1 <- ggplot(nnyfs, aes(x = sqrt(protein))) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"salmon\", \n                   col = \"white\") +\n    \n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"darkred\", size = 2) +\n    labs(title = \"Histogram with Normal fit\", \n         x = \"Square Root of Protein Consumption (g)\", y = \"# of subjects\")\n\n\np2 <- ggplot(nnyfs, aes(sample = sqrt(protein))) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    \n    labs(title = \"Normal Q-Q plot\",\n         y = \"Square Root of Protein Consumption (g)\")\n\np3 <- ggplot(nnyfs, aes(x = \"\", y = sqrt(protein))) +\n    geom_violin() +\n    geom_boxplot(width = 0.2, fill = \"salmon\", \n                 outlier.color = \"red\") +\n    \n    coord_flip() +\n    labs(title = \"Boxplot with Violin\",\n         x = \"\", y = \"Square Root of Protein Consumption (g)\")\n\np1 + p2 - p3 + plot_layout(ncol = 1, height = c(3, 1)) +\n    plot_annotation(title = \"Square Root of NNYFS Protein Consumption\")\n\n\n\n\nThat looks like a more symmetric distribution, certainly, although we still have some outliers on the right side of the distribution. Should we take another step down the ladder?\n\n\n10.13.2 The Logarithm\nWe might also try a logarithm of the energy circumference data. We can use either the natural logarithm (log, in R) or the base-10 logarithm (log10, in R) - either will have the same impact on skew.\n\nres <- mosaic::favstats(~ log(protein), data = nnyfs)\nbin_w <- 0.5 # specify binwidth\n\np1 <- ggplot(nnyfs, aes(x = log(protein))) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"salmon\", \n                   col = \"white\") +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"darkred\", size = 2) +\n    labs(title = \"Histogram with Normal fit\", \n         x = \"Log of Protein Consumption (g)\", y = \"# of subjects\")\n\n\np2 <- ggplot(nnyfs, aes(sample = log(protein))) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Normal Q-Q plot\",\n         y = \"Log of Protein Consumption (g)\")\n\np3 <- ggplot(nnyfs, aes(x = \"\", y = log(protein))) +\n    geom_violin() +\n    geom_boxplot(width = 0.2, fill = \"salmon\", \n                 outlier.color = \"red\") +\n    coord_flip() +\n    labs(title = \"Boxplot with Violin\",\n         x = \"\", y = \"Log of Protein Consumption (g)\")\n\np1 + p2 - p3 + plot_layout(ncol = 1, height = c(3, 1)) +\n    plot_annotation(title = \"Logarithm of NNYFS Protein Consumption\")\n\n\n\n\nNow, it looks like we may have gone too far in the other direction. It looks like the square root is a sensible choice to try to improve the fit of a Normal model to the protein consumption data.\n\n\n10.13.3 This course uses Natural Logarithms, unless otherwise specified\nIn this course, we will assume the use of natural logarithms unless we specify otherwise. Following R’s convention, we will use log for natural logarithms."
  },
  {
    "objectID": "10-assessingnormality.html#what-if-we-considered-all-9-available-transformations",
    "href": "10-assessingnormality.html#what-if-we-considered-all-9-available-transformations",
    "title": "10  Assessing Normality",
    "section": "10.14 What if we considered all 9 available transformations?",
    "text": "10.14 What if we considered all 9 available transformations?\n\np1 <- ggplot(nnyfs, aes(sample = protein^3)) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Cube (power 3)\",\n         y = \"Protein, Cubed\")\n\np2 <- ggplot(nnyfs, aes(sample = protein^2)) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Square (power 2)\",\n         y = \"Protein, Squared\")\n\np3 <- ggplot(nnyfs, aes(sample = protein)) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Original Data\",\n         y = \"Protein (g)\")\n\n\np4 <- ggplot(nnyfs, aes(sample = sqrt(protein))) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"sqrt (power 0.5)\",\n         y = \"Square Root of Protein\")\n\np5 <- ggplot(nnyfs, aes(sample = log(protein))) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"log (power 0)\",\n         y = \"Natural Log of Protein\")\n\np6 <- ggplot(nnyfs, aes(sample = protein^(-0.5))) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"1/sqrt (power -0.5)\",\n         y = \"1/Square Root(Protein)\")\n\n\np7 <- ggplot(nnyfs, aes(sample = 1/protein)) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Inverse (power -1)\",\n         y = \"1/Protein\")\n\np8 <- ggplot(nnyfs, aes(sample = 1/(protein^2))) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"1/Square (power -2)\",\n         y = \"1 /(Protein squared)\")\n\np9 <- ggplot(nnyfs, aes(sample = 1/(protein^3))) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"1/Cube (power -3)\",\n         y = \"1/(Protein cubed)\")\n\n\np1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 +\n    plot_layout(nrow = 3) +\n    plot_annotation(title = \"Transformations of NNYFS Protein Consumption\")\n\n\n\n\nThe square root still appears to be the best choice of transformation here, even after we consider all 8 transformation of the raw data."
  },
  {
    "objectID": "10-assessingnormality.html#a-simulated-data-set",
    "href": "10-assessingnormality.html#a-simulated-data-set",
    "title": "10  Assessing Normality",
    "section": "10.15 A Simulated Data Set",
    "text": "10.15 A Simulated Data Set\n\nset.seed(431); \ndata2 <- \n    data_frame(sample2 = 100*rbeta(n = 125, shape1 = 5, shape2 = 2))\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nPlease use `tibble()` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\nIf we’d like to transform these data so as to better approximate a Normal distribution, where should we start? What transformation do you suggest?\n\nres <- mosaic::favstats(~ sample2, data = data2)\nbin_w <- 4 # specify binwidth\n\np1 <- ggplot(data2, aes(x = sample2)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"royalblue\", \n                   col = \"white\") +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"darkred\", size = 2) +\n    labs(title = \"Histogram with Normal fit\", \n         x = \"Simulated Data\", y = \"# of subjects\")\n\n\np2 <- ggplot(data2, aes(sample = sample2)) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Normal Q-Q plot\",\n         y = \"Simulated Data\")\n\np3 <- ggplot(data2, aes(x = \"\", y = sample2)) +\n    geom_violin() +\n    geom_boxplot(width = 0.3, fill = \"royalblue\", \n                 outlier.color = \"royalblue\") +\n    coord_flip() +\n    labs(title = \"Boxplot with Violin\",\n         x = \"\", y = \"Simulated Data\")\n\np1 + p2 - p3 + plot_layout(ncol = 1, height = c(3, 1)) +\n    plot_annotation(title = \"Simulated Data\")\n\n\n\n\nGiven the left skew in the data, it looks like a step up in the ladder is warranted, perhaps by looking at the square of the data?\n\nres <- mosaic::favstats(~ sample2^2, data = data2)\nbin_w <- 600 # specify binwidth\n\np1 <- ggplot(data2, aes(x = sample2^2)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"royalblue\", \n                   col = \"white\") +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"darkred\", size = 2) +\n    labs(title = \"Histogram with Normal fit\", \n         x = \"Squared Simulated Data\", y = \"# of subjects\")\n\n\np2 <- ggplot(data2, aes(sample = sample2^2)) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Normal Q-Q plot\",\n         y = \"Squared Simulated Data\")\n\np3 <- ggplot(data2, aes(x = \"\", y = sample2^2)) +\n    geom_violin() +\n    geom_boxplot(width = 0.3, fill = \"royalblue\", \n                 outlier.color = \"royalblue\") +\n    coord_flip() +\n    labs(title = \"Boxplot with Violin\",\n         x = \"\", y = \"Squared Simulated Data\")\n\np1 + p2 - p3 + plot_layout(ncol = 1, height = c(3, 1)) +\n    plot_annotation(title = \"Squared Simulated Data\")\n\n\n\n\nLooks like at best a modest improvement. How about cubing the data, instead?\n\nres <- mosaic::favstats(~ sample2^3, data = data2)\nbin_w <- 100000 # specify binwidth\n\np1 <- ggplot(data2, aes(x = sample2^3)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"royalblue\", \n                   col = \"white\") +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"darkred\", size = 2) +\n    labs(title = \"Histogram with Normal fit\", \n         x = \"Cubed Simulated Data\", y = \"# of subjects\")\n\n\np2 <- ggplot(data2, aes(sample = sample2^3)) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Normal Q-Q plot\",\n         y = \"Cubed Simulated Data\")\n\np3 <- ggplot(data2, aes(x = \"\", y = sample2^3)) +\n    geom_violin() +\n    geom_boxplot(width = 0.3, fill = \"royalblue\", \n                 outlier.color = \"royalblue\") +\n    coord_flip() +\n    labs(title = \"Boxplot with Violin\",\n         x = \"\", y = \"Cubed Simulated Data\")\n\np1 + p2 - p3 + plot_layout(ncol = 1, height = c(3, 1)) +\n    plot_annotation(title = \"Cubed Simulated Data\")\n\n\n\n\nThe newly transformed (cube of the) data appears more symmetric, although somewhat light-tailed. Perhaps a Normal model would be more appropriate now, although the standard deviation is likely to overstate the variation we see in the data due to the light tails. Again, I wouldn’t be thrilled using a cube in practical work, as it is so hard to interpret, but it does look like a reasonable choice here."
  },
  {
    "objectID": "10-assessingnormality.html#what-if-we-considered-all-9-available-transformations-1",
    "href": "10-assessingnormality.html#what-if-we-considered-all-9-available-transformations-1",
    "title": "10  Assessing Normality",
    "section": "10.16 What if we considered all 9 available transformations?",
    "text": "10.16 What if we considered all 9 available transformations?\n\np1 <- ggplot(data2, aes(sample = sample2^3)) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Cube (power 3)\")\n\np2 <- ggplot(data2, aes(sample = sample2^2)) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Square (power 2)\")\n\np3 <- ggplot(data2, aes(sample = sample2)) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Original Data\")\n\np4 <- ggplot(data2, aes(sample = sqrt(sample2))) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"sqrt (power 0.5)\")\n\np5 <- ggplot(data2, aes(sample = log(sample2))) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"log (power 0)\")\n\np6 <- ggplot(data2, aes(sample = sample2^(0.5))) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"1/sqrt (power -0.5)\")\n\np7 <- ggplot(data2, aes(sample = 1/sample2)) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Inverse (power -1)\")\n\np8 <- ggplot(data2, aes(sample = 1/(sample2^2))) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"1/Square (power -2)\")\n\np9 <- ggplot(data2, aes(sample = 1/(sample2^3))) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"1/Cube (power -3)\")\n\np1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 +\n    plot_layout(nrow = 3) +\n    plot_annotation(title = \"Transformations of Simulated Sample\")\n\n\n\n\nAgain, either the cube or the square looks like best choice here, in terms of creating a more symmetric (albeit light-tailed) distribution."
  },
  {
    "objectID": "11-straightlinemodels.html",
    "href": "11-straightlinemodels.html",
    "title": "11  Straight Line Models",
    "section": "",
    "text": "knitr::opts_chunk$set(comment = NA)\n\nlibrary(broom)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "11-straightlinemodels.html#assessing-a-scatterplot",
    "href": "11-straightlinemodels.html#assessing-a-scatterplot",
    "title": "11  Straight Line Models",
    "section": "11.2 Assessing A Scatterplot",
    "text": "11.2 Assessing A Scatterplot\nLet’s consider the relationship of protein and fat consumption for children in the nnyfs data.\n\nnnyfs <- read_rds(\"data/nnyfs.Rds\")\n\nWe’ll begin our investigation, as we always should, by drawing a relevant picture. For the association of two quantitative variables, a scatterplot is usually the right start. Each subject in the nnyfs data is represented by one of the points below. To the plot, I’ve also used geom_smooth to add a straight line regression model, which we’ll discuss later.\n\nggplot(data = nnyfs, aes(x = protein, y = fat)) +\n    geom_point(shape = 1, size = 2, col = \"sienna\") +\n    geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE, col = \"steelblue\") +\n    labs(title = \"Protein vs. Fat consumption\",\n         subtitle = \"Children in the NNYFS data, with fitted straight line regression model\",\n         x = \"Protein (in g)\", y = \"Fat (in g)\")\n\n\n\n\nHere, I’ve arbitrarily decided to place fat on the vertical axis, and protein on the horizontal. Fitting a prediction model to this scatterplot will then require that we predict fat on the basis of protein.\nIn this case, the pattern appears to be:\n\ndirect, or positive, in that the values of the \\(x\\) variable (protein) increase, so do the values of the \\(y\\) variable (fat). Essentially, it appears that subjects who consumed more protein also consumed more fat, but we don’t know cause and effect here.\nfairly linear in that most of the points cluster around what appears to be a pattern which is well-fitted by a straight line.\nmoderately strong in that the range of values for fat associated with any particular value of protein is fairly tight. If we know someone’s protein consumption, that should meaningfully improve our ability to predict their fat consumption, among the subjects in these data.\nthat we see some unusual or outlier values, further away from the general pattern of most subjects shown in the data.\n\n\n11.2.1 Highlighting an unusual point\nConsider the subject with protein consumption close to 200 g, whose fat consumption is below 100 g. That’s well below the prediction of the linear model for example. We can identify the subject because it is the only person with protein > 190 and fat < 100 with BMI > 35 and waist.circ < 70. So I’ll create a subset of the nnyfs data containing the point that meets that standard, and then add a red point and a label to the plot.\n\n# identify outlier and place it in data frame s1 \ns1 <- filter(nnyfs, protein > 190 & fat < 100)\n\nggplot(data = nnyfs, aes(x = protein, y = fat)) +\n    geom_point(shape = 1, size = 2, col = \"sienna\") +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x, col = \"steelblue\") +\n    geom_point(data = s1, size = 2, col = \"red\") +\n    geom_text(data = s1, label = s1$SEQN, \n              vjust = -1, col = \"red\") +\n    labs(title = \"Protein vs. Fat consumption in NNYFS\",\n         subtitle = \"with regression line, and an outlier labeled by SEQN\",\n         x = \"Protein (in g)\", y = \"Fat (in g)\")\n\n\n\n\nWhile this subject is hardly the only unusual point in the data set, it is one of the more unusual ones, in terms of its vertical distance from the regression line. We can identify the subject by printing (part of) the tibble we created.\n\ns1 |> select(SEQN, sex, race_eth, age_child, protein, fat) |> kable()\n\n\n\n\nSEQN\nsex\nrace_eth\nage_child\nprotein\nfat\n\n\n\n\n71919\nFemale\n2_White Non-Hispanic\n14\n199.33\n86.08\n\n\n\n\n\nNow, does it seem to you like a straight line model will describe this protein-fat relationship well?\n\n\n11.2.2 Adding a Scatterplot Smooth using loess\nNext, we’ll use the loess procedure to fit a smooth curve to the data, which attempts to capture the general pattern.\n\nggplot(data = nnyfs, aes(x = protein, y = fat)) +\n    geom_point(shape = 1, size = 2, col = \"sienna\") +\n    geom_smooth(method = \"loess\", se = FALSE, formula = y ~ x, col = \"red\") +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x, col = \"steelblue\") +\n    labs(title = \"Protein vs. Fat consumption in NNYFS\",\n         subtitle = \"with loess smooth (red) and linear fit (blue)\",\n         x = \"Protein (in g)\", y = \"Fat (in g)\")\n\n\n\n\nThis “loess” smooth curve is fairly close to the straight line fit, indicating that perhaps a linear regression model might fit the data well.\nA loess smooth is a method of fitting a local polynomial regression model that R uses as its default smooth for scatterplots with fewer than 1000 observations. Think of the loess as a way of fitting a curve to data by tracking (at point x) the points within a neighborhood of point x, with more emphasis given to points near x. It can be adjusted by tweaking two specific parameters, in particular:\n\na span parameter (defaults to 0.75) which is also called \\(\\alpha\\) in the literature, that controls the degree of smoothing (essentially, how large the neighborhood should be), and\na degree parameter (defaults to 2) which specifies the degree of polynomial to be used. Normally, this is either 1 or 2 - more complex functions are rarely needed for simple scatterplot smoothing.\n\nIn addition to the curve, smoothing procedures can also provide confidence intervals around their main fitted line. Consider the following plot, which adjusts the span and also adds in the confidence intervals.\n\np1 <- ggplot(data = nnyfs, aes(x = protein, y = fat)) +\n    geom_point(shape = 1, size = 2, col = \"sienna\") +\n    geom_smooth(method = \"loess\", span = 0.75, se = TRUE, \n                col = \"red\", formula = y ~ x) +\n    labs(title = \"loess smooth (span = 0.75)\",\n         x = \"Protein (in g)\", y = \"Fat (in g)\")\n\np2 <- ggplot(data = nnyfs, aes(x = protein, y = fat)) +\n    geom_point(shape = 1, size = 2, col = \"sienna\") +\n    geom_smooth(method = \"loess\", span = 0.2, se = TRUE, \n                col = \"red\", formula = y ~ x) +\n    labs(title = \"loess smooth (span = 0.2)\",\n         x = \"Protein (in g)\", y = \"Fat (in g)\")\n\np1 + p2 + \n    plot_annotation(title = \"Impact of adjusting loess smooth span: NNYFS\")\n\n\n\n\nBy reducing the size of the span, the plot on the right shows a somewhat less “smooth” function than the plot on the left.\n\n\n11.2.3 What Line Does R Fit?\nReturning to the linear regression model, how can we, mathematically, characterize that line? As with any straight line, our model equation requires us to specify two parameters: a slope and an intercept (sometimes called the y-intercept.)\nTo identify the equation R used to fit this line (using the method of least squares), we use the lm command\n\nlm(fat ~ protein, data = nnyfs)\n\n\nCall:\nlm(formula = fat ~ protein, data = nnyfs)\n\nCoefficients:\n(Intercept)      protein  \n    18.8945       0.7251  \n\n\nSo the fitted line is specified as\n\\[\n\\mbox{fat} = 18.8945 + 0.7251 \\mbox{ protein }\n\\]\nA detailed summary of the fitted linear regression model is also available.\n\nsummary(lm(fat ~ protein, data = nnyfs))\n\n\nCall:\nlm(formula = fat ~ protein, data = nnyfs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-77.798 -14.841  -2.449  13.601 110.597 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  18.8945     1.5330   12.32   <2e-16 ***\nprotein       0.7251     0.0208   34.87   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.08 on 1516 degrees of freedom\nMultiple R-squared:  0.4451,    Adjusted R-squared:  0.4447 \nF-statistic:  1216 on 1 and 1516 DF,  p-value: < 2.2e-16\n\n\nThe way we’ll usually summarize the estimated coefficients of a linear model is to use the broom package’s tidy function to put the coefficient estimates into a tibble.\n\ntidy(lm(fat ~ protein, data = nnyfs),\n            conf.int = TRUE, conf.level = 0.95) |>\n    kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n18.895\n1.533\n12.325\n0\n15.887\n21.902\n\n\nprotein\n0.725\n0.021\n34.868\n0\n0.684\n0.766\n\n\n\n\n\nWe can also summarize the quality of fit in a linear model using the broom package’s glance function. For now, we’ll focus our attention on just one of the many summaries available for a linear model from glance: the R-squared value.\n\nglance(lm(fat ~ protein, data = nnyfs)) |> select(r.squared) |>\n    kable(digits = 3)\n\n\n\n\nr.squared\n\n\n\n\n0.445\n\n\n\n\n\nWe’ll spend a lot of time working with these regression summaries in this course.\nFor now, it will suffice to understand the following:\n\nThe outcome variable in this model is fat, and the predictor variable is protein.\nThe straight line model for these data fitted by least squares is fat = 18.9 + 0.725 protein\nThe slope of protein is positive, which indicates that as protein increases, we expect that fat will also increase. Specifically, we expect that for every additional gram of protein consumed, the fat consumption will be 0.725 gram larger.\nThe multiple R-squared (squared correlation coefficient) is 0.445, which implies that 44.5% of the variation in fat is explained using this linear model with protein.\nThis also implies that the Pearson correlation between fat and protein is the square root of 0.445, or 0.667. More on the Pearson correlation soon.\n\nSo, if we plan to use a simple (least squares) linear regression model to describe fat consumption as a function of protein consumption in the NNYFS data, does it look like a least squares (or linear regression) model will be an effective choice?"
  },
  {
    "objectID": "11-straightlinemodels.html#correlation-coefficients",
    "href": "11-straightlinemodels.html#correlation-coefficients",
    "title": "11  Straight Line Models",
    "section": "11.3 Correlation Coefficients",
    "text": "11.3 Correlation Coefficients\nTwo different correlation measures are worth our immediate attention.\n\nThe one most often used is called the Pearson correlation coefficient, and is symbolized with the letter r or sometimes the Greek letter rho (\\(\\rho\\)).\nAnother tool is the Spearman rank correlation coefficient, also occasionally symbolized by \\(\\rho\\).\n\nFor the nnyfs data, the Pearson correlation of fat and protein can be found using the cor() function.\n\nnnyfs |> select(fat, protein) |> cor()\n\n              fat   protein\nfat     1.0000000 0.6671209\nprotein 0.6671209 1.0000000\n\n\nNote that the correlation of any variable with itself is 1, and that the correlation of fat with protein is the same regardless of whether you enter fat first or protein first."
  },
  {
    "objectID": "11-straightlinemodels.html#the-pearson-correlation-coefficient",
    "href": "11-straightlinemodels.html#the-pearson-correlation-coefficient",
    "title": "11  Straight Line Models",
    "section": "11.4 The Pearson Correlation Coefficient",
    "text": "11.4 The Pearson Correlation Coefficient\nSuppose we have \\(n\\) observations on two variables, called X and Y. The Pearson correlation coefficient assesses how well the relationship between X and Y can be described using a linear function.\n\nThe Pearson correlation is dimension-free.\nIt falls between -1 and +1, with the extremes corresponding to situations where all the points in a scatterplot fall exactly on a straight line with negative and positive slopes, respectively.\nA Pearson correlation of zero corresponds to the situation where there is no linear association.\nUnlike the estimated slope in a regression line, the sample correlation coefficient is symmetric in X and Y, so it does not depend on labeling one of them (Y) the response variable, and one of them (X) the predictor.\n\nSuppose we have \\(n\\) observations on two variables, called \\(X\\) and \\(Y\\), where \\(\\bar{X}\\) is the sample mean of \\(X\\) and \\(s_x\\) is the standard deviation of \\(X\\). The Pearson correlation coefficient \\(r_{XY}\\) is:\n\\[\nr_{XY} = \\frac{1}{n-1} \\sum\\limits_{i=1}^n (\\frac{x_i - \\bar{x}}{s_x}) (\\frac{y_i - \\bar{y}}{s_y})\n\\]"
  },
  {
    "objectID": "11-straightlinemodels.html#studying-correlation-through-6-examples",
    "href": "11-straightlinemodels.html#studying-correlation-through-6-examples",
    "title": "11  Straight Line Models",
    "section": "11.5 Studying Correlation through 6 Examples",
    "text": "11.5 Studying Correlation through 6 Examples\nThe correx1 data file contains six different sets of (x,y) points, identified by the set variable.\n\ncorrex1 <- read_csv(\"data/correx1.csv\") \n\nRows: 277 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): set\ndbl (2): x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsummary(correx1)\n\n     set                  x                y         \n Length:277         Min.   : 5.897   Min.   : 7.308  \n Class :character   1st Qu.:29.487   1st Qu.:30.385  \n Mode  :character   Median :46.154   Median :46.923  \n                    Mean   :46.529   Mean   :49.061  \n                    3rd Qu.:63.333   3rd Qu.:68.077  \n                    Max.   :98.205   Max.   :95.385  \n\n\n\n11.5.1 Data Set Alex\nLet’s start by working with the Alex data set.\n\nggplot(filter(correx1, set == \"Alex\"), aes(x = x, y = y)) + \n    geom_point() +\n    labs(title = \"correx1: Data Set Alex\")\n\n\n\n\n\nggplot(filter(correx1, set == \"Alex\"), aes(x = x, y = y)) + \n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x, col = \"blue\") +\n    labs(title = \"correx1: Alex, with loess smooth\")\n\n\n\n\n\nsetA <- filter(correx1, set == \"Alex\")\n\nggplot(setA, aes(x = x, y = y)) + \n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x, col = \"red\") +\n    labs(title = \"correx1: Alex, with Fitted Linear Model\") +\n    annotate(\"text\", x = 75, y = 75, col = \"purple\", size = 6,\n             label = paste(\"Pearson r = \", signif(cor(setA$x, setA$y),3))) +\n    annotate(\"text\", x = 50, y = 15,  col = \"red\", size = 5,\n             label = paste(\"y = \", signif(coef(lm(setA$y ~ setA$x))[1],3), \n                           signif(coef(lm(setA$y ~ setA$x))[2],2), \"x\"))\n\n\n\n\n\n\n11.5.2 Data Set Bonnie\n\nsetB <- dplyr::filter(correx1, set == \"Bonnie\")\n\nggplot(setB, aes(x = x, y = y)) + \n    geom_point() +\n    labs(title = \"correx1: Data Set Bonnie\")\n\n\n\n\n\nggplot(setB, aes(x = x, y = y)) + \n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x, col = \"red\") +\n    labs(title = \"correx1: Bonnie, with Fitted Linear Model\") +\n    annotate(\"text\", x = 25, y = 65, col = \"purple\", size = 6,\n             label = paste(\"Pearson r = \", signif(cor(setB$x, setB$y),2))) +\n    annotate(\"text\", x = 50, y = 15,  col = \"red\", size = 5,\n             label = paste(\"y = \", signif(coef(lm(setB$y ~ setB$x))[1],3), \n                           \" + \",\n                           signif(coef(lm(setB$y ~ setB$x))[2],2), \"x\"))\n\n\n\n\n\n\n11.5.3 Correlations for All Six Data Sets in the Correx1 Example\nLet’s look at the Pearson correlations associated with each of the six data sets contained in the correx1 example.\n\ntab1 <- correx1 |>\n    group_by(set) |>\n    summarise(\"Pearson r\" = round(cor(x, y, use=\"complete\"),2))\n\nknitr::kable(tab1)\n\n\n\n\nset\nPearson r\n\n\n\n\nAlex\n-0.97\n\n\nBonnie\n0.80\n\n\nColin\n-0.80\n\n\nDanielle\n0.00\n\n\nEarl\n-0.01\n\n\nFiona\n0.00\n\n\n\n\n\n\n\n11.5.4 Data Set Colin\nIt looks like the picture for Colin should be very similar (in terms of scatter) to the picture for Bonnie, except that Colin will have a negative slope, rather than the positive one Bonnie has. Is that how this plays out?\n\nsetBC <- filter(correx1, set == \"Bonnie\" | set == \"Colin\")\n\nggplot(setBC, aes(x = x, y = y)) + \n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x, col = \"red\") +\n    facet_wrap(~ set)\n\n\n\n\nUh, oh. It looks like the point in Colin at the top right is twisting what would otherwise be a very straight regression model with an extremely strong negative correlation. There’s no better way to look for outliers than to examine the scatterplot.\n\n\n11.5.5 Draw the Picture!\nWe’ve seen that Danielle, Earl and Fiona all show Pearson correlations of essentially zero. However, the three data sets look very different in a scatterplot.\n\nggplot(correx1, aes(x = x, y = y)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x) +\n    facet_wrap(~ set)\n\n\n\n\nWhen we learn that the correlation is zero, we tend to assume we have a picture like the Danielle data set. If Danielle were our real data, we might well think that x would be of little use in predicting y.\n\nBut what if our data looked like Earl? In the Earl data set, x is incredibly helpful in predicting y, but we can’t use a straight line model - instead, we need a non-linear modeling approach.\nYou’ll recall that the Fiona data set also had a Pearson correlation of zero. But here, the picture is rather more interesting.\n\nSo, remember, draw the appropriate scatterplot whenever you make use of a correlation coefficient.\n\nrm(setA, setB, setBC, tab1)"
  },
  {
    "objectID": "11-straightlinemodels.html#estimating-correlation-from-scatterplots",
    "href": "11-straightlinemodels.html#estimating-correlation-from-scatterplots",
    "title": "11  Straight Line Models",
    "section": "11.6 Estimating Correlation from Scatterplots",
    "text": "11.6 Estimating Correlation from Scatterplots\nThe correx2 data set is designed to help you calibrate yourself a bit in terms of estimating a correlation from a scatterplot. There are 11 data sets buried within the correx2 example, and they are labeled by their Pearson correlation coefficients, ranging from r = 0.01 to r = 0.999\n\ncorrex2 <- read_csv(\"data/correx2.csv\")\n\nRows: 582 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): set\ndbl (3): x, y, group\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncorrex2 |>\n    group_by(set) |>\n    summarise(cor = round(cor(x, y, use=\"complete\"),3))\n\n# A tibble: 11 × 2\n   set       cor\n   <chr>   <dbl>\n 1 Set 01  0.01 \n 2 Set 10  0.102\n 3 Set 20  0.202\n 4 Set 30  0.301\n 5 Set 40  0.403\n 6 Set 50  0.499\n 7 Set 60  0.603\n 8 Set 70  0.702\n 9 Set 80  0.799\n10 Set 90  0.902\n11 Set 999 0.999\n\n\nHere is a plot of the 11 data sets, showing the increase in correlation from 0.01 (in Set 01) to 0.999 (in Set 999).\n\nggplot(correx2, aes(x = x, y = y)) +\n    geom_point() + \n    facet_wrap(~ set) +\n    labs(title = \"Pearson Correlations from 0.01 to 0.999\")\n\n\n\n\nNote that R will allow you to fit a straight line model to any of these relationships, no matter how appropriate it might be to do so.\n\nggplot(correx2, aes(x = x, y = y)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", formula = y ~ x, col = \"red\") +\n    facet_wrap(~ set) +\n    labs(title = \"R will fit a straight line to anything.\")\n\n\n\n\n\nggplot(correx2, aes(x = x, y = y)) +\n    geom_point() + \n    geom_smooth(col = \"blue\") +\n    facet_wrap(~ set) +\n    labs(title = \"Even if a loess smooth suggests non-linearity.\")\n\n\n\n\n\nggplot(correx2, aes(x = x, y = y, color = factor(group))) +\n    geom_point() + \n    guides(color = \"none\") +\n    facet_wrap(~ set) +\n    labs(title = \"Note: The same 10 points (in red) are in each plot.\")\n\n\n\n\nNote that the same 10 points are used in each of the data sets. It’s always possible that a lurking subgroup of the data within a scatterplot follows a very strong linear relationship. This is why it’s so important (and difficult) not to go searching for such a thing without a strong foundation of logic, theory and prior empirical evidence."
  },
  {
    "objectID": "11-straightlinemodels.html#the-spearman-rank-correlation",
    "href": "11-straightlinemodels.html#the-spearman-rank-correlation",
    "title": "11  Straight Line Models",
    "section": "11.7 The Spearman Rank Correlation",
    "text": "11.7 The Spearman Rank Correlation\nThe Spearman rank correlation coefficient is a rank-based measure of statistical dependence that assesses how well the relationship between X and Y can be described using a monotone function even if that relationship is not linear.\n\nA monotone function preserves order, that is, Y must either be strictly increasing as X increases, or strictly decreasing as X increases.\nA Spearman correlation of 1.0 indicates simply that as X increases, Y always increases.\nLike the Pearson correlation, the Spearman correlation is dimension-free, and falls between -1 and +1.\nA positive Spearman correlation corresponds to an increasing (but not necessarily linear) association between X and Y, while a negative Spearman correlation corresponds to a decreasing (but again not necessarily linear) association.\n\n\n11.7.1 Spearman Formula\nTo calculate the Spearman rank correlation, we take the ranks of the X and Y data, and then apply the usual Pearson correlation. To find the ranks, sort X and Y into ascending order, and then number them from 1 (smallest) to n (largest). In the event of a tie, assign the average rank to the tied subjects.\n\n\n11.7.2 Comparing Pearson and Spearman Correlations\nLet’s look at the nnyfs data again.\n\nnnyfs |> select(fat, protein) |> cor()\n\n              fat   protein\nfat     1.0000000 0.6671209\nprotein 0.6671209 1.0000000\n\nnnyfs |> select(fat, protein) %>% cor(., method = \"spearman\")\n\n              fat   protein\nfat     1.0000000 0.6577489\nprotein 0.6577489 1.0000000\n\n\nThe Spearman and Pearson correlations are not especially different in this case.\n\n\n11.7.3 Spearman vs. Pearson Example 1\nThe next few plots describe relationships where we anticipate the Pearson and Spearman correlations might differ in their conclusions.\n\nspear1 <- read_csv(\"data/spear1.csv\")\n\nRows: 22 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nspear2 <- read_csv(\"data/spear2.csv\")\n\nRows: 90 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nspear3 <- read_csv(\"data/spear3.csv\")\n\nRows: 55 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nspear4 <- read_csv(\"data/spear4.csv\")\n\nRows: 15 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# these are just toy examples with\n# two columns per data set and no row numbering\n\nExample 1 shows a function where the Pearson correlation is 0.925 (a strong but not perfect linear relation), but the Spearman correlation is 1 because the relationship is monotone, even though it is not perfectly linear.\n\nggplot(spear1, aes(x = x, y = y)) + \n    geom_point() +\n    labs(title = \"Spearman vs. Pearson, Example 1\") +\n    annotate(\"text\", x = -10, y = 20, \n             label = paste(\"Pearson r = \", \n                 signif(cor(spear1$x, spear1$y),2),\n                 \", Spearman r = \", \n                 signif(cor(spear1$x, spear1$y, method = \"spearman\"),2)))\n\n\n\n\nSo, a positive Spearman correlation corresponds to an increasing (but not necessarily linear) association between x and y.\n\n\n11.7.4 Spearman vs. Pearson Example 2\nExample 2 shows that a negative Spearman correlation corresponds to a decreasing (but, again, not necessarily linear) association between x and y.\n\nggplot(spear2, aes(x = x, y = y)) + \n    geom_point(col = \"purple\") +\n    geom_smooth(method = \"loess\", formula = y ~ x, se = FALSE) +\n    labs(title = \"Spearman vs. Pearson, Example 2\") +\n    annotate(\"text\", x = 10, y = 20, col = \"purple\",\n             label = paste(\"Pearson r = \", \n                    signif(cor(spear2$x, spear2$y),2),\n                    \", Spearman r = \", \n                    signif(cor(spear2$x, spear2$y, method = \"spearman\"),2)))\n\n\n\n\n\n\n11.7.5 Spearman vs. Pearson Example 3\nThe Spearman correlation is less sensitive than the Pearson correlation is to strong outliers that are unusual on either the X or Y axis, or both. That is because the Spearman rank coefficient limits the outlier to the value of its rank.\nIn Example 3, for instance, the Spearman correlation reacts much less to the outliers around X = 12 than does the Pearson correlation.\n\nggplot(spear3, aes(x = x, y = y)) + \n    geom_point(col = \"blue\") +\n    labs(title = \"Spearman vs. Pearson, Example 3\") +\n    annotate(\"text\", x = 5, y = -15, col = \"blue\",\n             label = paste(\"Pearson r = \", \n                signif(cor(spear3$x, spear3$y),2),\n                \", Spearman r = \", \n                signif(cor(spear3$x, spear3$y, method = \"spearman\"),2)))\n\n\n\n\n\n\n11.7.6 Spearman vs. Pearson Example 4\nThe use of a Spearman correlation is no substitute for looking at the data. For non-monotone data like what we see in Example 4, neither the Spearman nor the Pearson correlation alone provides much guidance, and just because they are (essentially) telling you the same thing, that doesn’t mean what they’re telling you is all that helpful.\n\nggplot(spear4, aes(x = x, y = y)) +\n    geom_point(col = \"purple\") +\n    geom_smooth(method = \"loess\", formula = y ~ x, se = FALSE) +\n    labs(title = \"Spearman vs. Pearson, Example 4\") +\n    annotate(\"text\", x = 10, y = 20, col = \"purple\",\n             label = paste(\"Pearson r = \", \n                 signif(cor(spear4$x, spear4$y),2),\n                 \", Spearman r = \", \n                 signif(cor(spear4$x, spear4$y, method = \"spearman\"),2)))"
  },
  {
    "objectID": "12-linear_trans.html",
    "href": "12-linear_trans.html",
    "title": "12  Linearizing Transformations",
    "section": "",
    "text": "knitr::opts_chunk$set(comment = NA)\n\nlibrary(broom)\nlibrary(car)\nlibrary(patchwork)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "12-linear_trans.html#linearize-the-association-between-quantitative-variables",
    "href": "12-linear_trans.html#linearize-the-association-between-quantitative-variables",
    "title": "12  Linearizing Transformations",
    "section": "12.2 “Linearize” The Association between Quantitative Variables",
    "text": "12.2 “Linearize” The Association between Quantitative Variables\nConfronted with a scatterplot describing a monotone association between two quantitative variables, we may decide the data are not well approximated by a straight line, and thus, that a least squares regression may not be not sufficiently useful. In these circumstances, we have at least two options, which are not mutually exclusive:\n\nLet the data be as they may, and summarize the scatterplot using tools like loess curves, polynomial functions, or cubic splines to model the relationship.\nConsider re-expressing the data (often we start with re-expressions of the outcome data [the Y variable]) using a transformation so that the transformed data may be modeled effectively using a straight line."
  },
  {
    "objectID": "12-linear_trans.html#the-box-cox-plot",
    "href": "12-linear_trans.html#the-box-cox-plot",
    "title": "12  Linearizing Transformations",
    "section": "12.3 The Box-Cox Plot",
    "text": "12.3 The Box-Cox Plot\nAs before, Tukey’s ladder of power transformations can guide our exploration.\n\n\n\n\n\n\n\n\n\n\n\n\n\nPower (\\(\\lambda\\))\n-2\n-1\n-1/2\n0\n1/2\n1\n2\n\n\n\n\nTransformation\n1/y2\n1/y\n1/\\(\\sqrt{y}\\)\nlog y\n\\(\\sqrt{y}\\)\ny\ny2\n\n\n\nThe Box-Cox plot, from the boxCox function in the car package, sifts through the ladder of options to suggest a transformation (for Y) to best linearize the outcome-predictor(s) relationship.\n\n12.3.1 A Few Caveats\n\nThese methods work well with monotone data, where a smooth function of Y is either strictly increasing, or strictly decreasing, as X increases.\nSome of these transformations require the data to be positive. We can rescale the Y data by adding a constant to every observation in a data set without changing shape.\nWe can use a natural logarithm (log in R), a base 10 logarithm (log10) or even sometimes a base 2 logarithm (log2) to good effect in Tukey’s ladder. All affect the association’s shape in the same way, so we’ll stick with log (base e).\nSome re-expressions don’t lead to easily interpretable results. Not many things that make sense in their original units also make sense in inverse square roots. There are times when we won’t care, but often, we will.\nIf our primary interest is in making predictions, we’ll generally be more interested in getting good predictions back on the original scale, and we can back-transform the point and interval estimates to accomplish this."
  },
  {
    "objectID": "12-linear_trans.html#a-simulated-example",
    "href": "12-linear_trans.html#a-simulated-example",
    "title": "12  Linearizing Transformations",
    "section": "12.4 A Simulated Example",
    "text": "12.4 A Simulated Example\n\nset.seed(999); \n  x.rand <- rbeta(80, 2, 5) * 20 + 3\nset.seed(1000); \n  y.rand <- abs(50 + 0.75*x.rand^(3) \n                - 0.65*x.rand + rnorm(80, 0, 200))\n\nscatter1 <- tibble(x = x.rand, y = y.rand) \nrm(x.rand, y.rand)\n\nggplot(scatter1, aes(x = x, y = y)) +\n    geom_point(shape = 1, size = 3) +\n    ## add loess smooth\n    geom_smooth(method = \"loess\", se = FALSE, \n                col = \"dodgerblue\", formula = y ~ x) +\n    ## then add linear fit\n    geom_smooth(method = \"lm\", se = FALSE, \n                col = \"red\", formula = y ~ x, linetype = \"dashed\") +\n    labs(title = \"Simulated scatter1 example: Y vs. X\")\n\n\n\n\n\n\n\n\nHaving simulated data that produces a curved scatterplot, I will now use the Box-Cox plot to lead my choice of an appropriate power transformation for Y in order to “linearize” the association of Y and X.\n\nboxCox(scatter1$y ~ scatter1$x) \n\n\n\npowerTransform(scatter1$y ~ scatter1$x)\n\nEstimated transformation parameter \n       Y1 \n0.4368753 \n\n\nThe Box-Cox plot peaks at the value \\(\\lambda\\) = 0.44, which is pretty close to \\(\\lambda\\) = 0.5. Now, 0.44 isn’t on Tukey’s ladder, but 0.5 is.\n\n\n\n\n\n\n\n\n\n\n\n\n\nPower (\\(\\lambda\\))\n-2\n-1\n-1/2\n0\n1/2\n1\n2\n\n\n\n\nTransformation\n1/y2\n1/y\n1/\\(\\sqrt{y}\\)\nlog y\n\\(\\sqrt{y}\\)\ny\ny2\n\n\n\nIf we use \\(\\lambda\\) = 0.5, on Tukey’s ladder of power transformations, it suggests we look at the relationship between the square root of Y and X, as shown next.\n\np1 <- ggplot(scatter1, aes(x = x, y = y)) +\n    geom_point(size = 2) +\n    geom_smooth(method = \"loess\", se = FALSE, \n                formula = y ~ x, col = \"dodgerblue\") +\n    geom_smooth(method = \"lm\", se = FALSE, \n                formula = y ~ x, col = \"red\", linetype = \"dashed\") +\n    labs(title = \"scatter1: Y vs. X\")\n\np2 <- ggplot(scatter1, aes(x = x, y = sqrt(y))) +\n    geom_point(size = 2) +\n    geom_smooth(method = \"loess\", se = FALSE, \n                formula = y ~ x, col = \"dodgerblue\") +\n    geom_smooth(method = \"lm\", se = FALSE, \n                formula = y ~ x, col = \"red\", linetype = \"dashed\") +\n    labs(title = \"scatter1: Square Root of Y vs. X\")\n\np1 + p2\n\n\n\n\nBy eye, I think the square root plot better matches the linear fit."
  },
  {
    "objectID": "12-linear_trans.html#checking-on-a-transformation-or-re-expression",
    "href": "12-linear_trans.html#checking-on-a-transformation-or-re-expression",
    "title": "12  Linearizing Transformations",
    "section": "12.5 Checking on a Transformation or Re-Expression",
    "text": "12.5 Checking on a Transformation or Re-Expression\nWe can do three more things to check on our transformation.\n\nWe can calculate the correlation of our original and re-expressed associations.\nWe can use the testTransform function in the car library in R to perform a statistical test comparing the optimal choice of power (\\(\\lambda\\) = 0.44) to various other transformations.\nWe can go ahead and fit the regression models using each approach and compare the plots of studentized residuals against fitted values from the data to see if the re-expression reduces the curve in that residual plot, as well.\n\nOption 3 is by far the most important in practice, and it’s the one we’ll focus on going forward, but we’ll demonstrate all three here.\n\n12.5.1 Checking the Correlation Coefficients\nHere, we calculate the correlation of original and re-expressed associations.\n\ncor(scatter1$y, scatter1$x)\n\n[1] 0.891198\n\ncor(sqrt(scatter1$y), scatter1$x)\n\n[1] 0.9144307\n\n\nThe Pearson correlation is a little stronger after the transformation. as we’d expect.\n\n\n12.5.2 Using the testTransform function\nHere, we use the testTransform function (also from the car package) to compare the optimal choice determined by the powerTransform function (here \\(\\lambda\\) = 0.44) to \\(\\lambda\\) = 0 (logarithm), 0.5 (square root) and 1 (no transformation).\n\ntestTransform(powerTransform(scatter1$y ~ scatter1$x), 0)\n\n                           LRT df      pval\nLR test, lambda = (0) 46.17947  1 1.079e-11\n\ntestTransform(powerTransform(scatter1$y ~ scatter1$x), 0.5)\n\n                             LRT df    pval\nLR test, lambda = (0.5) 1.024888  1 0.31136\n\ntestTransform(powerTransform(scatter1$y ~ scatter1$x), 1)\n\n                           LRT df       pval\nLR test, lambda = (1) 63.75953  1 1.4433e-15\n\n\n\nIt looks like only the square root (\\(\\lambda\\) = 0.5) of these three options is not significantly worse by the log-likelihood criterion applied here than the optimal choice.\nThat’s because it’s the only one with a p value larger than our usual standard for statistical significance, of 0.05.\n\n\n\n12.5.3 Comparing the Residual Plots\nWe can fit the regression models, obtain plots of residuals against fitted values, and compare them to see which one has less indication of a curve in the residuals.\n\nmodel.orig <- lm(scatter1$y ~ scatter1$x)\nmodel.sqrt <- lm(sqrt(scatter1$y) ~ scatter1$x)\n\np1 <- augment(model.orig) %>%\n    ggplot(., aes(x = scatter1$x, y = .resid)) + \n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x, se = FALSE) +\n    geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE, col = \"red\") +\n    labs(title = \"Y vs X Residual Plot\")\n\np2 <- augment(model.sqrt) %>%\n    ggplot(., aes(x = scatter1$x, y = .resid)) + \n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x, se = FALSE) +\n    geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE, col = \"red\") +\n    labs(title = \"Square Root Model Residuals\")\n\np1 + p2\n\n\n\n\nWhat we’re looking for in such a plot is the absence of a curve, among other things, we want to see “fuzzy football” shapes.\nAs compared to the original residual plot, the square root version, is a modest improvement in this regard. It does look a bit less curved, and a bit more like a random cluster of points, so that’s nice. Usually, we can do a little better in real data, as shown in the next example from the NNYFS data we introduced in Chapter @ref(NYFS-Study)."
  },
  {
    "objectID": "12-linear_trans.html#an-example-from-the-nnyfs-data",
    "href": "12-linear_trans.html#an-example-from-the-nnyfs-data",
    "title": "12  Linearizing Transformations",
    "section": "12.6 An Example from the NNYFS data",
    "text": "12.6 An Example from the NNYFS data\n\nnnyfs <- read_rds(\"data/nnyfs.Rds\")\n\nUsing the subjects in the nnyfs data with complete data on the two variables of interest, let’s look at the relationship between arm circumference (the outcome, shown on the Y axis) and arm length (the predictor, shown on the X axis.)\n\nnnyfs_c <- nnyfs |> \n    filter(complete.cases(arm_circ, arm_length)) |>\n    select(SEQN, arm_circ, arm_length)\n\n\n12.6.1 Pearson correlation and scatterplot\nHere is the Pearson correlation between these two variables.\n\nnnyfs_c |> select(arm_length, arm_circ) |> cor()\n\n           arm_length  arm_circ\narm_length  1.0000000 0.8120242\narm_circ    0.8120242 1.0000000\n\n\nHere’s the resulting scatterplot.\n\nggplot(nnyfs_c, aes(x = arm_length, y = arm_circ)) +\n    geom_point(alpha = 0.2) +\n    geom_smooth(method = \"loess\", formula = y ~ x, \n                se = FALSE, color = \"blue\") +\n    geom_smooth(method = \"lm\", formula = y ~ x, \n                se = FALSE, color = \"red\")\n\n\n\n\nWhile the Pearson correlation is still quite strong, note that the loess smooth (shown in blue) bends up from the straight line model (shown in red) at both the low and high end of arm length.\nNote also the use of alpha = 0.2 to show the points with greater transparency than they would be shown normally (the default setting is no transparency with alpha = 1.)\n\n\n12.6.2 Plotting the Residuals\nNow, let’s build a plot of residuals from the straight line model plotted against the arm length. We can obtain these residuals using the augment() function from the broom package.\n\nm1 <- lm(arm_circ ~ arm_length, data = nnyfs_c)\n\nnnyfs_c_aug1 <- augment(m1, data = nnyfs_c)\n\nnnyfs_c_aug1\n\n# A tibble: 1,511 × 9\n    SEQN arm_circ arm_length .fitted .resid     .hat .sigma   .cooksd .std.resid\n   <dbl>    <dbl>      <dbl>   <dbl>  <dbl>    <dbl>  <dbl>     <dbl>      <dbl>\n 1 71918     25.4       27.7    21.9  3.51  0.000695   3.21 0.000416       1.09 \n 2 71919     26         38.4    30.4 -4.38  0.00253    3.21 0.00237       -1.37 \n 3 71920     37.9       35.9    28.4  9.50  0.00167    3.20 0.00735        2.96 \n 4 71921     15.1       18.3    14.4  0.669 0.00304    3.21 0.0000663      0.209\n 5 71922     29.5       34.2    27.0  2.45  0.00124    3.21 0.000362       0.764\n 6 71923     27.9       33      26.1  1.80  0.00100    3.21 0.000159       0.562\n 7 71924     17.6       26.5    20.9 -3.34  0.000788   3.21 0.000427      -1.04 \n 8 71925     17.7       24.2    19.1 -1.41  0.00113    3.21 0.000110      -0.441\n 9 71926     19.9       26      20.5 -0.642 0.000844   3.21 0.0000169     -0.200\n10 71927     17.3       20      15.8  1.52  0.00234    3.21 0.000263       0.474\n# … with 1,501 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nOK. So the residuals are now stored in the .resid variable. We can create a residual plot, as follows.\n\nggplot(nnyfs_c_aug1, aes(x = arm_length, y = .resid)) +\n    geom_point(alpha = 0.2) +\n    geom_smooth(method = \"loess\", col = \"purple\",\n                formula = y ~ x, se = FALSE) +\n    labs(title = \"Residuals show a curve.\")\n\n\n\n\n\n\n12.6.3 Using the Box-Cox approach to identify a transformation\n\nlibrary(car)\nboxCox(nnyfs_c$arm_circ ~ nnyfs_c$arm_length) \n\n\n\npowerTransform(nnyfs_c$arm_circ ~ nnyfs_c$arm_length)\n\nEstimated transformation parameter \n        Y1 \n-0.9783135 \n\n\nThis suggests that we should transform the arm_circ data by taking its inverse (power = -1.) Let’s take a look at that result.\n\n\n12.6.4 Plots after Inverse Transformation\nLet’s build (on the left) the revised scatterplot and (on the right) the revised residual plot after transforming the outcome (arm_circ) by taking its inverse.\n\nnnyfs_c <- nnyfs_c |>\n    mutate(inv_arm_circ = 1/arm_circ)\n\np1 <- ggplot(nnyfs_c, aes(x = arm_length, y = inv_arm_circ)) +\n    geom_point(alpha = 0.2) +\n    geom_smooth(method = \"loess\", formula = y ~ x, \n                se = FALSE, color = \"blue\") +\n    geom_smooth(method = \"lm\", formula = y ~ x, \n                se = FALSE, color = \"red\") +\n    labs(title = \"Transformation reduces curve\")\n\nm2 <- lm(inv_arm_circ ~ arm_length, data = nnyfs_c)\n\nnnyfs_c_aug2 <- augment(m2, data = nnyfs_c)\n\np2 <- ggplot(nnyfs_c_aug2, aes(x = arm_length, y = .resid)) +\n    geom_point(alpha = 0.2) +\n    geom_smooth(method = \"loess\", col = \"purple\",\n                formula = y ~ x, se = FALSE) +\n    labs(title = \"Residuals much improved\")\n\np1 + p2 + \n    plot_annotation(title = \"Evaluating the Inverse Transformation\")"
  },
  {
    "objectID": "13-crabs.html",
    "href": "13-crabs.html",
    "title": "13  Studying Crab Claws",
    "section": "",
    "text": "For our next example, we’ll consider a study from zoology, specifically carcinology - the study of crustaceans. My source for these data is Chapter 7 in Ramsey and Schafer (2002) which drew the data from a figure in Yamada and Boulding (1998)."
  },
  {
    "objectID": "13-crabs.html#setup-packages-used-here",
    "href": "13-crabs.html#setup-packages-used-here",
    "title": "13  Studying Crab Claws",
    "section": "13.1 Setup: Packages Used Here",
    "text": "13.1 Setup: Packages Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor)\nlibrary(broom)\nlibrary(knitr)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\n\nWe will also use the describe function from the psych package."
  },
  {
    "objectID": "13-crabs.html#the-data",
    "href": "13-crabs.html#the-data",
    "title": "13  Studying Crab Claws",
    "section": "13.2 The Data",
    "text": "13.2 The Data\nThe available data are the mean closing forces (in Newtons) and the propodus heights (mm) of the claws on 38 crabs that came from three different species. The propodus is the segment of the crab’s clawed leg with an immovable finger and palm.\n\n\n\n\n\nSource: http://txmarspecies.tamug.edu/crustglossary.cfm\n\n\n\n\nThis was part of a study of the effects that predatory intertidal crab species have on populations of snails. The three crab species under study are:\n\n14 Hemigraspus nudus, also called the purple shore crab (14 crabs)\n12 Lophopanopeus bellus, also called the black-clawed pebble crab, and\n12 Cancer productus, one of several species of red rock crabs (12)\n\n\ncrabs <- read_csv(\"data/crabs.csv\") \n\nRows: 38 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): species\ndbl (3): crab, force, height\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncrabs\n\n# A tibble: 38 × 4\n    crab species              force height\n   <dbl> <chr>                <dbl>  <dbl>\n 1     1 Hemigraspus nudus      4      8  \n 2     2 Lophopanopeus bellus  15.1    7.9\n 3     3 Cancer productus       5      6.7\n 4     4 Lophopanopeus bellus   2.9    6.6\n 5     5 Hemigraspus nudus      3.2    5  \n 6     6 Hemigraspus nudus      9.5    7.9\n 7     7 Cancer productus      22.5    9.4\n 8     8 Hemigraspus nudus      7.4    8.3\n 9     9 Cancer productus      14.6   11.2\n10    10 Lophopanopeus bellus   8.7    8.6\n# … with 28 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThe species information is stored here as a character variable. How many different crabs are we talking about in each species?\n\ncrabs |> tabyl(species)\n\n              species  n   percent\n     Cancer productus 12 0.3157895\n    Hemigraspus nudus 14 0.3684211\n Lophopanopeus bellus 12 0.3157895\n\n\nAs it turns out, we’re going to want to treat the species information as a factor with three levels, rather than as a character variable.\n\ncrabs <- crabs |>\n    mutate(species = factor(species))\n\nHere’s a quick summary of the data. Take care to note the useless results for the first two variables. At least the function flags with a * those variables it thinks are non-numeric.\n\npsych::describe(crabs)\n\n         vars  n  mean    sd median trimmed   mad min  max range skew kurtosis\ncrab        1 38 19.50 11.11  19.50   19.50 14.08   1 38.0  37.0 0.00    -1.30\nspecies*    2 38  2.00  0.81   2.00    2.00  1.48   1  3.0   2.0 0.00    -1.50\nforce       3 38 12.13  8.98   8.70   11.53  9.04   2 29.4  27.4 0.47    -1.25\nheight      4 38  8.81  2.23   8.25    8.78  2.52   5 13.1   8.1 0.19    -1.14\n           se\ncrab     1.80\nspecies* 0.13\nforce    1.46\nheight   0.36\n\n\nActually, we’re more interested in these results after grouping by species.\n\ncrabs |>\n    group_by(species) |>\n    summarise(n = n(), median(force), median(height))\n\n# A tibble: 3 × 4\n  species                  n `median(force)` `median(height)`\n  <fct>                <int>           <dbl>            <dbl>\n1 Cancer productus        12            19.7            11.0 \n2 Hemigraspus nudus       14             3.7             7.9 \n3 Lophopanopeus bellus    12            14.8             8.15"
  },
  {
    "objectID": "13-crabs.html#association-of-size-and-force",
    "href": "13-crabs.html#association-of-size-and-force",
    "title": "13  Studying Crab Claws",
    "section": "13.3 Association of Size and Force",
    "text": "13.3 Association of Size and Force\nSuppose we want to describe force on the basis of height, across all 38 crabs. We’ll add titles and identify the three species of crab, using shape and color.\n\nggplot(crabs, aes(x = height, y = force, color = species, shape = species)) +\n    geom_point(size = 3) +\n    labs(title = \"Crab Claw Force by Size\", \n         x = \"Claw's propodus height (mm)\", y = \"Mean closing force (N)\") +\n    theme_bw()\n\n\n\n\nA faceted plot for each species really highlights the difference in force between the Hemigraspus nudus and the other two species of crab.\n\nggplot(crabs, aes(x = height, y = force, color = species)) +\n    geom_point(size = 3) +\n    facet_wrap(~ species) +\n    guides(color = \"none\") +\n    labs(title = \"Crab Claw Force by Size\", \n         x = \"Claw's propodus height (mm)\", y = \"Mean closing force (N)\") +\n    theme_bw()"
  },
  {
    "objectID": "13-crabs.html#loess_smooth",
    "href": "13-crabs.html#loess_smooth",
    "title": "13  Studying Crab Claws",
    "section": "13.4 The loess smooth",
    "text": "13.4 The loess smooth\nWe can obtain a smoothed curve (using several different approaches) to summarize the pattern presented by the data in any scatterplot. For instance, we might build such a plot for the complete set of 38 crabs, adding in a non-linear smooth function (called a loess smooth.)\n\nggplot(crabs, aes(x = height, y = force)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", se = FALSE, formula = y ~ x) +\n    labs(title = \"Crab Claw Force by Size\", \n         x = \"Claw's propodus height (mm)\", y = \"Mean closing force (N)\")\n\n\n\n\nAs we have discussed previously, a loess smooth fits a curve to data by tracking (at point x) the points within a neighborhood of point x, with more emphasis given to points near x. It can be adjusted by tweaking the span and degree parameters.\nIn addition to the curve, smoothing procedures can also provide confidence intervals around their main fitted line. Consider the following plot of the crabs information, which adjusts the span (from its default of 0.75) and also adds in the confidence intervals.\n\nggplot(crabs, aes(x = height, y = force)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x, span = 0.5, se = TRUE) +\n    labs(title = \"Crab Claw Force by Size\", \n         x = \"Claw's propodus height (mm)\", y = \"Mean closing force (N)\")\n\n\n\n\nBy reducing the size of the span, our resulting picture shows a much less smooth function that we generated previously.\n\n13.4.1 Smoothing within Species\nWe can, of course, produce the plot above with separate smooths for each of the three species of crab.\n\nggplot(crabs, aes(x = height, y = force, group = species, color = species)) +\n    geom_point(size = 3) +\n    geom_smooth(method = \"loess\", formula = y ~ x, se = FALSE) +\n    labs(title = \"Crab Claw Force by Size\", \n         x = \"Claw's propodus height (mm)\", y = \"Mean closing force (N)\")\n\n\n\n\nIf we want to add in the confidence intervals (here I’ll show them at 90% rather than the default of 95%) then this plot should be faceted. Note that by default, what is displayed when se = TRUE are 95% prediction intervals - the level function in stat_smooth [which can be used in place of geom_smooth] is used here to change the coverage percentage from 95% to 90%.\n\nggplot(crabs, aes(x = height, y = force, group = species, color = species)) +\n    geom_point() +\n    stat_smooth(method = \"loess\", formula = y ~ x, level = 0.90, se = TRUE) +\n    guides(color = \"none\") +\n    labs(title = \"Crab Claw Force by Size\", \n         caption = \"with loess smooths and 90% confidence intervals\",\n         x = \"Claw's propodus height (mm)\", y = \"Mean closing force (N)\") +\n    facet_wrap(~ species)\n\n\n\n\nMore on these and other confidence intervals later, especially in part B."
  },
  {
    "objectID": "13-crabs.html#fitting-a-linear-regression-model",
    "href": "13-crabs.html#fitting-a-linear-regression-model",
    "title": "13  Studying Crab Claws",
    "section": "13.5 Fitting a Linear Regression Model",
    "text": "13.5 Fitting a Linear Regression Model\nSuppose we plan to use a simple (least squares) linear regression model to describe force as a function of height. Is a least squares model likely to be an effective choice here?\nThe plot below shows the regression line predicting closing force as a function of propodus height. Here we annotate the plot to show the actual fitted regression line, which required fitting it with the lm statement prior to developing the graph.\n\nmod <- lm(force ~ height, data = crabs)\n\nggplot(crabs, aes(x = height, y = force)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x,  color = \"red\") +\n    labs(title = \"Crab Claw Force by Size with Linear Regression Model\", \n         x = \"Claw's propodus height (mm)\", y = \"Mean closing force (N)\") +\n    annotate(\"text\", x = 11, y = 0, color = \"red\", fontface = \"italic\",\n             label = paste( \"Force - \", signif(coef(mod)[1],3), \" + \", \n                            signif(coef(mod)[2],3), \" Height\" ))\n\n\n\nrm(mod)\n\nThe lm function, again, specifies the linear model we fit to predict force using height. Here’s the summary.\n\nsummary(lm(force ~ height, data = crabs))\n\n\nCall:\nlm(formula = force ~ height, data = crabs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.7945  -3.8113  -0.2394   4.1444  16.8814 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.0869     4.6224  -2.399   0.0218 *  \nheight        2.6348     0.5089   5.177 8.73e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.892 on 36 degrees of freedom\nMultiple R-squared:  0.4268,    Adjusted R-squared:  0.4109 \nF-statistic:  26.8 on 1 and 36 DF,  p-value: 8.73e-06\n\n\nAgain, the key things to realize are:\n\nThe outcome variable in this model is force, and the predictor variable is height.\nThe straight line model for these data fitted by least squares is force = -11.1 + 2.63 height.\nThe slope of height is positive, which indicates that as height increases, we expect that force will also increase. Specifically, we expect that for every additional mm of height, the force will increase by 2.63 Newtons.\nThe multiple R-squared (squared correlation coefficient) is 0.427, which implies that 42.7% of the variation in force is explained using this linear model with height. It also implies that the Pearson correlation between force and height is the square root of 0.427, or 0.653."
  },
  {
    "objectID": "13-crabs.html#is-a-linear-model-appropriate",
    "href": "13-crabs.html#is-a-linear-model-appropriate",
    "title": "13  Studying Crab Claws",
    "section": "13.6 Is a Linear Model Appropriate?",
    "text": "13.6 Is a Linear Model Appropriate?\nThe zoology (at least as described in Ramsey and Schafer (2002)) suggests that the actual nature of the relationship would be represented by a log-log relationship, where the log of force is predicted by the log of height.\nThis log-log model is an appropriate model when we think that percentage increases in X (height, here) lead to constant percentage increases in Y (here, force).\nTo see the log-log model in action, we plot the log of force against the log of height. We could use either base 10 (log10 in R) or natural (log in R) logarithms.\n\nggplot(crabs, aes(x = log(height), y = log(force))) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x) + \n    labs(title = \"Log-Log Model for Crabs data\")\n\n\n\n\nThe correlations between the raw force and height and between their logarithms turn out to be quite similar, and because the log transformation is monotone in these data, there’s actually no change at all in the Spearman correlations.\n\n\n\nCorrelation of\nPearson r\nSpearman r\n\n\n\n\nforce and height\n0.653\n0.657\n\n\nlog(force) and log(height)\n0.662\n0.657\n\n\n\n\n13.6.1 The log-log model\n\ncrab_loglog <- lm(log(force) ~ log(height), data = crabs)\n\nsummary(crab_loglog)\n\n\nCall:\nlm(formula = log(force) ~ log(height), data = crabs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5657 -0.4450  0.1884  0.4798  1.2422 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -2.7104     0.9251  -2.930  0.00585 ** \nlog(height)   2.2711     0.4284   5.302 5.96e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6748 on 36 degrees of freedom\nMultiple R-squared:  0.4384,    Adjusted R-squared:  0.4228 \nF-statistic: 28.11 on 1 and 36 DF,  p-value: 5.96e-06\n\n\nOur regression equation is log(force) = -2.71 + 2.27 log(height).\nSo, for example, if we found a crab with propodus height = 10 mm, our prediction for that crab’s claw force (in Newtons) based on this log-log model would be…\n\nlog(force) = -2.71 + 2.27 log(10)\nlog(force) = -2.71 + 2.27 x 2.3025851\nlog(force) = 2.5190953\nand so predicted force = exp(2.5190953) = 12.4173582 Newtons, which, naturally, we would round to 12.4 Newtons to match the data set’s level of precision.\n\n\n\n13.6.2 How does this compare to our original linear model?\n\ncrab_linear <- lm(force ~ height, data = crabs)\n\nsummary(crab_linear)\n\n\nCall:\nlm(formula = force ~ height, data = crabs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.7945  -3.8113  -0.2394   4.1444  16.8814 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.0869     4.6224  -2.399   0.0218 *  \nheight        2.6348     0.5089   5.177 8.73e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.892 on 36 degrees of freedom\nMultiple R-squared:  0.4268,    Adjusted R-squared:  0.4109 \nF-statistic:  26.8 on 1 and 36 DF,  p-value: 8.73e-06\n\n\nThe linear regression equation is force = -11.1 + 2.63 height.\nSo, for example, if we found a crab with propodus height = 10 mm, our prediction for that crab’s claw force (in Newtons) based on this linear model would be…\n\nforce = -11.0869025 + 2.6348232 x 10\nforce = -11.0869025 + 26.3482321\nso predicted force = 15.2613297, which we would round to 15.3 Newtons.\n\nSo, it looks like the two models give meaningfully different predictions."
  },
  {
    "objectID": "13-crabs.html#making-predictions-with-a-model",
    "href": "13-crabs.html#making-predictions-with-a-model",
    "title": "13  Studying Crab Claws",
    "section": "13.7 Making Predictions with a Model",
    "text": "13.7 Making Predictions with a Model\nThe broom package’s augment function provides us with a consistent method for obtaining predictions (also called fitted values) for a new crab or for our original data. Suppose we want to predict the force level for two new crabs: one with height = 10 mm, and another with height = 12 mm.\n\nnewcrab <- tibble(crab = c(\"Crab_A\", \"Crab_B\"), height = c(10, 12))\n\naugment(crab_linear, newdata = newcrab)\n\n# A tibble: 2 × 3\n  crab   height .fitted\n  <chr>   <dbl>   <dbl>\n1 Crab_A     10    15.3\n2 Crab_B     12    20.5\n\n\nShould we want to obtain a prediction interval, we can use the predict function:\n\npredict(crab_linear, newdata = newcrab, interval = \"prediction\", level = 0.95)\n\n       fit      lwr      upr\n1 15.26133 1.048691 29.47397\n2 20.53098 5.994208 35.06774\n\n\nWe’d interpret this result as saying that the linear model’s predicted force associated with a single new crab claw with propodus height 10 mm is 15.3 Newtons, and that a 95% prediction interval for the true value of such a force for such a claw is between 1.0 and 29.5 Newtons. More on prediction intervals later.\n\n13.7.1 Predictions After a Transformation\nWe can also get predictions from the log-log model. The default choice is a 95% prediction interval.\n\npredict(crab_loglog, newdata = newcrab, interval = \"prediction\")\n\n       fit      lwr      upr\n1 2.519095 1.125900 3.912291\n2 2.933174 1.515548 4.350800\n\n\nOf course, these predictions describe the log(force) for such a crab claw. To get the prediction in terms of simple force, we’d need to back out of the logarithm, by exponentiating our point estimate and the prediction interval endpoints.\n\nexp(predict(crab_loglog, newdata = newcrab, interval = \"prediction\"))\n\n       fit      lwr      upr\n1 12.41736 3.082989 50.01341\n2 18.78716 4.551916 77.54044\n\n\nWe’d interpret this result as saying, for the first new crab, that the log-log model’s predicted force associated with a single new crab claw with propodus height 10 mm is 12.4 Newtons, and that a 95% prediction interval for the true value of such a force for such a claw is between 3.1 and 50.0 Newtons.\n\n\n13.7.2 Comparing Model Predictions\nSuppose we wish to build a plot of force vs height with a straight line for the linear model’s predictions, and a new curve for the log-log model’s predictions, so that we can compare and contrast the implications of the two models on a common scale. The predict function, when not given a new data frame, will use the existing predictor values that are in our crabs data. Such predictions are often called fitted values.\nTo put the two sets of predictions on the same scale despite the differing outcomes in the two models, we’ll exponentiate the results of the log-log model, and build a little data frame containing the heights and the predicted forces from that model.\n\nloglogdat <- tibble(height = crabs$height, force = exp(predict(crab_loglog)))\n\nA cleaner way to do this might be to use the augment function directly from broom:\n\naugment(crab_loglog)\n\n# A tibble: 38 × 7\n   `log(force)` `log(height)` .fitted   .hat .sigma  .cooksd .std.resid\n          <dbl>         <dbl>   <dbl>  <dbl>  <dbl>    <dbl>      <dbl>\n 1         1.39          2.08   2.01  0.0280  0.676 1.28e- 2  -0.941   \n 2         2.71          2.07   1.98  0.0287  0.673 1.79e- 2   1.10    \n 3         1.61          1.90   1.61  0.0499  0.684 8.06e-10  -0.000175\n 4         1.06          1.89   1.58  0.0530  0.679 1.69e- 2  -0.778   \n 5         1.16          1.61   0.945 0.142   0.683 1.01e- 2   0.349   \n 6         2.25          2.07   1.98  0.0287  0.683 2.39e- 3   0.402   \n 7         3.11          2.24   2.38  0.0301  0.673 1.90e- 2   1.11    \n 8         2.00          2.12   2.10  0.0266  0.684 2.75e- 4  -0.142   \n 9         2.68          2.42   2.78  0.0561  0.684 6.30e- 4  -0.146   \n10         2.16          2.15   2.18  0.0263  0.684 5.34e- 6  -0.0199  \n# … with 28 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nNow, we’re ready to use the geom_smooth approach to plot the linear fit, and geom_line (which also fits curves) to display the log-log fit.\n\nggplot(crabs, aes(x = height, y = force)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, \n                formula = y ~ x, col=\"blue\", linetype = 2) +\n    geom_line(data = loglogdat, col = \"red\", linetype = 2, size = 1) +\n    annotate(\"text\", 7, 12, label = \"Linear Model\", col = \"blue\") +\n    annotate(\"text\", 10, 8, label = \"Log-Log Model\", col = \"red\") +\n    labs(title = \"Comparing the Linear and Log-Log Models for Crab Claw data\")\n\n\n\n\nBased on these 38 crabs, we see some modest differences between the predictions of the two models, with the log-log model predicting generally lower closing force for a given propodus height than would be predicted by a linear model.\n\n\n\n\nRamsey, Fred L., and Daniel W. Schafer. 2002. The Statistical Sleuth: A Course in Methods of Data Analysis. Second. Pacific Grove, CA: Duxbury.\n\n\nYamada, SB, and EG Boulding. 1998. “Claw Morphology, Prey Size Selection and Foraging Efficiency in Generalist and Specialist Shell-Breaking Crabs.” Journal of Experimental Marine Biology and Ecology 220: 191–211. http://www.science.oregonstate.edu/~yamadas/SylviaCV/BehrensYamada_Boulding1998.pdf."
  },
  {
    "objectID": "14-hydrate.html",
    "href": "14-hydrate.html",
    "title": "14  Dehydration Recovery",
    "section": "",
    "text": "knitr::opts_chunk$set(comment = NA)\n\nlibrary(knitr)\nlibrary(broom)\nlibrary(patchwork)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\n\nWe will also use the ggpairs function from the GGally package, and the favstats function from the mosaic package."
  },
  {
    "objectID": "14-hydrate.html#the-data",
    "href": "14-hydrate.html#the-data",
    "title": "14  Dehydration Recovery",
    "section": "14.2 The Data",
    "text": "14.2 The Data\nThe hydrate data describe the degree of recovery that takes place 90 minutes following treatment of moderate to severe dehydration, for 36 children diagnosed at a hospital’s main pediatric clinic.\nUpon diagnosis and study entry, patients were treated with an electrolytic solution at one of seven dose levels (0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0 mEq/l) in a frozen, flavored, ice popsicle. The degree of rehydration was determined using a subjective scale based on physical examination and parental input, converted to a 0 to 100 point scale, representing the percent of recovery (recov.score). Each child’s age (in years) and weight (in pounds) are also available.\nFirst, we’ll check ranges (and for missing data) in the hydrate file.\n\nhydrate <- read_csv(\"data/hydrate.csv\")\n\nsummary(hydrate)\n\n       id         recov.score          dose            age        \n Min.   : 1.00   Min.   : 44.00   Min.   :0.000   Min.   : 3.000  \n 1st Qu.: 9.75   1st Qu.: 61.50   1st Qu.:1.000   1st Qu.: 5.000  \n Median :18.50   Median : 71.50   Median :1.500   Median : 6.500  \n Mean   :18.50   Mean   : 71.56   Mean   :1.569   Mean   : 6.667  \n 3rd Qu.:27.25   3rd Qu.: 80.00   3rd Qu.:2.500   3rd Qu.: 8.000  \n Max.   :36.00   Max.   :100.00   Max.   :3.000   Max.   :11.000  \n     weight     \n Min.   :22.00  \n 1st Qu.:34.50  \n Median :47.50  \n Mean   :46.89  \n 3rd Qu.:57.25  \n Max.   :76.00  \n\n\nThere are no missing values, and all of the ranges make sense. There are no especially egregious problems to report."
  },
  {
    "objectID": "14-hydrate.html#a-scatterplot-matrix",
    "href": "14-hydrate.html#a-scatterplot-matrix",
    "title": "14  Dehydration Recovery",
    "section": "14.3 A Scatterplot Matrix",
    "text": "14.3 A Scatterplot Matrix\nNext, we’ll use a scatterplot matrix to summarize relationships between the outcome recov.score and the key predictor dose as well as the ancillary predictors age and weight, which are of less interest, but are expected to be related to our outcome. The one below uses the ggpairs function in the GGally package, as introduced in Part A of the Notes. We place the outcome in the bottom row, and the key predictor immediately above it, with age and weight in the top rows, using the select function within the `ggpairs call.\n\nGGally::ggpairs(dplyr::select(hydrate, age, weight, dose, recov.score), \n                title = \"Scatterplot Matrix for hydrate data\")\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\n\n\n\nWhat can we conclude here?\n\nIt looks like recov.score has a moderately strong negative relationship with both age and weight (with correlations in each case around -0.5), but a positive relationship with dose (correlation = 0.36).\nThe distribution of recov.score looks to be pretty close to Normal. No potential predictors (age, weight and dose) show substantial non-Normality.\nage and weight, as we’d expect, show a very strong and positive linear relationship, with r = 0.94\nNeither age nor weight shows a meaningful relationship with dose. (r = 0.16)"
  },
  {
    "objectID": "14-hydrate.html#are-the-recovery-scores-well-described-by-a-normal-model",
    "href": "14-hydrate.html#are-the-recovery-scores-well-described-by-a-normal-model",
    "title": "14  Dehydration Recovery",
    "section": "14.4 Are the recovery scores well described by a Normal model?",
    "text": "14.4 Are the recovery scores well described by a Normal model?\nNext, we’ll do a more thorough graphical summary of our outcome, recovery score.\n\np1 <- ggplot(hydrate, aes(sample = recov.score)) +\n  geom_qq(col = '#440154') + geom_qq_line(col = \"red\") + \n  theme(aspect.ratio = 1) + \n  labs(title = \"Normal Q-Q plot: hydrate\")\n\np2 <- ggplot(hydrate, aes(x = recov.score)) +\n  geom_histogram(aes(y = stat(density)), \n                 bins = 10, fill = '#440154', col = '#FDE725') +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(hydrate$recov.score), \n                            sd = sd(hydrate$recov.score)),\n                col = \"red\", lwd = 1.5) +\n  labs(title = \"Density Function: hydrate\")\n\np3 <- ggplot(hydrate, aes(x = recov.score, y = \"\")) +\n  geom_boxplot(fill = '#440154', outlier.color = '#440154') + \n  labs(title = \"Boxplot: hydrate\", y = \"\")\n\np1 + (p2 / p3 + plot_layout(heights = c(4,1)))\n\n\n\nmosaic::favstats(~ recov.score, data = hydrate) |> kable(digits = 1)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\n\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\n\n44\n61.5\n71.5\n80\n100\n71.6\n12.9\n36\n0\n\n\n\n\n\nI see no serious problems with assuming Normality for these recovery scores. Our outcome variable doesn’t in any way need to follow a Normal distribution, but it’s nice when it does, because summaries involving means and standard deviations make sense."
  },
  {
    "objectID": "14-hydrate.html#simple-regression-using-dose-to-predict-recovery",
    "href": "14-hydrate.html#simple-regression-using-dose-to-predict-recovery",
    "title": "14  Dehydration Recovery",
    "section": "14.5 Simple Regression: Using Dose to predict Recovery",
    "text": "14.5 Simple Regression: Using Dose to predict Recovery\nTo start, consider a simple (one predictor) regression model using dose alone to predict the % Recovery (recov.score). Ignoring the age and weight covariates, what can we conclude about this relationship?"
  },
  {
    "objectID": "14-hydrate.html#the-scatterplot-with-fitted-linear-model",
    "href": "14-hydrate.html#the-scatterplot-with-fitted-linear-model",
    "title": "14  Dehydration Recovery",
    "section": "14.6 The Scatterplot, with fitted Linear Model",
    "text": "14.6 The Scatterplot, with fitted Linear Model\n\nggplot(hydrate, aes(x = dose, y = recov.score)) +\n    geom_point(size = 2) +\n    geom_smooth(method = \"lm\", formula = y ~ x, col = \"red\") +\n    labs(title = \"Simple Regression model for the hydrate data\",\n         x = \"Dose (mEq/l)\", y = \"Recovery Score (points)\")"
  },
  {
    "objectID": "14-hydrate.html#the-fitted-linear-model",
    "href": "14-hydrate.html#the-fitted-linear-model",
    "title": "14  Dehydration Recovery",
    "section": "14.7 The Fitted Linear Model",
    "text": "14.7 The Fitted Linear Model\nTo obtain the fitted linear regression model, we use the lm function:\n\nm1 <- lm(recov.score ~ dose, data = hydrate)\n\ntidy(m1) |> kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n63.90\n3.97\n16.09\n0.00\n\n\ndose\n4.88\n2.17\n2.25\n0.03\n\n\n\n\n\nSo, our fitted regression model (prediction model) is recov.score = 63.9 + 4.88 dose.\n\n14.7.1 Confidence Intervals\nWe can obtain confidence intervals around the coefficients of our fitted model with tidy, too.\n\ntidy(m1, conf.int = TRUE, conf.level = 0.90) |> kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n63.90\n3.97\n16.09\n0.00\n57.18\n70.61\n\n\ndose\n4.88\n2.17\n2.25\n0.03\n1.21\n8.55\n\n\n\n\n\nSo, our 90% confidence interval for the slope of dose ranges from 1.21 to 8.55."
  },
  {
    "objectID": "14-hydrate.html#coefficient-plots",
    "href": "14-hydrate.html#coefficient-plots",
    "title": "14  Dehydration Recovery",
    "section": "14.8 Coefficient Plots",
    "text": "14.8 Coefficient Plots\nThe tidy method makes it easy to construct coefficient plots using ggplot2.\n\ntd <- tidy(m1, conf.int = TRUE, conf.level = 0.90)\nggplot(td, aes(x = estimate, y = term, col = term)) +\n  geom_point() +\n  geom_crossbarh(aes(xmin = conf.low, xmax = conf.high)) +\n  geom_vline(xintercept = 0) +\n  guides(col = \"none\") +\n  labs(title = \"Estimates with 90% confidence intervals from m1 in hydrate\")\n\n\n\n\nAnother option would be to use geom_errorbarh in this setting, perhaps with a different color scheme…\n\ntd <- tidy(m1, conf.int = TRUE, conf.level = 0.90)\nggplot(td, aes(x = estimate, y = term, col = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +\n  geom_vline(xintercept = 0) +\n  scale_color_viridis_d(end = 0.5) +\n  guides(col = \"none\") +\n  labs(title = \"Estimates with 90% confidence intervals from m1 in hydrate\")"
  },
  {
    "objectID": "14-hydrate.html#the-summary-output",
    "href": "14-hydrate.html#the-summary-output",
    "title": "14  Dehydration Recovery",
    "section": "14.9 The Summary Output",
    "text": "14.9 The Summary Output\nTo get a more complete understanding of the fitted model, we’ll summarize it.\n\nsummary(lm(recov.score ~ dose, data = hydrate))\n\n\nCall:\nlm(formula = recov.score ~ dose, data = hydrate)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.3360  -7.2763   0.0632   8.4233  23.9028 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   63.896      3.970  16.093   <2e-16 ***\ndose           4.881      2.172   2.247   0.0313 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.21 on 34 degrees of freedom\nMultiple R-squared:  0.1293,    Adjusted R-squared:  0.1037 \nF-statistic: 5.047 on 1 and 34 DF,  p-value: 0.03127\n\n\n\n14.9.1 Model Specification\n\nThe first part of the output specifies the model that has been fit.\n\nHere, we have a simple regression model that predicts recov.score on the basis of dose.\nNotice that we’re treating dose here as a quantitative variable. If we wanted dose to be treated as a factor, we’d have specified that in the model.\n\n\n\n\n14.9.2 Residual Summary\n\nThe second part of the output summarizes the regression residuals across the subjects involved in fitting the model.\n\nThe residual is defined as the Actual value of our outcome minus the predicted value of that outcome fitted by the model.\nIn our case, the residual for a given child is their actual recov.score minus the predicted recov.score according to our model, for that child.\nThe residual summary gives us a sense of how “incorrect” our predictions are for the hydrate observations.\n\nA positive residual means that the observed value was higher than the predicted value from the linear regression model, so the prediction was too low.\nA negative residual means that the observed value was lower than the predicted value from the linear regression model, so the prediction was too high.\nThe residuals will center near 0 (the ordinary least squares model fitting process is designed so the mean of the residuals will always be zero)\nWe hope to see the median of the residuals also be near zero, generally. In this case, the median prediction is 0.06 point too low.\nThe minimum and maximum show us the largest prediction errors, made in the subjects used to fit this model.\nHere, we predicted a recovery score that was 22.3 points too high for one patient, and another of our predicted recovery scores was 23.9 points too low.\nThe middle half of our predictions were between 8.4 points too low and 7.3 points too high.\n\n\n\n\n\n14.9.3 Coefficients Output\n\nThe Coefficients output begins with a table of the estimated coefficients from the regression equation.\n\nGenerally, we write a simple regression model as \\(y = \\beta_0 + \\beta_1 x\\).\nIn the hydrate model, we have recov.score = \\(\\beta_0\\) + \\(\\beta_1\\) dose.\nThe first column of the table gives the estimated \\(\\beta\\) coefficients for our model\n\nHere the estimated intercept \\(\\hat{\\beta_0} = 63.9\\)\nThe estimated slope of dose \\(\\hat{\\beta_1} = 4.88\\)\nThus, our model is recov.score= 63.9 + 4.88 dose\n\n\n\nWe interpret these coefficients as follows:\n\nThe intercept (63.9) is the predicted recov.score for a patient receiving a dose of 0 mEq/l of the electrolytic solution.\nThe slope (4.88) of the dose is the predicted change in recov.score associated with a 1 mEq/l increase in the dose of electrolytic solution.\n\nEssentially, if we have two children like the ones studied here, and we give Roger a popsicle with dose X and Sarah a popsicle with dose X + 1, then this model predicts that Sarah will have a recovery score that is 4.88 points higher than will Roger.\nFrom the confidence interval output we saw previously with the function confint(lm(recov.score ~ dose)), we are 95% confident that the true slope for dose is between (0.47, 9.30) mEq/l. We are also 95% confident that the true intercept is between (55.8, 72.0).\n\n\n\n\n14.9.4 Correlation and Slope\nIf we like, we can use the cor function to specify the Pearson correlation of recov.score and dose, which turns out to be 0.36. - Note that the slope in a simple regression model will follow the sign of the Pearson correlation coefficient, in this case, both will be positive.\n\nhydrate |> select(recov.score, dose) |> cor()\n\n            recov.score     dose\nrecov.score    1.000000 0.359528\ndose           0.359528 1.000000\n\n\n\n\n14.9.5 Coefficient Testing\n\nsummary(lm(recov.score ~ dose, data = hydrate))\n\n\nCall:\nlm(formula = recov.score ~ dose, data = hydrate)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.3360  -7.2763   0.0632   8.4233  23.9028 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   63.896      3.970  16.093   <2e-16 ***\ndose           4.881      2.172   2.247   0.0313 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.21 on 34 degrees of freedom\nMultiple R-squared:  0.1293,    Adjusted R-squared:  0.1037 \nF-statistic: 5.047 on 1 and 34 DF,  p-value: 0.03127\n\n\nNext to each coefficient in the summary regression table is its estimated standard error, followed by the coefficient’s t value (the coefficient value divided by the standard error), and the associated two-tailed p value for the test of:\n\nH0: This coefficient’s \\(\\beta\\) value = 0 vs. \nHA: This coefficient’s \\(\\beta\\) value \\(\\neq\\) 0.\n\nFor the slope coefficient, we can interpret this choice as:\n\nH0: This predictor adds no predictive value to the model vs. \nHA: This predictor adds some predictive value to the model.\n\nIn the hydrate simple regression model, by running either tidy with or just the confint function shown below, we can establish a confidence interval for each of the estimated regression coefficients.\n\ntidy(m1, conf.int = TRUE, conf.level = 0.95) |> kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n63.90\n3.97\n16.09\n0.00\n55.83\n71.96\n\n\ndose\n4.88\n2.17\n2.25\n0.03\n0.47\n9.30\n\n\n\n\n\n\nconfint(m1, level = .95)\n\n                2.5 %    97.5 %\n(Intercept) 55.826922 71.964589\ndose         0.465695  9.295466\n\n\nIf the slope of dose was in fact zero, then this would mean that knowing the dose information would be of no additional value in predicting the outcome over just guessing the mean of recov.score for every subject.\nSo, since the confidence interval for the slope of dose does not include zero, it appears that there is at least some evidence that the model m1 is more effective than a model that ignores the dose information (and simply predicts the mean of recov.score for each subject.) That’s not saying much, actually.\n\n\n14.9.6 Summarizing the Quality of Fit\n\nThe next part of the regression summary output is a summary of fit quality.\n\nThe residual standard error estimates the standard deviation of the prediction errors made by the model.\n\nIf assumptions hold, the model will produce residuals that follow a Normal distribution with mean 0 and standard deviation equal to this residual standard error.\n\nSo we’d expect roughly 95% of our residuals to fall between -2(12.21) and +2(12.21), or roughly -24.4 to +24.4 and that we’d see virtually no residuals outside the range of -3(12.21) to +3(12.21), or roughly -36.6 to +36.6.\nThe output at the top of the summary tells us about the observed regression residuals, and that they actually range from -22 to +24.\nIn context, it’s hard to know whether or not we should be happy about this. On a scale from 0 to 100, rarely missing by more than 24 seems OK to me, but not terrific.\n\nThe degrees of freedom here are the same as the denominator degrees of freedom in the ANOVA to follow. The calculation is \\(n - k\\), where \\(n\\) = the number of observations and \\(k\\) is the number of coefficients estimated by the regression (including the intercept and any slopes).\n\nHere, there are 36 observations in the model, and we fit k = 2 coefficients; the slope and the intercept, as in any simple regression model, so df = 36 - 2 = 34.\n\n\nThe multiple R-squared value is usually just referred to as R-squared.\n\nThis is interpreted as the proportion of variation in the outcome variable that has been accounted for by our regression model.\n\nHere, we’ve accounted for just under 13% of the variation in % Recovery using Dose.\n\nThe R in multiple R-squared is the Pearson correlation of recov.score and dose, which in this case is 0.3595.\n\nSquaring this value gives the R-squared for this simple regression.\n(0.3595)^2 = 0.129\n\n\nR-squared is greedy.\n\nR-squared will always suggest that we make our models as big as possible, often including variables of dubious predictive value.\nAs a result, there are various methods for adjusting or penalizing R-squared so that we wind up with smaller models.\nThe adjusted R-squared is often a useful way to compare multiple models for the same response.\n\n\\(R^2_{adj} = 1 - \\frac{(1-R^2)(n - 1)}{n - k}\\), where \\(n\\) = the number of observations and \\(k\\) is the number of coefficients estimated by the regression (including the intercept and any slopes).\nSo, in this case, \\(R^2_{adj} = 1 - \\frac{(1 - 0.1293)(35)}{34} = 0.1037\\)\nThe adjusted R-squared value is not, technically, a proportion of anything, but it is comparable across models for the same outcome.\nThe adjusted R-squared will always be less than the (unadjusted) R-squared.\n\n\n\n\n14.9.7 ANOVA F test\n\nThe last part of the standard summary of a regression model is the overall ANOVA F test.\n\nThe hypotheses for this test are:\n\nH0: Each of the coefficients in the model (other than the intercept) has \\(\\beta\\) = 0 vs.\nHA: At least one regression slope has \\(\\beta \\neq\\) 0\n\nSince we are doing a simple regression with just one predictor, the ANOVA F test hypotheses are exactly the same as the t test for dose:\n\nH0: The slope for dose has \\(\\beta\\) = 0 vs.\nHA: The slope for dose has \\(\\beta \\neq\\) 0\n\nIn this case, we have an F statistic of 5.05 on 1 and 34 degrees of freedom, yielding p = 0.03\nThis provides some evidence that “something” in our model (here, dose is the only predictor) predicts the outcome to a degree beyond that easily attributed to chance alone. This is not actually surprising, nor is it especially interesting. The confidence interval for the slope is definitely more interesting than this.\n\nIn simple regression (regression with only one predictor), the t test for the slope (dose) always provides the same p value as the ANOVA F test.\n\nThe F test statistic in a simple regression is always by definition just the square of the slope’s t test statistic.\nHere, F = 5.047, and this is the square of t = 2.247 from the Coefficients output\n\n\nThis test is basically just a combination of the R-squared value (13%) and the sample size. We don’t learn much from it that’s practically interesting or useful."
  },
  {
    "objectID": "14-hydrate.html#viewing-the-complete-anova-table",
    "href": "14-hydrate.html#viewing-the-complete-anova-table",
    "title": "14  Dehydration Recovery",
    "section": "14.10 Viewing the complete ANOVA table",
    "text": "14.10 Viewing the complete ANOVA table\nWe can obtain the complete ANOVA table associated with this particular model, and the details behind this F test using the anova function:\n\nanova(lm(recov.score ~ dose, data = hydrate))\n\nAnalysis of Variance Table\n\nResponse: recov.score\n          Df Sum Sq Mean Sq F value  Pr(>F)  \ndose       1  752.2  752.15  5.0473 0.03127 *\nResiduals 34 5066.7  149.02                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe R-squared for our regression model is equal to the \\(\\eta^2\\) for this ANOVA model.\n\nIf we divide SS(dose) = 752.2 by the total sum of squares (752.2 + 5066.7), we’ll get the multiple R-squared [0.1293]\n\nNote that this is not the same ANOVA model we would get if we treated dose as a factor with seven levels, rather than as a quantitative variable."
  },
  {
    "objectID": "14-hydrate.html#using-glance-to-summarize-the-models-fit",
    "href": "14-hydrate.html#using-glance-to-summarize-the-models-fit",
    "title": "14  Dehydration Recovery",
    "section": "14.11 Using glance to summarize the model’s fit",
    "text": "14.11 Using glance to summarize the model’s fit\nWhen applied to a linear model, the glance function from the broom package summarizes 12 characteristics of the model’s fit.\nLet’s look at the eight of these that we’ve already addressed.\n\nglance(m1) |> select(r.squared:df, df.residual, nobs) |>\n  kable(digits = c(3, 3, 1, 2, 3, 0, 0, 0))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\ndf.residual\nnobs\n\n\n\n\n0.129\n0.104\n12.2\n5.05\n0.031\n1\n34\n36\n\n\n\n\n\n\nWe’ve discussed the R-square value, shown in r.squared.\nWe’ve also discussed the adjusted R-square value, in adj.r.squared\nsigma is the residual standard error.\nstatistic is the ANOVA F statistic.\np.value is the p value associated with the ANOVA F statistic.\ndf is the numerator degrees of freedom (here, the df associated with dose) for the ANOVA test associated with this model.\ndf.residual is the denominator degrees of freedom (here the df associated with residual) for that same ANOVA test.\nRemember that the F-statistic at the bottom of the summary output provides these last four statistics, as well.\nnobs is the number of observations (rows) used to fit the model.\n\nNow, let’s look at the remaining four summaries:\n\nglance(m1) |> select(logLik:deviance) |>\n  kable(digits = 1)\n\n\n\n\nlogLik\nAIC\nBIC\ndeviance\n\n\n\n\n-140.1\n286.3\n291\n5066.7\n\n\n\n\n\n\nlogLik is the log-likelihood value for the model, and is most commonly used for a model (like the ordinary least squares model fit by lm that is fit using the method of maximum likelihood). Thus, the log-likelihood value will be maximized in this fit.\nAIC is the Akaike Information Criterion for the model. When comparing models fitted by maximum likelihood to the same outcome variable (using the same transformation, for example), the smaller the AIC, the better the fit.\nBIC is the Bayes Information Criterion for the model. When comparing models fitted by maximum likelihood to the same outcome variable (using the same transformation, for example), the smaller the BIC, the better the fit. BIC often prefers models with fewer coefficients to estimate than does AIC.\n\nAIC and BIC can be estimated using several different approaches in R, but we’ll need to use the same one across multiple models if we’re comparing the results, because the concepts are only defined up to a constant.\n\ndeviance is the fitted model’s deviance, a measure of lack of fit. It is a generalization of the residual sum of squares seen in the ANOVA table, and takes the same value in the case of a simple linear regression model fit with lm as we have here. For some generalized linear models, we’ll use this for hypothesis testing, just as the ANOVA table does in the linear model case."
  },
  {
    "objectID": "14-hydrate.html#plotting-residuals-vs.-fitted-values",
    "href": "14-hydrate.html#plotting-residuals-vs.-fitted-values",
    "title": "14  Dehydration Recovery",
    "section": "14.12 Plotting Residuals vs. Fitted Values",
    "text": "14.12 Plotting Residuals vs. Fitted Values\nTo save the residuals and predicted (fitted) values from this simple regression model, we can use the resid and fitted commands, respectively, or we can use the augment function in the broom package to obtain a tidy data set containing these objects and others.\n\naugment(m1) %>%\n    ggplot(., aes(x = .fitted, y = .resid)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x, se = F) +\n    geom_smooth(method = \"lm\", formula = y ~ x, se = F, col = \"red\") +\n    labs(title = \"Residuals vs. Fitted values for Model m1\")\n\n\n\n\nWe can also obtain a plot of residuals vs. fitted values for m1 using the following code from base R.\n\nplot(m1, which = 1)\n\n\n\n\nWe hope in this plot to see a generally random scatter of points, perhaps looking like a “fuzzy football”. Since we only have seven possible dose values, we obtain only seven distinct predicted values, which explains the seven vertical lines in the plot. Here, the smooth red line indicates a gentle curve, but no evidence of a strong curve, or any other regular pattern in this residual plot."
  },
  {
    "objectID": "15-wcgs.html",
    "href": "15-wcgs.html",
    "title": "15  The WCGS",
    "section": "",
    "text": "knitr::opts_chunk$set(comment = NA)\n\nlibrary(knitr)\nlibrary(janitor)\nlibrary(broom)\nlibrary(patchwork)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\n\nWe will also use the geom_density_ridges function from the ggridges package, and the favstats function from the mosaic package, and the ggpairs function from the GGally package."
  },
  {
    "objectID": "15-wcgs.html#the-western-collaborative-group-study-wcgs-data-set",
    "href": "15-wcgs.html#the-western-collaborative-group-study-wcgs-data-set",
    "title": "15  The WCGS",
    "section": "15.2 The Western Collaborative Group Study (wcgs) data set",
    "text": "15.2 The Western Collaborative Group Study (wcgs) data set\nVittinghoff et al. (2012) explore data from the Western Collaborative Group Study (WCGS) in great detail1. We’ll touch lightly on some key issues in this Chapter.\n\nThe Western Collaborative Group Study (WCGS) was designed to test the hypothesis that the so-called Type A behavior pattern (TABP) - “characterized particularly by excessive drive, aggressiveness, and ambition, frequently in association with a relatively greater preoccupation with competitive activity, vocational deadlines, and similar pressures” - is a cause of coronary heart disease (CHD). Two additional goals, developed later in the study, were (1) to investigate the comparability of formulas developed in WCGS and in the Framingham Study (FS) for prediction of CHD risk, and (2) to determine how addition of TABP to an existing multivariate prediction formula affects ability to select subjects for intervention programs.\n\nThe study enrolled over 3,000 men ages 39-59 who were employed in San Francisco or Los Angeles, during 1960 and 1961.\n\nwcgs <- read_csv(\"data/wcgs.csv\") |>\n    mutate(across(where(is.character), as_factor))\n\nwcgs\n\n# A tibble: 3,154 × 22\n      id   age agec  height weight lnwght wghtcat   bmi   sbp lnsbp   dbp  chol\n   <dbl> <dbl> <fct>  <dbl>  <dbl>  <dbl> <fct>   <dbl> <dbl> <dbl> <dbl> <dbl>\n 1  2343    50 46-50     67    200   5.30 170-200  31.3   132  4.88    90   249\n 2  3656    51 51-55     73    192   5.26 170-200  25.3   120  4.79    74   194\n 3  3526    59 56-60     70    200   5.30 170-200  28.7   158  5.06    94   258\n 4 22057    51 51-55     69    150   5.01 140-170  22.1   126  4.84    80   173\n 5 12927    44 41-45     71    160   5.08 140-170  22.3   126  4.84    80   214\n 6 16029    47 46-50     64    158   5.06 140-170  27.1   116  4.75    76   206\n 7  3894    40 35-40     70    162   5.09 140-170  23.2   122  4.80    78   190\n 8 11389    41 41-45     70    160   5.08 140-170  23.0   130  4.87    84   212\n 9 12681    50 46-50     71    195   5.27 170-200  27.2   112  4.72    70   130\n10 10005    43 41-45     68    187   5.23 170-200  28.4   120  4.79    80   233\n# … with 3,144 more rows, and 10 more variables: behpat <fct>, dibpat <fct>,\n#   smoke <fct>, ncigs <dbl>, arcus <dbl>, chd69 <fct>, typchd69 <dbl>,\n#   time169 <dbl>, t1 <dbl>, uni <dbl>\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nHere, we have 3154 rows (subjects) and 22 columns (variables). After importing the data and creating a tibble with read_csv, I used mutate(across(where(is.character), as_factor) to convert all variables containing character data into factors.\n\n15.2.1 Structure of wcgs\nWe can specify the (sometimes terrible) variable names, through the names function, or we can add other elements of the structure, so that we can identify elements of particular interest.\n\nstr(wcgs)\n\ntibble [3,154 × 22] (S3: tbl_df/tbl/data.frame)\n $ id      : num [1:3154] 2343 3656 3526 22057 12927 ...\n $ age     : num [1:3154] 50 51 59 51 44 47 40 41 50 43 ...\n $ agec    : Factor w/ 5 levels \"46-50\",\"51-55\",..: 1 2 3 2 4 1 5 4 1 4 ...\n $ height  : num [1:3154] 67 73 70 69 71 64 70 70 71 68 ...\n $ weight  : num [1:3154] 200 192 200 150 160 158 162 160 195 187 ...\n $ lnwght  : num [1:3154] 5.3 5.26 5.3 5.01 5.08 ...\n $ wghtcat : Factor w/ 4 levels \"170-200\",\"140-170\",..: 1 1 1 2 2 2 2 2 1 1 ...\n $ bmi     : num [1:3154] 31.3 25.3 28.7 22.1 22.3 ...\n $ sbp     : num [1:3154] 132 120 158 126 126 116 122 130 112 120 ...\n $ lnsbp   : num [1:3154] 4.88 4.79 5.06 4.84 4.84 ...\n $ dbp     : num [1:3154] 90 74 94 80 80 76 78 84 70 80 ...\n $ chol    : num [1:3154] 249 194 258 173 214 206 190 212 130 233 ...\n $ behpat  : Factor w/ 4 levels \"A1\",\"A2\",\"B3\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ dibpat  : Factor w/ 2 levels \"Type A\",\"Type B\": 1 1 1 1 1 1 1 1 1 1 ...\n $ smoke   : Factor w/ 2 levels \"Yes\",\"No\": 1 1 2 2 2 1 2 1 2 1 ...\n $ ncigs   : num [1:3154] 25 25 0 0 0 80 0 25 0 25 ...\n $ arcus   : num [1:3154] 1 0 1 1 0 0 0 0 1 0 ...\n $ chd69   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ typchd69: num [1:3154] 0 0 0 0 0 0 0 0 0 0 ...\n $ time169 : num [1:3154] 1367 2991 2960 3069 3081 ...\n $ t1      : num [1:3154] -1.63 -4.06 0.64 1.12 2.43 ...\n $ uni     : num [1:3154] 0.486 0.186 0.728 0.624 0.379 ...\n\n\n\n\n15.2.2 Codebook for wcgs\nThis table was lovingly hand-crafted, and involved a lot of typing. We’ll look for better ways in 432.\n\n\n\n\n\n\n\n\n\nName\nStored As\nType\nDetails (units, levels, etc.)\n\n\n\n\nid\ninteger\n(nominal)\nID #, nominal and uninteresting\n\n\nage\ninteger\nquantitative\nage, in years - no decimal places\n\n\nagec\nfactor (5)\n(ordinal)\nage: 35-40, 41-45, 46-50, 51-55, 56-60\n\n\nheight\ninteger\nquantitative\nheight, in inches\n\n\nweight\ninteger\nquantitative\nweight, in pounds\n\n\nlnwght\nnumber\nquantitative\nnatural logarithm of weight\n\n\nwghtcat\nfactor (4)\n(ordinal)\nwt: < 140, 140-170, 170-200, > 200\n\n\nbmi\nnumber\nquantitative\nbody-mass index:\n\n\n\n\n\n703 * weight in lb / (height in in)2\n\n\nsbp\ninteger\nquantitative\nsystolic blood pressure, in mm Hg\n\n\nlnsbp\nnumber\nquantitative\nnatural logarithm of sbp\n\n\ndbp\ninteger\nquantitative\ndiastolic blood pressure, mm Hg\n\n\nchol\ninteger\nquantitative\ntotal cholesterol, mg/dL\n\n\nbehpat\nfactor (4)\n(nominal)\nbehavioral pattern: A1, A2, B3 or B4\n\n\ndibpat\nfactor (2)\n(binary)\nbehavioral pattern: A or B\n\n\nsmoke\nfactor (2)\n(binary)\ncigarette smoker: Yes or No\n\n\nncigs\ninteger\nquantitative\nnumber of cigarettes smoked per day\n\n\narcus\ninteger\n(nominal)\narcus senilis present (1) or absent (0)\n\n\nchd69\nfactor (2)\n(binary)\nCHD event: Yes or No\n\n\ntypchd69\ninteger\n(4 levels)\nevent: 0 = no CHD, 1 = MI or SD,\n\n\n\n\n\n2 = silent MI, 3 = angina\n\n\ntime169\ninteger\nquantitative\nfollow-up time in days\n\n\nt1\nnumber\nquantitative\nheavy-tailed (random draws)\n\n\nuni\nnumber\nquantitative\nlight-tailed (random draws)\n\n\n\n\n\n15.2.3 Quick Summary\n\nsummary(wcgs)\n\n       id             age           agec          height          weight   \n Min.   : 2001   Min.   :39.00   46-50: 750   Min.   :60.00   Min.   : 78  \n 1st Qu.: 3741   1st Qu.:42.00   51-55: 528   1st Qu.:68.00   1st Qu.:155  \n Median :11406   Median :45.00   56-60: 242   Median :70.00   Median :170  \n Mean   :10478   Mean   :46.28   41-45:1091   Mean   :69.78   Mean   :170  \n 3rd Qu.:13115   3rd Qu.:50.00   35-40: 543   3rd Qu.:72.00   3rd Qu.:182  \n Max.   :22101   Max.   :59.00                Max.   :78.00   Max.   :320  \n                                                                           \n     lnwght         wghtcat          bmi             sbp            lnsbp      \n Min.   :4.357   170-200:1171   Min.   :11.19   Min.   : 98.0   Min.   :4.585  \n 1st Qu.:5.043   140-170:1538   1st Qu.:22.96   1st Qu.:120.0   1st Qu.:4.787  \n Median :5.136   > 200  : 213   Median :24.39   Median :126.0   Median :4.836  \n Mean   :5.128   < 140  : 232   Mean   :24.52   Mean   :128.6   Mean   :4.850  \n 3rd Qu.:5.204                  3rd Qu.:25.84   3rd Qu.:136.0   3rd Qu.:4.913  \n Max.   :5.768                  Max.   :38.95   Max.   :230.0   Max.   :5.438  \n                                                                               \n      dbp              chol       behpat       dibpat     smoke     \n Min.   : 58.00   Min.   :103.0   A1: 264   Type A:1589   Yes:1502  \n 1st Qu.: 76.00   1st Qu.:197.2   A2:1325   Type B:1565   No :1652  \n Median : 80.00   Median :223.0   B3:1216                           \n Mean   : 82.02   Mean   :226.4   B4: 349                           \n 3rd Qu.: 86.00   3rd Qu.:253.0                                     \n Max.   :150.00   Max.   :645.0                                     \n                  NA's   :12                                        \n     ncigs          arcus        chd69         typchd69         time169    \n Min.   : 0.0   Min.   :0.0000   No :2897   Min.   :0.0000   Min.   :  18  \n 1st Qu.: 0.0   1st Qu.:0.0000   Yes: 257   1st Qu.:0.0000   1st Qu.:2842  \n Median : 0.0   Median :0.0000              Median :0.0000   Median :2942  \n Mean   :11.6   Mean   :0.2985              Mean   :0.1363   Mean   :2684  \n 3rd Qu.:20.0   3rd Qu.:1.0000              3rd Qu.:0.0000   3rd Qu.:3037  \n Max.   :99.0   Max.   :1.0000              Max.   :3.0000   Max.   :3430  \n                NA's   :2                                                  \n       t1                 uni           \n Min.   :-47.43147   Min.   :0.0007097  \n 1st Qu.: -1.00337   1st Qu.:0.2573755  \n Median :  0.00748   Median :0.5157779  \n Mean   : -0.03336   Mean   :0.5052159  \n 3rd Qu.:  0.97575   3rd Qu.:0.7559902  \n Max.   : 47.01623   Max.   :0.9994496  \n NA's   :39                             \n\n\nFor a more detailed description, we might consider Hmisc::describe, psych::describe, mosaic::favstats, etc."
  },
  {
    "objectID": "15-wcgs.html#are-the-sbps-normally-distributed",
    "href": "15-wcgs.html#are-the-sbps-normally-distributed",
    "title": "15  The WCGS",
    "section": "15.3 Are the SBPs Normally Distributed?",
    "text": "15.3 Are the SBPs Normally Distributed?\nConsider the question of whether the distribution of the systolic blood pressure results is well-approximated by the Normal.\n\nres <- mosaic::favstats(~ sbp, data = wcgs)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nbin_w <- 5 # specify binwidth\n\nggplot(wcgs, aes(x = sbp)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"orchid\", \n                   col = \"blue\") +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"navy\") +\n    labs(title = \"Systolic BP for `wcgs` subjects\",\n     x = \"Systolic BP (mm Hg)\", y = \"\",\n     caption = \"Superimposed Normal model\")\n\n\n\n\nSince the data contain both sbp and lnsbp (its natural logarithm), let’s compare them. Note that in preparing the graph, we’ll need to change the location for the text annotation.\n\nres <- mosaic::favstats(~ lnsbp, data = wcgs)\nbin_w <- 0.05 # specify binwidth\n\nggplot(wcgs, aes(x = lnsbp)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"orange\", \n                   col = \"blue\") +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"navy\") +\n    labs(title = \"ln(Systolic BP) for `wcgs` subjects\",\n     x = \"ln(Systolic BP)\", y = \"\",\n     caption = \"Superimposed Normal model\")\n\n\n\n\nWe can also look at Normal Q-Q plots, for instance…\n\np1 <- ggplot(wcgs, aes(sample = sbp)) +\n    geom_qq(color = \"orchid\") + \n    geom_qq_line(color = \"red\") +\n    labs(y = \"Ordered SBP\", title = \"sbp in wcgs\")\n\np2 <- ggplot(wcgs, aes(sample = lnsbp)) +\n    geom_qq(color = \"orange\") + \n    geom_qq_line(color = \"red\") +\n    labs(y = \"Ordered ln(SBP)\", title = \"ln(sbp) in wcgs\")\n\n## next step requires library(patchwork)\n\np1 + p2 + \n    plot_annotation(title = \"Normal Q-Q plots of SBP and ln(SBP) in wcgs\")\n\n\n\n\nThere’s at best a small improvement from sbp to lnsbp in terms of approximation by a Normal distribution."
  },
  {
    "objectID": "15-wcgs.html#identifying-and-describing-sbp-outliers",
    "href": "15-wcgs.html#identifying-and-describing-sbp-outliers",
    "title": "15  The WCGS",
    "section": "15.4 Identifying and Describing SBP outliers",
    "text": "15.4 Identifying and Describing SBP outliers\nIt looks like there’s an outlier (or a series of them) in the SBP data.\n\nggplot(wcgs, aes(x = \"\", y = sbp)) +\n    geom_violin() +\n    geom_boxplot(width = 0.3, fill = \"royalblue\", \n                 outlier.color = \"royalblue\") +\n    labs(title = \"Boxplot with Violin of SBP in `wcgs` data\",\n         y = \"Systolic Blood Pressure (mm Hg)\", \n         x = \"\") +\n    coord_flip() \n\n\n\n\n\nHmisc::describe(wcgs$sbp)\n\nwcgs$sbp \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3154        0       62    0.996    128.6    16.25      110      112 \n     .25      .50      .75      .90      .95 \n     120      126      136      148      156 \n\nlowest :  98 100 102 104 106, highest: 200 208 210 212 230\n\n\nThe maximum value here is 230, and is clearly the most extreme value in the data set. One way to gauge this is to describe that observation’s Z score, the number of standard deviations away from the mean that the observation falls. Here, the maximum value, 230 is 6.71 standard deviations above the mean, and thus has a Z score of 6.7.\nA negative Z score would indicate a point below the mean, while a positive Z score indicates, as we’ve seen, a point above the mean. The minimum systolic blood pressure, 98 is 2.03 standard deviations below the mean, so it has a Z score of -2.\nRecall that the Empirical Rule suggests that if a variable follows a Normal distribution, it would have approximately 95% of its observations falling inside a Z score of (-2, 2), and 99.74% falling inside a Z score range of (-3, 3). Do the systolic blood pressures appear Normally distributed?"
  },
  {
    "objectID": "15-wcgs.html#does-weight-category-relate-to-sbp",
    "href": "15-wcgs.html#does-weight-category-relate-to-sbp",
    "title": "15  The WCGS",
    "section": "15.5 Does Weight Category Relate to SBP?",
    "text": "15.5 Does Weight Category Relate to SBP?\nThe data are collected into four groups based on the subject’s weight (in pounds).\n\nggplot(wcgs, aes(x = wghtcat, y = sbp)) +\n    geom_violin() +\n    geom_boxplot(aes(fill = wghtcat), width = 0.3, notch = TRUE) +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") + \n    labs(title = \"Boxplot of Systolic BP by Weight Category in WCGS\", \n         x = \"Weight Category\", y = \"Systolic Blood Pressure\")"
  },
  {
    "objectID": "15-wcgs.html#re-leveling-a-factor",
    "href": "15-wcgs.html#re-leveling-a-factor",
    "title": "15  The WCGS",
    "section": "15.6 Re-Leveling a Factor",
    "text": "15.6 Re-Leveling a Factor\nWell, that’s not so good. We really want those weight categories (the levels) to be ordered more sensibly.\n\nwcgs |> tabyl(wghtcat)\n\n wghtcat    n    percent\n 170-200 1171 0.37127457\n 140-170 1538 0.48763475\n   > 200  213 0.06753329\n   < 140  232 0.07355739\n\n\nLike all factor variables in R, the categories are specified as levels. We want to change the order of the levels in a new version of this factor variable so they make sense. There are multiple ways to do this, but I prefer the fct_relevel function from the forcats package (part of the tidyverse.) Which order is more appropriate?\nI’ll add a new variable to the wcgs data called weight_f that relevels the wghtcat data.\n\nwcgs <- wcgs |>\n    mutate(weight_f = fct_relevel(wghtcat, \"< 140\", \"140-170\", \"170-200\", \"> 200\"))\n\nwcgs |> tabyl(weight_f)\n\n weight_f    n    percent\n    < 140  232 0.07355739\n  140-170 1538 0.48763475\n  170-200 1171 0.37127457\n    > 200  213 0.06753329\n\n\nFor more on the forcats package, check out Wickham and Grolemund (2022), especially the Section on Factors.\n\n15.6.1 SBP by Weight Category\n\nggplot(wcgs, aes(x = weight_f, y = sbp, fill = weight_f)) +\n    geom_boxplot(notch = TRUE) +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Systolic Blood Pressure by Reordered Weight Category in WCGS\", \n         x = \"Weight Category\", y = \"Systolic Blood Pressure\")\n\n\n\n\nWe might see some details well with a ridgeline plot, too.\n\nggplot(wcgs, aes(x = sbp, y = weight_f, fill = weight_f, height = ..density..)) +\n    ggridges::geom_density_ridges(scale = 2) +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"SBP by Weight Category (wcgs)\",\n         x = \"Systolic Blood Pressure\",\n         y = \"Weight Category\") \n\nPicking joint bandwidth of 3.74\n\n\n\n\n\nAs the plots suggest, patients in the heavier groups generally had higher systolic blood pressures.\n\nmosaic::favstats(sbp ~ weight_f, data = wcgs)\n\n  weight_f min  Q1 median  Q3 max     mean       sd    n missing\n1    < 140  98 112    120 130 196 123.1379 14.73394  232       0\n2  140-170 100 118    124 134 192 126.2939 13.65294 1538       0\n3  170-200 100 120    130 140 230 131.1136 15.57024 1171       0\n4    > 200 110 126    132 150 212 137.8685 16.75522  213       0"
  },
  {
    "objectID": "15-wcgs.html#are-weight-and-sbp-linked",
    "href": "15-wcgs.html#are-weight-and-sbp-linked",
    "title": "15  The WCGS",
    "section": "15.7 Are Weight and SBP Linked?",
    "text": "15.7 Are Weight and SBP Linked?\nLet’s build a scatter plot of SBP (Outcome) by Weight (Predictor), rather than breaking down into categories.\n\nggplot(wcgs, aes(x = weight, y = sbp)) +\n    geom_point(size=3, shape=1, color=\"forestgreen\") + ## default size = 2\n    stat_smooth(method=lm, color=\"red\") + ## add se=FALSE to hide conf. interval\n    stat_smooth(method=loess, se=FALSE, color=\"blue\") +\n    ggtitle(\"SBP vs. Weight in 3,154 WCGS Subjects\") \n\n\n\n\n\nThe mass of the data is hidden from us - showing 3154 points in one plot can produce little more than a blur where there are lots of points on top of each other.\nHere the least squares regression line (in red), and loess scatterplot smoother, (in blue) can help.\n\nThe relationship between systolic blood pressure and weight appears to be very close to linear, but of course there is considerable scatter around that generally linear relationship. It turns out that the Pearson correlation of these two variables is 0.253."
  },
  {
    "objectID": "15-wcgs.html#sbp-and-weight-by-arcus-senilis-groups",
    "href": "15-wcgs.html#sbp-and-weight-by-arcus-senilis-groups",
    "title": "15  The WCGS",
    "section": "15.8 SBP and Weight by Arcus Senilis groups?",
    "text": "15.8 SBP and Weight by Arcus Senilis groups?\nAn issue of interest to us will be to assess whether the SBP-Weight relationship we see above is similar among subjects who have arcus senilis and those who do not.\n\nArcus senilis is an old age syndrome where there is a white, grey, or blue opaque ring in the corneal margin (peripheral corneal opacity), or white ring in front of the periphery of the iris. It is present at birth but then fades; however, it is quite commonly present in the elderly. It can also appear earlier in life as a result of hypercholesterolemia.\nWikipedia article on Arcus Senilis, retrieved 2017-08-15\n\nLet’s start with a quick look at the arcus data.\n\nwcgs |> tabyl(arcus)\n\n arcus    n      percent valid_percent\n     0 2211 0.7010145847     0.7014594\n     1  941 0.2983512999     0.2985406\n    NA    2 0.0006341154            NA\n\n\nWe have 2 missing values, so we probably want to do something about that before plotting the data, and we may also want to create a factor variable with more meaningful labels than 1 (which means yes, arcus senilis is present) and 0 (which means no, it isn’t.)\n\nwcgs <- wcgs |>\n    mutate(arcus_f = fct_recode(factor(arcus),\n                                \"Arcus senilis\" = \"1\",\n                                \"No arcus senilis\" = \"0\"),\n           arcus_f = fct_relevel(arcus_f, \"Arcus senilis\"))\n\nwcgs |> tabyl(arcus_f, arcus)\n\n          arcus_f    0   1 NA_\n    Arcus senilis    0 941   0\n No arcus senilis 2211   0   0\n             <NA>    0   0   2\n\n\nLet’s build a version of the wcgs data that eliminates all missing data in the variables of immediate interest, and then plot the SBP-weight relationship in groups of patients with and without arcus senilis.\n\nwcgs |>\n    filter(complete.cases(arcus_f, sbp, weight)) |>\n    ggplot(aes(x = weight, y = sbp, group = arcus_f)) +\n    geom_point(shape = 1) + \n    stat_smooth(method=lm, color=\"red\") +\n    stat_smooth(method=loess, se=FALSE, color=\"blue\") +\n    labs(title = \"SBP vs. Weight by Arcus Senilis status\",\n         caption = \"3,152 Western Collaborative Group Study subjects with known arcus senilis status\") + \n    facet_wrap(~ arcus_f)"
  },
  {
    "objectID": "15-wcgs.html#linear-model-for-sbp-weight-relationship-subjects-without-arcus-senilis",
    "href": "15-wcgs.html#linear-model-for-sbp-weight-relationship-subjects-without-arcus-senilis",
    "title": "15  The WCGS",
    "section": "15.9 Linear Model for SBP-Weight Relationship: subjects without Arcus Senilis",
    "text": "15.9 Linear Model for SBP-Weight Relationship: subjects without Arcus Senilis\n\nmodel.noarcus <- \n    lm(sbp ~ weight, data = filter(wcgs, arcus == 0))\n\ntidy(model.noarcus) |> kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n95.92\n2.56\n37.54\n0\n\n\nweight\n0.19\n0.01\n12.77\n0\n\n\n\n\nglance(model.noarcus) |> select(r.squared:p.value, AIC) |> kable(digits = 3)\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\nAIC\n\n\n\n\n0.069\n0.068\n14.799\n162.959\n0\n18193.78\n\n\n\n\nsummary(model.noarcus)\n\n\nCall:\nlm(formula = sbp ~ weight, data = filter(wcgs, arcus == 0))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.011 -10.251  -2.447   7.553  99.848 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  95.9219     2.5552   37.54   <2e-16 ***\nweight        0.1902     0.0149   12.77   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.8 on 2209 degrees of freedom\nMultiple R-squared:  0.0687,    Adjusted R-squared:  0.06828 \nF-statistic:   163 on 1 and 2209 DF,  p-value: < 2.2e-16\n\n\nThe linear model for the 2211 patients without Arcus Senilis has R-squared = 6.87%.\n\nThe regression equation is 95.92 - 0.19 weight, for those patients without Arcus Senilis."
  },
  {
    "objectID": "15-wcgs.html#linear-model-for-sbp-weight-relationship-subjects-with-arcus-senilis",
    "href": "15-wcgs.html#linear-model-for-sbp-weight-relationship-subjects-with-arcus-senilis",
    "title": "15  The WCGS",
    "section": "15.10 Linear Model for SBP-Weight Relationship: subjects with Arcus Senilis",
    "text": "15.10 Linear Model for SBP-Weight Relationship: subjects with Arcus Senilis\n\nmodel.witharcus <- \n    lm(sbp ~ weight, data = filter(wcgs, arcus == 1))\n\ntidy(model.witharcus) |> kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n101.88\n3.76\n27.13\n0\n\n\nweight\n0.16\n0.02\n7.39\n0\n\n\n\n\nglance(model.witharcus) |> select(r.squared:p.value, AIC) |> kable(digits = 3)\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\nAIC\n\n\n\n\n0.055\n0.054\n14.192\n54.583\n0\n7666.828\n\n\n\n\nsummary(model.witharcus)\n\n\nCall:\nlm(formula = sbp ~ weight, data = filter(wcgs, arcus == 1))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-30.335  -9.636  -1.961   7.973  76.738 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 101.87847    3.75572  27.126  < 2e-16 ***\nweight        0.16261    0.02201   7.388 3.29e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.19 on 939 degrees of freedom\nMultiple R-squared:  0.05494,   Adjusted R-squared:  0.05393 \nF-statistic: 54.58 on 1 and 939 DF,  p-value: 3.29e-13\n\n\nThe linear model for the 941 patients with Arcus Senilis has R-squared = 5.49%.\n\nThe regression equation is 101.88 - 0.163 weight, for those patients with Arcus Senilis."
  },
  {
    "objectID": "15-wcgs.html#including-arcus-status-in-the-model",
    "href": "15-wcgs.html#including-arcus-status-in-the-model",
    "title": "15  The WCGS",
    "section": "15.11 Including Arcus Status in the model",
    "text": "15.11 Including Arcus Status in the model\n\nmodel3 <- lm(sbp ~ weight * arcus, data = filter(wcgs, !is.na(arcus)))\n\ntidy(model3) |> kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n95.92\n2.52\n38.00\n0.00\n\n\nweight\n0.19\n0.01\n12.92\n0.00\n\n\narcus\n5.96\n4.62\n1.29\n0.20\n\n\nweight:arcus\n-0.03\n0.03\n-1.02\n0.31\n\n\n\n\nglance(model3) |> select(r.squared:p.value, AIC) |> kable(digits = 3)\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\nAIC\n\n\n\n\n0.066\n0.065\n14.62\n74.094\n0\n25860.96\n\n\n\n\nsummary(model3)\n\n\nCall:\nlm(formula = sbp ~ weight * arcus, data = filter(wcgs, !is.na(arcus)))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-30.335 -10.152  -2.349   7.669  99.848 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  95.92190    2.52440  37.998   <2e-16 ***\nweight        0.19017    0.01472  12.921   <2e-16 ***\narcus         5.95657    4.61972   1.289    0.197    \nweight:arcus -0.02756    0.02703  -1.019    0.308    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.62 on 3148 degrees of freedom\nMultiple R-squared:  0.06595,   Adjusted R-squared:  0.06506 \nF-statistic: 74.09 on 3 and 3148 DF,  p-value: < 2.2e-16\n\n\nThe actual regression equation in this setting includes both weight, and an indicator variable (1 = yes, 0 = no) for arcus senilis status, and the product term combining weight and that 1/0 indicator. In 432, we’ll spend substantial time and energy discussing these product terms, but we’ll not do much of that in 431.\n\nNote the use of the product term weight*arcus in the setup of the model to allow both the slope of weight and the intercept term in the model to change depending on arcus senilis status.\n\nFor a patient who has arcus, the regression equation is SBP = 95.92 + 0.19 weight + 5.96 (1) - 0.028 weight (1) = 101.88 + 0.162 weight.\nFor a patient without arcus senilis, the regression equation is SBP = 95.92 + 0.19 weight + 5.96 (0) - 0.028 weight (0) = 95.92 + 0.19 weight.\n\n\nThe linear model including the interaction of weight and arcus to predict sbp for the 3152 patients with known Arcus Senilis status has R-squared = 6.6%. Again, we’ll discuss interaction more substantially in 432."
  },
  {
    "objectID": "15-wcgs.html#predictions-from-these-linear-models",
    "href": "15-wcgs.html#predictions-from-these-linear-models",
    "title": "15  The WCGS",
    "section": "15.12 Predictions from these Linear Models",
    "text": "15.12 Predictions from these Linear Models\nWhat is our predicted SBP for a subject weighing 175 pounds?\nHow does that change if our subject weighs 200 pounds?\nRecall that\n\nWithout Arcus Senilis, linear model for SBP = 95.9 + 0.19 x weight\nWith Arcus Senilis, linear model for SBP = 101.9 + 0.16 x weight\n\nSo the predictions for a 175 pound subject are:\n\n95.9 + 0.19 x 175 = 129 mm Hg without Arcus Senilis, and\n101.9 + 0.16 x 175 = 130 mm Hg with Arcus Senilis.\n\nAnd thus, the predictions for a 200 pound subject are:\n\n95.9 + 0.19 x 200 = 134 mm Hg without Arcus Senilis, and\n101.9 + 0.16 x 200 = 134.4 mm Hg with Arcus Senilis."
  },
  {
    "objectID": "15-wcgs.html#scatterplots-with-facets-across-a-categorical-variable",
    "href": "15-wcgs.html#scatterplots-with-facets-across-a-categorical-variable",
    "title": "15  The WCGS",
    "section": "15.13 Scatterplots with Facets Across a Categorical Variable",
    "text": "15.13 Scatterplots with Facets Across a Categorical Variable\nWe can use facets in ggplot2 to show scatterplots across the levels of a categorical variable, like behpat.\n\nggplot(wcgs, aes(x = weight, y = sbp, col = behpat)) +\n    geom_point() +\n    facet_wrap(~ behpat) +\n    geom_smooth(method = \"lm\", se = FALSE, \n                formula = y ~ x, col = \"black\") +\n    guides(color = \"none\") +\n    theme(strip.text = element_text(face=\"bold\", size=rel(1.25), color=\"white\"),\n          strip.background = element_rect(fill=\"royalblue\")) +\n    labs(title = \"Scatterplots of SBP vs. Weight within Behavior Pattern\")"
  },
  {
    "objectID": "15-wcgs.html#scatterplot-and-correlation-matrices",
    "href": "15-wcgs.html#scatterplot-and-correlation-matrices",
    "title": "15  The WCGS",
    "section": "15.14 Scatterplot and Correlation Matrices",
    "text": "15.14 Scatterplot and Correlation Matrices\nA scatterplot matrix can be very helpful in understanding relationships between multiple variables simultaneously. There are several ways to build such a thing, including the pairs function…\n\npairs (~ sbp + age + weight + height, data=wcgs, main=\"Simple Scatterplot Matrix\")\n\n\n\n\n\n15.14.1 Displaying a Correlation Matrix\n\nwcgs |>\n    dplyr::select(sbp, age, weight, height) |>\n    cor() |>\n    kable(digits = 3)\n\n\n\n\n\nsbp\nage\nweight\nheight\n\n\n\n\nsbp\n1.000\n0.166\n0.253\n0.018\n\n\nage\n0.166\n1.000\n-0.034\n-0.095\n\n\nweight\n0.253\n-0.034\n1.000\n0.533\n\n\nheight\n0.018\n-0.095\n0.533\n1.000\n\n\n\n\n\n\n\n15.14.2 Using the GGally package\nThe ggplot2 system doesn’t have a built-in scatterplot system. There are some nice add-ins in the world, though. One option I sort of like is in the GGally package, which can produce both correlation matrices and scatterplot matrices.\nThe ggpairs function provides a density plot on each diagonal, Pearson correlations on the upper right and scatterplots on the lower left of the matrix.\n\nGGally::ggpairs(wcgs |> select(sbp, age, weight, height), \n                title = \"Scatterplot Matrix via ggpairs\")\n\n\n\n\n\n\n\n\nVittinghoff, Eric, David V. Glidden, Stephen C. Shiboski, and Charles E. McCulloch. 2012. Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models. Second. Springer-Verlag, Inc. http://www.biostat.ucsf.edu/vgsm/.\n\n\nWickham, Hadley, and Garrett Grolemund. 2022. R for Data Science. Second. O’Reilly. https://r4ds.hadley.nz/."
  },
  {
    "objectID": "98_gettingdataintoR.html",
    "href": "98_gettingdataintoR.html",
    "title": "Appendix A — Getting Data Into R",
    "section": "",
    "text": "To use data from an R package, for instance, the bechdel data from the fivethirtyeight package, you can simply load the relevant package with library and then the data frame will be available\n\nlibrary(fivethirtyeight)\nlibrary(tidyverse)\n\nbechdel\n\n# A tibble: 1,794 × 15\n    year imdb    title test  clean…¹ binary budget domgr…² intgr…³ code  budge…⁴\n   <int> <chr>   <chr> <chr> <ord>   <chr>   <int>   <dbl>   <dbl> <chr>   <int>\n 1  2013 tt1711… 21 &… nota… notalk  FAIL   1.3 e7  2.57e7  4.22e7 2013…  1.3 e7\n 2  2012 tt1343… Dred… ok-d… ok      PASS   4.5 e7  1.34e7  4.09e7 2012…  4.57e7\n 3  2013 tt2024… 12 Y… nota… notalk  FAIL   2   e7  5.31e7  1.59e8 2013…  2   e7\n 4  2013 tt1272… 2 Gu… nota… notalk  FAIL   6.1 e7  7.56e7  1.32e8 2013…  6.1 e7\n 5  2013 tt0453… 42    men   men     FAIL   4   e7  9.50e7  9.50e7 2013…  4   e7\n 6  2013 tt1335… 47 R… men   men     FAIL   2.25e8  3.84e7  1.46e8 2013…  2.25e8\n 7  2013 tt1606… A Go… nota… notalk  FAIL   9.2 e7  6.73e7  3.04e8 2013…  9.2 e7\n 8  2013 tt2194… Abou… ok-d… ok      PASS   1.2 e7  1.53e7  8.73e7 2013…  1.2 e7\n 9  2013 tt1814… Admi… ok    ok      PASS   1.3 e7  1.80e7  1.80e7 2013…  1.3 e7\n10  2013 tt1815… Afte… nota… notalk  FAIL   1.3 e8  6.05e7  2.44e8 2013…  1.3 e8\n# … with 1,784 more rows, 4 more variables: domgross_2013 <dbl>,\n#   intgross_2013 <dbl>, period_code <int>, decade_code <int>, and abbreviated\n#   variable names ¹​clean_test, ²​domgross, ³​intgross, ⁴​budget_2013\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names"
  },
  {
    "objectID": "98_gettingdataintoR.html#using-read_rds-to-read-in-an-r-data-set",
    "href": "98_gettingdataintoR.html#using-read_rds-to-read-in-an-r-data-set",
    "title": "Appendix A — Getting Data Into R",
    "section": "Using read_rds to read in an R data set",
    "text": "Using read_rds to read in an R data set\nWe have provided the nnyfs.Rds data file on the course data page.\nSuppose you have downloaded this data file into a directory on your computer called data which is a sub-directory of the directory where you plan to do your work, perhaps called 431-nnyfs.\nOpen RStudio and create a new project into the 431-nnyfs directory on your computer. You should see a data subdirectory in the Files window in RStudio after the project is created.\nNow, read in the nnyfs.Rds file to a new tibble in R called nnyfs_new with the following command:\n\nnnyfs_new <- read_rds(\"data/nnyfs.Rds\")\n\nHere are the results…\n\nnnyfs_new\n\n# A tibble: 1,518 × 45\n    SEQN sex    age_ch…¹ race_…² educ_…³ langu…⁴ sampl…⁵ incom…⁶ age_a…⁷ educ_…⁸\n   <dbl> <fct>     <dbl> <fct>     <dbl> <fct>     <dbl>   <dbl>   <dbl> <fct>  \n 1 71917 Female       15 3_Blac…       9 English  28299.    0.21      46 2_9-11…\n 2 71918 Female        8 3_Blac…       2 English  15127.    5         46 3_High…\n 3 71919 Female       14 2_Whit…       8 English  29977.    5         42 5_Coll…\n 4 71920 Female       15 2_Whit…       8 English  80652.    0.87      53 3_High…\n 5 71921 Male          3 2_Whit…      NA English  55592.    4.34      31 3_High…\n 6 71922 Male         12 1_Hisp…       6 English  27365.    5         42 4_Some…\n 7 71923 Male         12 2_Whit…       5 English  86673.    5         39 2_9-11…\n 8 71924 Female        8 4_Othe…       2 English  39549.    2.74      31 3_High…\n 9 71925 Male          7 1_Hisp…       0 English  42333.    0.46      45 2_9-11…\n10 71926 Male          8 3_Blac…       2 English  15307.    1.57      56 3_High…\n# … with 1,508 more rows, 35 more variables: respondent <fct>, salt_used <fct>,\n#   energy <dbl>, protein <dbl>, sugar <dbl>, fat <dbl>, diet_yesterday <fct>,\n#   water <dbl>, plank_time <dbl>, height <dbl>, weight <dbl>, bmi <dbl>,\n#   bmi_cat <fct>, arm_length <dbl>, waist <dbl>, arm_circ <dbl>,\n#   calf_circ <dbl>, calf_skinfold <dbl>, triceps_skinfold <dbl>,\n#   subscapular_skinfold <dbl>, active_days <dbl>, tv_hours <dbl>,\n#   computer_hours <dbl>, physical_last_week <fct>, enjoy_recess <fct>, …\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names"
  },
  {
    "objectID": "98_gettingdataintoR.html#using-read_csv-to-read-in-a-comma-separated-version-of-a-data-file",
    "href": "98_gettingdataintoR.html#using-read_csv-to-read-in-a-comma-separated-version-of-a-data-file",
    "title": "Appendix A — Getting Data Into R",
    "section": "Using read_csv to read in a comma-separated version of a data file",
    "text": "Using read_csv to read in a comma-separated version of a data file\nWe have provided the nnyfs.csv data file on the course data page.\nSuppose you have downloaded this data file into a directory on your computer called data which is a sub-directory of the directory where you plan to do your work, perhaps called 431-nnyfs.\nOpen RStudio and create a new project into the 431-nnyfs directory on your computer. You should see a data subdirectory in the Files window in RStudio after the project is created.\nNow, read in the nnyfs.csv file to a new tibble in R called nnyfs_new2 with the following command:\n\nnnyfs_new2 <- read_csv(\"data/nnyfs.csv\")\n\nRows: 1518 Columns: 45\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (18): sex, race_eth, language, educ_adult, respondent, salt_used, diet_y...\ndbl (27): SEQN, age_child, educ_child, sampling_wt, income_pov, age_adult, e...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnnyfs_new2\n\n# A tibble: 1,518 × 45\n    SEQN sex    age_ch…¹ race_…² educ_…³ langu…⁴ sampl…⁵ incom…⁶ age_a…⁷ educ_…⁸\n   <dbl> <chr>     <dbl> <chr>     <dbl> <chr>     <dbl>   <dbl>   <dbl> <chr>  \n 1 71917 Female       15 3_Blac…       9 English  28299.    0.21      46 2_9-11…\n 2 71918 Female        8 3_Blac…       2 English  15127.    5         46 3_High…\n 3 71919 Female       14 2_Whit…       8 English  29977.    5         42 5_Coll…\n 4 71920 Female       15 2_Whit…       8 English  80652.    0.87      53 3_High…\n 5 71921 Male          3 2_Whit…      NA English  55592.    4.34      31 3_High…\n 6 71922 Male         12 1_Hisp…       6 English  27365.    5         42 4_Some…\n 7 71923 Male         12 2_Whit…       5 English  86673.    5         39 2_9-11…\n 8 71924 Female        8 4_Othe…       2 English  39549.    2.74      31 3_High…\n 9 71925 Male          7 1_Hisp…       0 English  42333.    0.46      45 2_9-11…\n10 71926 Male          8 3_Blac…       2 English  15307.    1.57      56 3_High…\n# … with 1,508 more rows, 35 more variables: respondent <chr>, salt_used <chr>,\n#   energy <dbl>, protein <dbl>, sugar <dbl>, fat <dbl>, diet_yesterday <chr>,\n#   water <dbl>, plank_time <dbl>, height <dbl>, weight <dbl>, bmi <dbl>,\n#   bmi_cat <chr>, arm_length <dbl>, waist <dbl>, arm_circ <dbl>,\n#   calf_circ <dbl>, calf_skinfold <dbl>, triceps_skinfold <dbl>,\n#   subscapular_skinfold <dbl>, active_days <dbl>, tv_hours <dbl>,\n#   computer_hours <dbl>, physical_last_week <chr>, enjoy_recess <chr>, …\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nIf you also want to convert the character variables to factors, as you will often want to do before analyzing the results, you should instead use:\n\nnnyfs_new3 <- read_csv(\"data/nnyfs.csv\") %>%\n    mutate(across(where(is.character), as_factor))\n\nRows: 1518 Columns: 45\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (18): sex, race_eth, language, educ_adult, respondent, salt_used, diet_y...\ndbl (27): SEQN, age_child, educ_child, sampling_wt, income_pov, age_adult, e...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnnyfs_new3\n\n# A tibble: 1,518 × 45\n    SEQN sex    age_ch…¹ race_…² educ_…³ langu…⁴ sampl…⁵ incom…⁶ age_a…⁷ educ_…⁸\n   <dbl> <fct>     <dbl> <fct>     <dbl> <fct>     <dbl>   <dbl>   <dbl> <fct>  \n 1 71917 Female       15 3_Blac…       9 English  28299.    0.21      46 2_9-11…\n 2 71918 Female        8 3_Blac…       2 English  15127.    5         46 3_High…\n 3 71919 Female       14 2_Whit…       8 English  29977.    5         42 5_Coll…\n 4 71920 Female       15 2_Whit…       8 English  80652.    0.87      53 3_High…\n 5 71921 Male          3 2_Whit…      NA English  55592.    4.34      31 3_High…\n 6 71922 Male         12 1_Hisp…       6 English  27365.    5         42 4_Some…\n 7 71923 Male         12 2_Whit…       5 English  86673.    5         39 2_9-11…\n 8 71924 Female        8 4_Othe…       2 English  39549.    2.74      31 3_High…\n 9 71925 Male          7 1_Hisp…       0 English  42333.    0.46      45 2_9-11…\n10 71926 Male          8 3_Blac…       2 English  15307.    1.57      56 3_High…\n# … with 1,508 more rows, 35 more variables: respondent <fct>, salt_used <fct>,\n#   energy <dbl>, protein <dbl>, sugar <dbl>, fat <dbl>, diet_yesterday <fct>,\n#   water <dbl>, plank_time <dbl>, height <dbl>, weight <dbl>, bmi <dbl>,\n#   bmi_cat <fct>, arm_length <dbl>, waist <dbl>, arm_circ <dbl>,\n#   calf_circ <dbl>, calf_skinfold <dbl>, triceps_skinfold <dbl>,\n#   subscapular_skinfold <dbl>, active_days <dbl>, tv_hours <dbl>,\n#   computer_hours <dbl>, physical_last_week <fct>, enjoy_recess <fct>, …\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nNote that, for example, sex and race_eth are now listed as factor (fctr) variables. One place where this distinction between character and factor variables matters is when you summarize the data.\n\nsummary(nnyfs_new2$race_eth)\n\n   Length     Class      Mode \n     1518 character character \n\n\n\nsummary(nnyfs_new3$race_eth)\n\n  3_Black Non-Hispanic   2_White Non-Hispanic             1_Hispanic \n                   338                    610                    450 \n4_Other Race/Ethnicity \n                   120"
  },
  {
    "objectID": "98_gettingdataintoR.html#converting-character-variables-into-factors",
    "href": "98_gettingdataintoR.html#converting-character-variables-into-factors",
    "title": "Appendix A — Getting Data Into R",
    "section": "Converting Character Variables into Factors",
    "text": "Converting Character Variables into Factors\nThe command you want to create newdata from olddata is:\nnewdata <- olddata %>%\n    mutate(across(where(is.character), as_factor))\nFor more on factors, visit https://r4ds.had.co.nz/factors.html"
  },
  {
    "objectID": "98_gettingdataintoR.html#converting-data-frames-to-tibbles",
    "href": "98_gettingdataintoR.html#converting-data-frames-to-tibbles",
    "title": "Appendix A — Getting Data Into R",
    "section": "Converting Data Frames to Tibbles",
    "text": "Converting Data Frames to Tibbles\nUse as_tibble() or simply tibble() to assign the attributes of a tibble to a data frame. Note that read_rds and read_csv automatically create tibbles.\nFor more on tibbles, visit https://r4ds.had.co.nz/tibbles.html."
  },
  {
    "objectID": "98_gettingdataintoR.html#for-more-advice",
    "href": "98_gettingdataintoR.html#for-more-advice",
    "title": "Appendix A — Getting Data Into R",
    "section": "For more advice",
    "text": "For more advice\nConsider visiting the software tutorials page under the R and Data heading on our main web site."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Appendix B — References",
    "section": "",
    "text": "Baumer, Benjamin S., Daniel T. Kaplan, and Nicholas J. Horton. 2017.\nModern Data Science with r. Boca Raton, FL: CRC Press. https://mdsr-book.github.io/.\n\n\nBock, David E., Paul F. Velleman, and Richard D. De Veaux. 2004.\nStats: Modelling the World. Boston MA: Pearson\nAddison-Wesley.\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using\nRegression and Multilevel-Hierarchical Models. New York: Cambridge\nUniversity Press. http://www.stat.columbia.edu/~gelman/arm/.\n\n\nGelman, Andrew, and Deborah Nolan. 2017. Teaching Statistics: A Bag\nof Tricks. Second Edition. Oxford, UK: Oxford University Press.\n\n\nIsmay, Chester, and Albert Y. Kim. 2022. ModernDive: Statistical\nInference via Data Science. http://moderndive.com/.\n\n\nNorman, Geoffrey R., and David L. Streiner. 2014. Biostatistics: The\nBare Essentials. Fourth. People’s Medical Publishing House.\n\n\nRamsey, Fred L., and Daniel W. Schafer. 2002. The Statistical\nSleuth: A Course in Methods of Data Analysis. Second. Pacific\nGrove, CA: Duxbury.\n\n\nVittinghoff, Eric, David V. Glidden, Stephen C. Shiboski, and Charles E.\nMcCulloch. 2012. Regression Methods in Biostatistics: Linear,\nLogistic, Survival, and Repeated Measures Models. Second.\nSpringer-Verlag, Inc. http://www.biostat.ucsf.edu/vgsm/.\n\n\nWainer, Howard. 1997. Visual Revelations: Graphical Tales of Fate\nand Deception from Napoleon Bonaparte to Ross Perot. New York:\nSpringer-Verlag.\n\n\n———. 2005. Graphic Discovery: A Trout in the Milk and Other Visual\nAdventures. Princeton, NJ: Princeton University Press.\n\n\n———. 2013. Medical Illuminations: Using Evidence, Visualization and\nStatistical Thinking to Improve Healthcare. New York: Oxford\nUniversity Press.\n\n\nWickham, Hadley, and Garrett Grolemund. 2022. R for Data\nScience. Second. O’Reilly. https://r4ds.hadley.nz/.\n\n\nYamada, SB, and EG Boulding. 1998. “Claw Morphology, Prey Size\nSelection and Foraging Efficiency in Generalist and Specialist\nShell-Breaking Crabs.” Journal of Experimental Marine Biology\nand Ecology 220: 191–211. http://www.science.oregonstate.edu/~yamadas/SylviaCV/BehrensYamada_Boulding1998.pdf."
  }
]